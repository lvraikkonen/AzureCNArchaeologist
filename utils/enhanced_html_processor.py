#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®æ­£ç‰ˆHTMLå¤„ç†å·¥å…·
ç¡®ä¿è¡¨æ ¼å†…å®¹å®Œæ•´ä¿ç•™ï¼Œä¸¥æ ¼æŒ‰ç…§å‚è€ƒè¾“å‡ºæ ‡å‡†
"""

import json
import os
import re
from typing import Dict, List, Set, Optional, Tuple
from bs4 import BeautifulSoup, Comment, NavigableString, Tag
from datetime import datetime


class RegionFilterProcessor:
    """åŒºåŸŸè¡¨æ ¼è¿‡æ»¤å¤„ç†å™¨"""
    
    def __init__(self, config_file_path: str = "soft-category.json"):
        self.config_file_path = config_file_path
        self.region_filter_config = {}
        self.active_region = None
        self.load_region_filter_config()
    
    def load_region_filter_config(self) -> bool:
        """åŠ è½½åŒºåŸŸè¿‡æ»¤é…ç½®"""
        try:
            if not os.path.exists(self.config_file_path):
                print(f"âš  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file_path}")
                return False
                
            with open(self.config_file_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            for item in config_data:
                os_name = item.get('os', '')
                region = item.get('region', '')
                table_ids = item.get('tableIDs', [])
                
                if region:
                    if os_name not in self.region_filter_config:
                        self.region_filter_config[os_name] = {}
                    self.region_filter_config[os_name][region] = table_ids
            
            print(f"âœ“ å·²åŠ è½½åŒºåŸŸè¿‡æ»¤é…ç½®: {len(config_data)}æ¡è§„åˆ™")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def set_active_region(self, region: str, product: str = "Azure Database for MySQL"):
        """è®¾ç½®å½“å‰æ´»è·ƒåŒºåŸŸ"""
        self.active_region = region
        self.active_product = product
        print(f"âœ“ è®¾ç½®æ´»è·ƒåŒºåŸŸ: {region} (äº§å“: {product})")
    
    def should_filter_table(self, table_id: str) -> bool:
        """åˆ¤æ–­è¡¨æ ¼æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
        try:
            if not table_id or not isinstance(table_id, str):
                return False
                
            if not self.active_region or not hasattr(self, 'active_product'):
                return False

            product_config = self.region_filter_config.get(self.active_product, {})
            excluded_tables = product_config.get(self.active_region)
            
            if excluded_tables is None:
                return False

            if not excluded_tables:
                return False

            table_id_clean = table_id.strip()
            table_id_with_hash = f"#{table_id_clean}" if not table_id_clean.startswith('#') else table_id_clean
            table_id_without_hash = table_id_clean[1:] if table_id_clean.startswith('#') and len(table_id_clean) > 1 else table_id_clean

            is_excluded = (table_id_clean in excluded_tables or
                          table_id_with_hash in excluded_tables or
                          table_id_without_hash in excluded_tables)
            
            return is_excluded
                
        except Exception as e:
            print(f"âš  è¡¨æ ¼è¿‡æ»¤åˆ¤æ–­å¼‚å¸¸ (table_id: {table_id}): {e}")
            return False


class FixedHTMLProcessor:
    """ä¿®æ­£ç‰ˆHTMLå¤„ç†å™¨ - ç¡®ä¿è¡¨æ ¼å†…å®¹å®Œæ•´ä¿ç•™"""
    
    def __init__(self, region_filter: Optional[RegionFilterProcessor] = None):
        self.region_filter = region_filter or RegionFilterProcessor()
        self.removed_elements_log = []
    
    def careful_clean_html(self, soup: BeautifulSoup) -> int:
        """
        è°¨æ…æ¸…ç†HTMLï¼Œç¡®ä¿ä¸ä¸¢å¤±è¡¨æ ¼å†…å®¹
        """
        print("ğŸ”§ å¼€å§‹è°¨æ…æ¸…ç†HTMLï¼ˆç¡®ä¿è¡¨æ ¼å†…å®¹å®Œæ•´ï¼‰...")
        removed_count = 0
        
        # 1. ç§»é™¤æ ·å¼å’Œè„šæœ¬ï¼ˆä¸å½±å“å†…å®¹ï¼‰
        removed_count += self._remove_styles_and_scripts_only(soup)
        
        # 2. ç§»é™¤å¯¼èˆªå’Œäº¤äº’å…ƒç´ ï¼ˆä¿ç•™å†…å®¹å®¹å™¨ï¼‰
        removed_count += self._remove_navigation_carefully(soup)
        
        # 3. è°¨æ…å±•å¼€tabç»“æ„ï¼ˆç¡®ä¿å†…å®¹å®Œæ•´æå–ï¼‰
        removed_count += self._carefully_flatten_tabs(soup)
        
        # 4. æ¸…ç†å±æ€§ï¼ˆä¿ç•™é‡è¦å±æ€§ï¼‰
        removed_count += self._clean_attributes_safely(soup)
        
        # 5. è½»é‡æ¸…ç†ç©ºå…ƒç´ ï¼ˆä¸åˆ é™¤æœ‰å†…å®¹çš„å…ƒç´ ï¼‰
        removed_count += self._light_cleanup_empty_elements(soup)
        
        print(f"âœ“ è°¨æ…æ¸…ç†å®Œæˆï¼Œå…±å¤„ç† {removed_count} ä¸ªå…ƒç´ ")
        return removed_count
    
    def _remove_styles_and_scripts_only(self, soup: BeautifulSoup) -> int:
        """åªç§»é™¤æ ·å¼å’Œè„šæœ¬ï¼Œä¸è§¦ç¢°å†…å®¹"""
        print("  ğŸ¨ ç§»é™¤æ ·å¼å’Œè„šæœ¬...")
        removed_count = 0
        
        # ç§»é™¤æ ·å¼å’Œè„šæœ¬æ ‡ç­¾
        for tag in soup.find_all(['link', 'style', 'script', 'noscript']):
            tag.decompose()
            removed_count += 1
        
        # ç§»é™¤HTMLæ³¨é‡Š
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
            removed_count += 1
        
        # ç§»é™¤å†…è”æ ·å¼å±æ€§
        for tag in soup.find_all():
            if tag.get('style'):
                del tag['style']
        
        return removed_count
    
    def _remove_navigation_carefully(self, soup: BeautifulSoup) -> int:
        """è°¨æ…ç§»é™¤å¯¼èˆªå…ƒç´ ï¼Œä¸å½±å“å†…å®¹åŒºåŸŸ"""
        print("  ğŸ§­ è°¨æ…ç§»é™¤å¯¼èˆªå…ƒç´ ...")
        removed_count = 0
        
        # åªç§»é™¤æ˜ç¡®çš„å¯¼èˆªå’Œäº¤äº’å…ƒç´ 
        navigation_selectors = [
            ('div', 'region-container'),
            ('div', 'software-kind-container'),
            ('ul', 'tab-nav'),
            ('div', 'dropdown-container'),
            ('div', 'dropdown-box'),
            ('div', 'bread-crumb'),
            ('div', 'left-navigation-select'),
            ('div', 'documentation-navigation'),
            ('div', 'acn-header-container'),
            ('div', 'public_footerpage'),
            ('nav', None),
            ('header', None),
            ('footer', None),
        ]
        
        for tag_name, class_name in navigation_selectors:
            if class_name:
                elements = soup.find_all(tag_name, class_=class_name)
            else:
                elements = soup.find_all(tag_name)
                
            for element in elements:
                element.decompose()
                removed_count += 1
        
        # ç§»é™¤è¡¨å•å…ƒç´ 
        for tag in soup.find_all(['form', 'input', 'select', 'option', 'button', 'textarea']):
            tag.decompose()
            removed_count += 1
        
        return removed_count
    
    def _carefully_flatten_tabs(self, soup: BeautifulSoup) -> int:
        """è°¨æ…å±•å¼€tabç»“æ„ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½è¢«ä¿ç•™"""
        print("  ğŸ“‚ è°¨æ…å±•å¼€tabç»“æ„...")
        removed_count = 0
        
        # æŸ¥æ‰¾æ‰€æœ‰tabå®¹å™¨
        tab_containers = soup.find_all('div', class_=lambda x: x and any(
            keyword in str(x).lower() for keyword in ['tab-content', 'tab-container', 'tab-panel']
        ))
        
        # è¿‡æ»¤æ‰Noneå€¼å’Œæ— æ•ˆå®¹å™¨
        valid_containers = [container for container in tab_containers if container and hasattr(container, 'get')]
        
        print(f"    ğŸ” æ‰¾åˆ° {len(valid_containers)} ä¸ªæœ‰æ•ˆtabå®¹å™¨")
        
        for container in valid_containers:
            try:
                container_class = container.get('class', ['unknown'])
                if isinstance(container_class, list):
                    container_class = ' '.join(container_class)
                print(f"    ğŸ“‹ å¤„ç†å®¹å™¨: {container_class}")
                
                # æ”¶é›†å®¹å™¨ä¸­çš„æ‰€æœ‰å†…å®¹å…ƒç´ 
                content_elements = []
                
                # é€’å½’æå–æ‰€æœ‰æœ‰æ„ä¹‰çš„å†…å®¹
                self._extract_all_content_recursive(container, content_elements)
                
                # åœ¨å®¹å™¨ä½ç½®æ’å…¥æå–çš„å†…å®¹
                for element in content_elements:
                    if element and isinstance(element, Tag):
                        container.insert_before(element)
                
                # ç§»é™¤åŸå§‹å®¹å™¨
                container.decompose()
                removed_count += 1
                
                print(f"    âœ“ æå–äº† {len(content_elements)} ä¸ªå†…å®¹å…ƒç´ ")
                
            except Exception as e:
                print(f"    âš  å¤„ç†å®¹å™¨æ—¶å‡ºé”™: {e}")
                continue
        
        return removed_count
    
    def _extract_all_content_recursive(self, container: Tag, content_list: List[Tag]):
        """é€’å½’æå–å®¹å™¨ä¸­çš„æ‰€æœ‰å†…å®¹"""
        
        # æ£€æŸ¥å®¹å™¨æ˜¯å¦æœ‰æ•ˆ
        if not container or not hasattr(container, 'children'):
            return
        
        # è¦ä¿ç•™çš„å†…å®¹æ ‡ç­¾
        content_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'table', 'ul', 'ol', 'div', 'section']
        
        try:
            for child in container.children:
                if isinstance(child, Tag) and child:
                    # å¦‚æœæ˜¯å†…å®¹æ ‡ç­¾ï¼Œç›´æ¥æå–
                    if child.name in content_tags:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯tabç›¸å…³çš„åŒ…è£…å™¨
                        if self._is_tab_wrapper(child):
                            # å¦‚æœæ˜¯tabåŒ…è£…å™¨ï¼Œé€’å½’æå–å…¶å†…å®¹
                            self._extract_all_content_recursive(child, content_list)
                        else:
                            # å¦‚æœæ˜¯å®é™…å†…å®¹ï¼Œå…‹éš†å¹¶æ·»åŠ åˆ°åˆ—è¡¨
                            cloned = self._safe_clone_element(child)
                            if cloned:
                                content_list.append(cloned)
                    else:
                        # å¯¹äºå…¶ä»–æ ‡ç­¾ï¼Œä¹Ÿé€’å½’æ£€æŸ¥
                        self._extract_all_content_recursive(child, content_list)
        except Exception as e:
            print(f"    âš  é€’å½’æå–å†…å®¹æ—¶å‡ºé”™: {e}")
            return
    
    def _is_tab_wrapper(self, element: Tag) -> bool:
        """åˆ¤æ–­å…ƒç´ æ˜¯å¦æ˜¯tabåŒ…è£…å™¨"""
        try:
            if not isinstance(element, Tag) or not element:
                return False
            
            classes = element.get('class', [])
            if isinstance(classes, str):
                classes = [classes]
            
            tab_wrapper_classes = [
                'tab-content', 'tab-panel', 'tab-container', 'tab-items',
                'dropdown-container', 'technical-azure-selector'
            ]
            
            return any(tab_class in ' '.join(classes) for tab_class in tab_wrapper_classes)
        except Exception as e:
            print(f"    âš  æ£€æŸ¥tabåŒ…è£…å™¨æ—¶å‡ºé”™: {e}")
            return False
    
    def _safe_clone_element(self, element: Tag) -> Optional[Tag]:
        """å®‰å…¨å…‹éš†å…ƒç´ ï¼Œä¿æŒæ‰€æœ‰å†…å®¹"""
        try:
            if not isinstance(element, Tag) or not element:
                return None
            
            # ä½¿ç”¨BeautifulSoupçš„å­—ç¬¦ä¸²è¡¨ç¤ºæ¥åˆ›å»ºå®Œæ•´å‰¯æœ¬
            element_html = str(element)
            cloned_soup = BeautifulSoup(element_html, 'html.parser')
            cloned_element = cloned_soup.find()
            
            return cloned_element
            
        except Exception as e:
            print(f"    âš  å…‹éš†å…ƒç´ å¤±è´¥: {e}")
            return None
    
    def _clean_attributes_safely(self, soup: BeautifulSoup) -> int:
        """å®‰å…¨æ¸…ç†å±æ€§ï¼Œä¿ç•™é‡è¦çš„å±æ€§"""
        print("  ğŸ§¹ å®‰å…¨æ¸…ç†å±æ€§...")
        cleaned_count = 0
        
        # è¦ä¿ç•™çš„é‡è¦å±æ€§
        important_attrs = {
            'id', 'href', 'src', 'alt', 'title', 'width', 'align', 
            'cellpadding', 'cellspacing', 'class'
        }
        
        # è¦ä¿ç•™çš„é‡è¦class
        important_classes = {
            'common-banner', 'common-banner-image', 'common-banner-title',
            'pricing-page-section', 'more-detail'
        }
        
        try:
            all_tags = soup.find_all()
            for tag in all_tags:
                if not tag or not hasattr(tag, 'attrs'):
                    continue
                
                try:
                    # æ¸…ç†ä¸é‡è¦çš„å±æ€§
                    attrs_to_remove = []
                    for attr in tag.attrs:
                        if attr not in important_attrs:
                            attrs_to_remove.append(attr)
                    
                    for attr in attrs_to_remove:
                        if attr in tag.attrs:  # å†æ¬¡æ£€æŸ¥å±æ€§æ˜¯å¦å­˜åœ¨
                            del tag.attrs[attr]
                            cleaned_count += 1
                    
                    # è¿‡æ»¤classå±æ€§
                    if tag.get('class'):
                        current_classes = tag['class'] if isinstance(tag['class'], list) else [tag['class']]
                        filtered_classes = [cls for cls in current_classes if cls in important_classes]
                        if filtered_classes:
                            tag['class'] = filtered_classes
                        else:
                            if 'class' in tag.attrs:
                                del tag['class']
                                
                except Exception as e:
                    print(f"    âš  æ¸…ç†æ ‡ç­¾å±æ€§æ—¶å‡ºé”™: {e}")
                    continue
                    
        except Exception as e:
            print(f"    âš  å±æ€§æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")
        
        return cleaned_count
    
    def _light_cleanup_empty_elements(self, soup: BeautifulSoup) -> int:
        """è½»é‡æ¸…ç†ç©ºå…ƒç´ ï¼Œä¸åˆ é™¤å¯èƒ½æœ‰å†…å®¹çš„å…ƒç´ """
        print("  ğŸ—‘ï¸ è½»é‡æ¸…ç†ç©ºå…ƒç´ ...")
        removed_count = 0
        
        try:
            # åªæ¸…ç†æ˜ç¡®ä¸ºç©ºä¸”ä¸é‡è¦çš„å…ƒç´ 
            elements_to_check = soup.find_all(['span', 'div'])
            for tag in elements_to_check:
                if not tag or not hasattr(tag, 'get'):
                    continue
                
                try:
                    # è·³è¿‡é‡è¦å®¹å™¨
                    tag_classes = tag.get('class', [])
                    if tag_classes and any(
                        cls in ['common-banner', 'pricing-page-section', 'more-detail'] 
                        for cls in (tag_classes if isinstance(tag_classes, list) else [tag_classes])
                    ):
                        continue
                    
                    # è·³è¿‡æœ‰IDçš„å…ƒç´ 
                    if tag.get('id'):
                        continue
                    
                    # åªåˆ é™¤çœŸæ­£ä¸ºç©ºçš„å…ƒç´ 
                    if self._is_completely_empty(tag):
                        tag.decompose()
                        removed_count += 1
                        
                except Exception as e:
                    print(f"    âš  æ£€æŸ¥ç©ºå…ƒç´ æ—¶å‡ºé”™: {e}")
                    continue
                    
        except Exception as e:
            print(f"    âš  æ¸…ç†ç©ºå…ƒç´ è¿‡ç¨‹å‡ºé”™: {e}")
        
        return removed_count
    
    def _is_completely_empty(self, element: Tag) -> bool:
        """åˆ¤æ–­å…ƒç´ æ˜¯å¦å®Œå…¨ä¸ºç©º"""
        try:
            if not isinstance(element, Tag) or not element:
                return False
            
            # æœ‰ä»»ä½•æ–‡æœ¬å†…å®¹å°±ä¸æ˜¯ç©ºçš„
            if element.get_text(strip=True):
                return False
            
            # åŒ…å«ä»»ä½•å­å…ƒç´ å°±ä¸æ˜¯ç©ºçš„
            if element.find_all():
                return False
            
            return True
            
        except Exception as e:
            print(f"    âš  åˆ¤æ–­ç©ºå…ƒç´ æ—¶å‡ºé”™: {e}")
            return False
    
    def filter_tables_precisely(self, soup: BeautifulSoup, region: str, 
                               product: str = "Azure Database for MySQL") -> Tuple[int, int, List[str]]:
        """ç²¾ç¡®è¿‡æ»¤è¡¨æ ¼ï¼Œç¡®ä¿ä¿ç•™çš„è¡¨æ ¼å†…å®¹å®Œæ•´"""
        if not self.region_filter:
            return 0, 0, []
        
        self.region_filter.set_active_region(region, product)
        
        all_tables = soup.find_all('table')
        total_tables = len(all_tables)
        
        print(f"ğŸ“Š å¼€å§‹ç²¾ç¡®è¡¨æ ¼è¿‡æ»¤: æ€»è®¡{total_tables}ä¸ªè¡¨æ ¼")
        
        # å…ˆæ ‡è®°è¦ç§»é™¤çš„è¡¨æ ¼ï¼Œä¸è¦ç«‹å³åˆ é™¤
        tables_to_remove = []
        retained_table_ids = []
        
        for table in all_tables:
            table_id = table.get('id', '')
            
            if self.region_filter.should_filter_table(table_id):
                tables_to_remove.append(table)
                print(f"  âœ— æ ‡è®°è¿‡æ»¤: {table_id}")
            else:
                if table_id:
                    retained_table_ids.append(table_id)
                print(f"  âœ“ ä¿ç•™è¡¨æ ¼: {table_id} (è¡Œæ•°: {len(table.find_all('tr'))})")
        
        # æ‰¹é‡ç§»é™¤æ ‡è®°çš„è¡¨æ ¼
        for table in tables_to_remove:
            self._remove_table_and_context(table)
        
        filtered_count = len(tables_to_remove)
        retained_count = total_tables - filtered_count
        
        print(f"ğŸ“Š è¡¨æ ¼è¿‡æ»¤å®Œæˆ: è¿‡æ»¤{filtered_count}ä¸ªï¼Œä¿ç•™{retained_count}ä¸ª")
        
        # éªŒè¯ä¿ç•™çš„è¡¨æ ¼å†…å®¹å®Œæ•´æ€§
        remaining_tables = soup.find_all('table')
        print(f"ğŸ” éªŒè¯: å®é™…ä¿ç•™{len(remaining_tables)}ä¸ªè¡¨æ ¼")
        for table in remaining_tables:
            table_id = table.get('id', 'no-id')
            row_count = len(table.find_all('tr'))
            print(f"  ğŸ“‹ {table_id}: {row_count}è¡Œæ•°æ®")
        
        return filtered_count, retained_count, retained_table_ids
    
    def _remove_table_and_context(self, table: Tag):
        """ç§»é™¤è¡¨æ ¼åŠå…¶ä¸Šä¸‹æ–‡ï¼Œä½†è¦è°¨æ…å¤„ç†"""
        elements_to_remove = [table]
        
        # å‘å‰æŸ¥æ‰¾å¯èƒ½çš„ç›¸å…³æ ‡é¢˜
        current = table.previous_sibling
        search_count = 0
        
        while current and search_count < 3:  # é™åˆ¶æœç´¢èŒƒå›´
            if isinstance(current, Tag):
                if current.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    title_text = current.get_text(strip=True).lower()
                    # åªç§»é™¤æ˜æ˜¾ç›¸å…³çš„æ ‡é¢˜
                    if len(title_text) < 50 and any(
                        keyword in title_text for keyword in ['ç³»åˆ—', 'å±‚çº§', 'tier']
                    ):
                        elements_to_remove.insert(0, current)
                elif current.name == 'p':
                    p_text = current.get_text(strip=True)
                    # åªç§»é™¤å¾ˆçŸ­çš„æè¿°æ€§æ®µè½
                    if len(p_text) < 100:
                        elements_to_remove.insert(0, current)
                    else:
                        break  # é•¿æ®µè½å¾ˆå¯èƒ½æ˜¯é‡è¦å†…å®¹
                elif current.name == 'table':
                    break  # é‡åˆ°å…¶ä»–è¡¨æ ¼ï¼Œåœæ­¢
                search_count += 1
            current = current.previous_sibling
        
        # ç§»é™¤æ ‡è®°çš„å…ƒç´ 
        for element in elements_to_remove:
            if element and hasattr(element, 'decompose'):
                element.decompose()


class FixedHTMLBuilder:
    """ä¿®æ­£ç‰ˆHTMLæ„å»ºå™¨"""
    
    @staticmethod
    def build_complete_html(body_content: str, title: str = "Azure Database for MySQLå®šä»·", 
                           region: str = "ä¸­å›½åŒ—éƒ¨3") -> str:
        """æ„å»ºå®Œæ•´çš„HTMLï¼Œç¡®ä¿æ ¼å¼ä¸å‚è€ƒä¸€è‡´"""
        
        # æ¸…ç†bodyæ ‡ç­¾åŒ…è£…
        if body_content.strip().startswith('<body'):
            start_tag_end = body_content.find('>')
            end_tag_start = body_content.rfind('</body>')
            if start_tag_end != -1 and end_tag_start != -1:
                body_content = body_content[start_tag_end + 1:end_tag_start]
        
        full_title = f"{title} - {region}" if region else title
        
        return f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{full_title}</title>
</head>
<body>
{body_content}
</body>
</html>"""


def extract_mysql_content_fixed(html_file_path: str, region: str = "north-china3", 
                                config_file: str = "soft-category.json") -> str:
    """
    ä¿®æ­£ç‰ˆMySQLå†…å®¹æå–ï¼Œç¡®ä¿è¡¨æ ¼å†…å®¹å®Œæ•´
    
    Args:
        html_file_path: åŸå§‹HTMLæ–‡ä»¶è·¯å¾„
        region: ç›®æ ‡åŒºåŸŸ
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        str: æ¸…ç†åçš„å®Œæ•´HTMLå†…å®¹
    """
    
    print(f"ğŸ”§ å¼€å§‹ä¿®æ­£ç‰ˆMySQLå†…å®¹æå–")
    print(f"ğŸ“ æºæ–‡ä»¶: {html_file_path}")
    print(f"ğŸŒ ç›®æ ‡åŒºåŸŸ: {region}")
    print("=" * 60)
    
    # 1. åŠ è½½HTMLæ–‡ä»¶
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        soup = BeautifulSoup(html_content, 'html.parser')
        print(f"âœ“ æˆåŠŸåŠ è½½HTMLæ–‡ä»¶ï¼Œå¤§å°: {len(html_content):,} å­—ç¬¦")
        
        # ç»Ÿè®¡åŸå§‹è¡¨æ ¼ä¿¡æ¯
        original_tables = soup.find_all('table')
        print(f"ğŸ“Š åŸå§‹æ–‡ä»¶åŒ…å« {len(original_tables)} ä¸ªè¡¨æ ¼")
        for i, table in enumerate(original_tables[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            table_id = table.get('id', f'table-{i+1}')
            row_count = len(table.find_all('tr'))
            print(f"  ğŸ“‹ {table_id}: {row_count}è¡Œ")
            
    except Exception as e:
        raise Exception(f"åŠ è½½HTMLæ–‡ä»¶å¤±è´¥: {e}")
    
    # 2. åˆå§‹åŒ–ä¿®æ­£ç‰ˆå¤„ç†å™¨
    region_filter = RegionFilterProcessor(config_file)
    processor = FixedHTMLProcessor(region_filter)
    
    # 3. è°¨æ…æ¸…ç†HTML
    print(f"\nğŸ”§ ç¬¬ä¸€æ­¥ï¼šè°¨æ…æ¸…ç†HTMLç»“æ„")
    cleaned_count = processor.careful_clean_html(soup)
    
    # éªŒè¯æ¸…ç†åçš„è¡¨æ ¼
    after_clean_tables = soup.find_all('table')
    print(f"ğŸ“Š æ¸…ç†åä¿ç•™ {len(after_clean_tables)} ä¸ªè¡¨æ ¼")
    
    # 4. æŒ‰åŒºåŸŸè¿‡æ»¤è¡¨æ ¼
    print(f"\nğŸ“Š ç¬¬äºŒæ­¥ï¼šæŒ‰åŒºåŸŸè¿‡æ»¤è¡¨æ ¼")
    region_names = {
        "north-china": "ä¸­å›½åŒ—éƒ¨",
        "east-china": "ä¸­å›½ä¸œéƒ¨", 
        "north-china2": "ä¸­å›½åŒ—éƒ¨2",
        "east-china2": "ä¸­å›½ä¸œéƒ¨2",
        "north-china3": "ä¸­å›½åŒ—éƒ¨3",
        "east-china3": "ä¸­å›½ä¸œéƒ¨3"
    }
    
    filtered_count, retained_count, retained_table_ids = processor.filter_tables_precisely(
        soup, region, "Azure Database for MySQL"
    )
    
    # 5. æœ€ç»ˆéªŒè¯
    print(f"\nğŸ” ç¬¬ä¸‰æ­¥ï¼šæœ€ç»ˆéªŒè¯")
    final_tables = soup.find_all('table')
    print(f"ğŸ“Š æœ€ç»ˆä¿ç•™ {len(final_tables)} ä¸ªè¡¨æ ¼")
    
    total_rows = 0
    for table in final_tables:
        table_id = table.get('id', 'no-id')
        rows = table.find_all('tr')
        total_rows += len(rows)
        print(f"  ğŸ“‹ {table_id}: {len(rows)}è¡Œæ•°æ®")
        
        # éªŒè¯è¡¨æ ¼å†…å®¹
        if len(rows) < 2:
            print(f"    âš  è­¦å‘Š: è¡¨æ ¼ {table_id} å¯èƒ½ç¼ºå°‘æ•°æ®è¡Œ")
    
    print(f"ğŸ“Š æ€»è®¡æ•°æ®è¡Œ: {total_rows}")
    
    # 6. æ„å»ºæœ€ç»ˆHTML
    print(f"\nğŸ—ï¸ ç¬¬å››æ­¥ï¼šæ„å»ºæœ€ç»ˆHTML")
    region_name = region_names.get(region, region)
    
    body_content = str(soup.body) if soup.body else str(soup)
    final_html = FixedHTMLBuilder.build_complete_html(body_content, region=region_name)
    
    # 7. æœ€ç»ˆç»Ÿè®¡
    print(f"\nâœ… ä¿®æ­£ç‰ˆæå–å®Œæˆï¼")
    print(f"ğŸ“„ æœ€ç»ˆHTMLå¤§å°: {len(final_html):,} å­—ç¬¦")
    print(f"ğŸ“‹ ä¿ç•™è¡¨æ ¼: {retained_count} ä¸ª")
    print(f"ğŸ—‘ï¸ è¿‡æ»¤è¡¨æ ¼: {filtered_count} ä¸ª")
    print(f"ğŸ“Š æ•°æ®è¡Œæ€»æ•°: {total_rows}")
    print(f"ğŸ·ï¸ ä¿ç•™çš„è¡¨æ ¼ID: {retained_table_ids}")
    
    return final_html


# æµ‹è¯•å’ŒéªŒè¯å‡½æ•°
def verify_table_content(html_content: str) -> Dict:
    """éªŒè¯HTMLä¸­çš„è¡¨æ ¼å†…å®¹"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    verification = {
        "table_count": 0,
        "total_rows": 0,
        "tables_with_data": 0,
        "table_details": []
    }
    
    tables = soup.find_all('table')
    verification["table_count"] = len(tables)
    
    for table in tables:
        table_id = table.get('id', 'no-id')
        rows = table.find_all('tr')
        row_count = len(rows)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®è¡Œï¼ˆé™¤äº†è¡¨å¤´ï¼‰
        data_rows = rows[1:] if len(rows) > 1 else []
        has_data = len(data_rows) > 0
        
        verification["total_rows"] += row_count
        if has_data:
            verification["tables_with_data"] += 1
        
        # æ£€æŸ¥è¡¨æ ¼å†…å®¹å®Œæ•´æ€§
        sample_cells = []
        for row in data_rows[:3]:  # æ£€æŸ¥å‰3è¡Œæ•°æ®
            cells = row.find_all(['td', 'th'])
            if cells:
                sample_cells.append([cell.get_text(strip=True) for cell in cells])
        
        verification["table_details"].append({
            "id": table_id,
            "row_count": row_count,
            "data_row_count": len(data_rows),
            "has_data": has_data,
            "sample_data": sample_cells[:2]  # åªä¿ç•™å‰2è¡Œä½œä¸ºæ ·æœ¬
        })
    
    return verification


if __name__ == "__main__":
    # æµ‹è¯•éªŒè¯
    print("ğŸ§ª ä¿®æ­£ç‰ˆHTMLå¤„ç†å™¨æµ‹è¯•")
    print("=" * 40)
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    test_html = """
    <table id="test_table">
        <tr><th>åˆ—1</th><th>åˆ—2</th></tr>
        <tr><td>æ•°æ®1</td><td>æ•°æ®2</td></tr>
        <tr><td>æ•°æ®3</td><td>æ•°æ®4</td></tr>
    </table>
    """
    
    verification = verify_table_content(test_html)
    print(f"æµ‹è¯•ç»“æœ: {verification}")
    print("âœ… ä¿®æ­£ç‰ˆå¤„ç†å™¨å‡†å¤‡å°±ç»ª")