#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MySQLå®šä»·é¡µé¢å†…å®¹æå–è„šæœ¬
æ”¯æŒå¤šåŒºåŸŸã€é…ç½®åŒ–ã€æ¨¡å—åŒ–å¤„ç†ã€tabå®¹å™¨å±•å¼€ã€å†…å®¹æ¸…ç†
"""

import json
import os
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup

# å¯¼å…¥è‡ªå®šä¹‰å·¥å…·æ¨¡å—
try:
    from utils.html_processor import (
        RegionFilterProcessor, 
        HTMLProcessor, 
        HTMLBuilder,
        validate_html_structure
    )
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥utilsæ¨¡å—ï¼Œè¯·ç¡®ä¿utils/html_processor.pyå­˜åœ¨")
    print("æç¤º: å°†æ”¹è¿›çš„HTMLå¤„ç†å·¥å…·ä¿å­˜ä¸º utils/html_processor.py")
    exit(1)


class MySQLHTMLExtractor:
    """MySQL HTMLæå–å™¨ v2.0"""
    
    def __init__(self, config_file: str = "soft-category.json", output_dir: str = "output"):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            config_file: åŒºåŸŸé…ç½®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.config_file = config_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.region_filter = RegionFilterProcessor(config_file)
        self.html_processor = HTMLProcessor(self.region_filter)
        
        # æ”¯æŒçš„åŒºåŸŸæ˜ å°„
        self.region_names = {
            "north-china": "ä¸­å›½åŒ—éƒ¨",
            "east-china": "ä¸­å›½ä¸œéƒ¨",
            "north-china2": "ä¸­å›½åŒ—éƒ¨2",
            "east-china2": "ä¸­å›½ä¸œéƒ¨2", 
            "north-china3": "ä¸­å›½åŒ—éƒ¨3",
            "east-china3": "ä¸­å›½ä¸œéƒ¨3"
        }
        
        # äº§å“ä¿¡æ¯
        self.product_info = {
            "name": "Azure Database for MySQL",
            "name_en": "Azure Database for MySQL",
            "description": "é¢å‘åº”ç”¨å¼€å‘äººå‘˜çš„æ‰˜ç®¡ MySQL æ•°æ®åº“æœåŠ¡"
        }
    
    def load_html_file(self, html_file_path: str) -> BeautifulSoup:
        """
        åŠ è½½HTMLæ–‡ä»¶
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            BeautifulSoup: è§£æåçš„HTMLå¯¹è±¡
        """
        if not os.path.exists(html_file_path):
            raise FileNotFoundError(f"HTMLæ–‡ä»¶ä¸å­˜åœ¨: {html_file_path}")
        
        try:
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            print(f"âœ“ æˆåŠŸåŠ è½½HTMLæ–‡ä»¶: {html_file_path}")
            print(f"  æ–‡ä»¶å¤§å°: {len(html_content):,} å­—ç¬¦")
            
            # æ˜¾ç¤ºåŸå§‹HTMLç»“æ„ä¿¡æ¯
            self._print_html_structure_info(soup, "åŸå§‹HTMLç»“æ„")
            
            return soup
            
        except Exception as e:
            raise Exception(f"åŠ è½½HTMLæ–‡ä»¶å¤±è´¥: {e}")
    
    def _print_html_structure_info(self, soup: BeautifulSoup, title: str):
        """æ‰“å°HTMLç»“æ„ä¿¡æ¯"""
        print(f"\nğŸ“‹ {title}:")
        print(f"  æ€»å…ƒç´ æ•°: {len(soup.find_all()):,}")
        print(f"  è¡¨æ ¼æ•°é‡: {len(soup.find_all('table'))}")
        print(f"  tab-content: {len(soup.find_all('div', class_='tab-content'))}")
        print(f"  tab-panel: {len(soup.find_all('div', class_='tab-panel'))}")
        print(f"  æ ‡é¢˜æ•°é‡: {len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']))}")
        print(f"  æ®µè½æ•°é‡: {len(soup.find_all('p'))}")
    
    def extract_for_region(self, html_file_path: str, region: str, 
                          validate: bool = True, 
                          keep_structure: bool = False) -> Dict:
        """
        ä¸ºæŒ‡å®šåŒºåŸŸæå–HTMLå†…å®¹
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            region: ç›®æ ‡åŒºåŸŸID
            validate: æ˜¯å¦éªŒè¯HTMLç»“æ„
            keep_structure: æ˜¯å¦ä¿ç•™åŸå§‹ç»“æ„ï¼ˆè°ƒè¯•ç”¨ï¼‰
            
        Returns:
            Dict: æå–ç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹æå– {self.region_names.get(region, region)} çš„MySQLå®šä»·å†…å®¹")
        print("=" * 60)
        
        # 1. åŠ è½½HTML
        soup = self.load_html_file(html_file_path)
        original_soup = BeautifulSoup(str(soup), 'html.parser')  # ä¿å­˜åŸå§‹å‰¯æœ¬ç”¨äºç»Ÿè®¡
        
        # 2. ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼ˆåŒ…æ‹¬tabå±•å¼€ï¼‰
        print("\nğŸ“ ç¬¬ä¸€æ­¥ï¼šæ¸…ç†ä¸éœ€è¦çš„HTMLå…ƒç´ å’Œå±•å¼€tabç»“æ„")
        removed_count = self.html_processor.remove_unwanted_elements(soup)
        print(f"âœ“ å¤„ç†äº† {removed_count} ä¸ªå…ƒç´ /å®¹å™¨")
        
        # æ˜¾ç¤ºå±•å¼€åçš„ç»“æ„
        self._print_html_structure_info(soup, "tabå±•å¼€åçš„HTMLç»“æ„")
        
        # 3. æŒ‰åŒºåŸŸè¿‡æ»¤è¡¨æ ¼
        print(f"\nğŸ“Š ç¬¬äºŒæ­¥ï¼šæŒ‰åŒºåŸŸ '{region}' è¿‡æ»¤è¡¨æ ¼")
        total_original_tables = len(original_soup.find_all('table'))
        filtered_count, retained_count, retained_table_ids = self.html_processor.filter_tables_by_region(
            soup, region, self.product_info["name"]
        )
        
        # 4. æ¸…ç†HTMLå±æ€§
        print("\nğŸ§¹ ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†HTMLå±æ€§")
        cleaned_attrs = self.html_processor.clean_attributes(soup)
        print(f"âœ“ æ¸…ç†äº† {cleaned_attrs} ä¸ªä¸éœ€è¦çš„å±æ€§")
        
        # 5. éªŒè¯HTMLç»“æ„ï¼ˆå¯é€‰ï¼‰
        validation_issues = []
        if validate:
            print("\nğŸ” ç¬¬å››æ­¥ï¼šéªŒè¯HTMLç»“æ„")
            validation_issues = validate_html_structure(soup)
            if validation_issues:
                print("âš  å‘ç°ä»¥ä¸‹é—®é¢˜:")
                for issue in validation_issues:
                    print(f"  - {issue}")
            else:
                print("âœ“ HTMLç»“æ„éªŒè¯é€šè¿‡")
        
        # 6. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“ˆ ç¬¬äº”æ­¥ï¼šç”Ÿæˆç»Ÿè®¡ä¿¡æ¯")
        statistics = self.html_processor.generate_statistics(
            soup, retained_table_ids, filtered_count, total_original_tables
        )
        
        # 7. æ„å»ºæœ€ç»ˆHTML
        print("\nğŸ—ï¸ ç¬¬å…­æ­¥ï¼šæ„å»ºæœ€ç»ˆHTML")
        region_name = self.region_names.get(region, region)
        title = f"{self.product_info['name']}å®šä»·"
        
        # è·å–bodyå†…å®¹
        body_content = str(soup.body) if soup.body else str(soup)
        cleaned_html = HTMLBuilder.build_clean_html(body_content, title, region_name)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æ„
        final_soup = BeautifulSoup(cleaned_html, 'html.parser')
        self._print_html_structure_info(final_soup, "æœ€ç»ˆHTMLç»“æ„")
        
        # 8. ç»„è£…ç»“æœ
        result = {
            "success": True,
            "region": {
                "id": region,
                "name": region_name
            },
            "product": self.product_info,
            "processing_info": {
                "original_file": html_file_path,
                "processed_at": datetime.now().isoformat(),
                "removed_elements": removed_count,
                "cleaned_attributes": cleaned_attrs,
                "validation_issues": validation_issues,
                "tab_containers_flattened": True
            },
            "content": {
                "html": cleaned_html,
                "length": len(cleaned_html)
            },
            "statistics": statistics,
            "extracted_sections": self.html_processor.extract_content_sections(soup)
        }
        
        print(f"\nâœ… æå–å®Œæˆï¼")
        print(f"ğŸ“„ æœ€ç»ˆHTMLå¤§å°: {len(cleaned_html):,} å­—ç¬¦")
        print(f"ğŸ“‹ ä¿ç•™è¡¨æ ¼: {retained_count} ä¸ª")
        print(f"ğŸ—‘ï¸ è¿‡æ»¤è¡¨æ ¼: {filtered_count} ä¸ª")
        print(f"ğŸ“‚ Tabå®¹å™¨å·²å±•å¼€ï¼Œå†…å®¹å·²å¹³é“º")
        
        return result
    
    def save_result(self, result: Dict, region: str, save_sections: bool = False) -> Tuple[str, str]:
        """
        ä¿å­˜æå–ç»“æœ
        
        Args:
            result: æå–ç»“æœ
            region: åŒºåŸŸID
            save_sections: æ˜¯å¦ä¿å­˜ç‹¬ç«‹çš„å†…å®¹åŒºå—æ–‡ä»¶
            
        Returns:
            Tuple[str, str]: (HTMLæ–‡ä»¶è·¯å¾„, æŠ¥å‘Šæ–‡ä»¶è·¯å¾„)
        """
        region_name = self.region_names.get(region, region)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜HTMLæ–‡ä»¶
        html_filename = f"mysql_{region}_extracted_v2_{timestamp}.html"
        html_path = self.output_dir / html_filename
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(result["content"]["html"])
        
        # ä¿å­˜ç‹¬ç«‹çš„å†…å®¹åŒºå—ï¼ˆå¯é€‰ï¼‰
        if save_sections and "extracted_sections" in result:
            sections_dir = self.output_dir / f"mysql_{region}_sections_{timestamp}"
            sections_dir.mkdir(exist_ok=True)
            
            for section_name, section_soup in result["extracted_sections"].items():
                if section_soup:
                    section_file = sections_dir / f"{section_name}.html"
                    with open(section_file, 'w', encoding='utf-8') as f:
                        f.write(str(section_soup))
            
            print(f"ğŸ“ å†…å®¹åŒºå—å·²ä¿å­˜åˆ°: {sections_dir}")
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        report_filename = f"mysql_{region}_report_v2_{timestamp}.json"
        report_path = self.output_dir / report_filename
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®ï¼ˆç§»é™¤HTMLå†…å®¹ä»¥å‡å°æ–‡ä»¶å¤§å°ï¼‰
        report_data = result.copy()
        del report_data["content"]["html"]  # ç§»é™¤HTMLå†…å®¹
        # è½¬æ¢BeautifulSoupå¯¹è±¡ä¸ºå­—ç¬¦ä¸²
        if "extracted_sections" in report_data:
            report_data["extracted_sections"] = {
                k: str(v) if v else None 
                for k, v in report_data["extracted_sections"].items()
            }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ–‡ä»¶ä¿å­˜å®Œæˆ:")
        print(f"ğŸ“„ HTMLæ–‡ä»¶: {html_path}")
        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {report_path}")
        
        return str(html_path), str(report_path)
    
    def extract_all_regions(self, html_file_path: str, regions: Optional[List[str]] = None, 
                           save_sections: bool = False) -> Dict[str, Dict]:
        """
        æå–æ‰€æœ‰åŒºåŸŸçš„å†…å®¹
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            regions: è¦æå–çš„åŒºåŸŸåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæå–æ‰€æœ‰æ”¯æŒçš„åŒºåŸŸ
            save_sections: æ˜¯å¦ä¿å­˜ç‹¬ç«‹çš„å†…å®¹åŒºå—æ–‡ä»¶
            
        Returns:
            Dict[str, Dict]: æ¯ä¸ªåŒºåŸŸçš„æå–ç»“æœ
        """
        if regions is None:
            # è·å–é…ç½®æ–‡ä»¶ä¸­æ”¯æŒçš„åŒºåŸŸ
            available_regions = self.region_filter.get_available_regions(self.product_info["name"])
            regions = [r for r in available_regions if r in self.region_names]
        
        print(f"\nğŸŒ å¼€å§‹æ‰¹é‡æå– {len(regions)} ä¸ªåŒºåŸŸçš„å†…å®¹")
        print(f"åŒºåŸŸåˆ—è¡¨: {[self.region_names.get(r, r) for r in regions]}")
        
        results = {}
        
        for i, region in enumerate(regions, 1):
            print(f"\n{'='*60}")
            print(f"å¤„ç†åŒºåŸŸ {i}/{len(regions)}: {self.region_names.get(region, region)} ({region})")
            print(f"{'='*60}")
            
            try:
                result = self.extract_for_region(html_file_path, region)
                html_path, report_path = self.save_result(result, region, save_sections)
                
                results[region] = {
                    "result": result,
                    "files": {
                        "html": html_path,
                        "report": report_path
                    }
                }
                
                print(f"âœ… {self.region_names.get(region, region)} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {self.region_names.get(region, region)} å¤„ç†å¤±è´¥: {e}")
                import traceback
                print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                results[region] = {
                    "error": str(e),
                    "files": {}
                }
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self._generate_summary_report(results)
        
        return results
    
    def _generate_summary_report(self, results: Dict[str, Dict]):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_path = self.output_dir / f"mysql_extraction_summary_v2_{timestamp}.json"
        
        summary = {
            "extraction_info": {
                "version": "2.0",
                "extraction_time": datetime.now().isoformat(),
                "features": [
                    "tabå®¹å™¨å±•å¼€",
                    "æ™ºèƒ½å†…å®¹æ¸…ç†", 
                    "åŒºåŸŸè¡¨æ ¼è¿‡æ»¤",
                    "HTMLç»“æ„éªŒè¯"
                ]
            },
            "statistics": {
                "total_regions": len(results),
                "successful_regions": len([r for r in results.values() if "result" in r]),
                "failed_regions": len([r for r in results.values() if "error" in r])
            },
            "regions": {}
        }
        
        for region, data in results.items():
            region_name = self.region_names.get(region, region)
            
            if "result" in data:
                stats = data["result"]["statistics"]
                summary["regions"][region] = {
                    "name": region_name,
                    "status": "success",
                    "tables_retained": stats["è¡¨æ ¼ä¿¡æ¯"]["ä¿ç•™è¡¨æ ¼æ•°"],
                    "tables_filtered": stats["è¡¨æ ¼ä¿¡æ¯"]["è¿‡æ»¤è¡¨æ ¼æ•°"],
                    "html_size": data["result"]["content"]["length"],
                    "content_blocks": stats["å†…å®¹åŒºå—"],
                    "files": data["files"]
                }
            else:
                summary["regions"][region] = {
                    "name": region_name,
                    "status": "failed",
                    "error": data["error"]
                }
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“‹ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ¯ æ‰¹é‡æå–æ€»ç»“:")
        print(f"âœ… æˆåŠŸ: {summary['statistics']['successful_regions']} ä¸ªåŒºåŸŸ")
        print(f"âŒ å¤±è´¥: {summary['statistics']['failed_regions']} ä¸ªåŒºåŸŸ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æ‰“å°æˆåŠŸåŒºåŸŸçš„è¯¦ç»†ä¿¡æ¯
        if summary['statistics']['successful_regions'] > 0:
            print(f"\nğŸ“Š æˆåŠŸåŒºåŸŸè¯¦æƒ…:")
            for region, info in summary["regions"].items():
                if info["status"] == "success":
                    print(f"  {info['name']}: {info['tables_retained']}ä¸ªè¡¨æ ¼, {info['html_size']:,}å­—ç¬¦")
    
    def compare_regions(self, html_file_path: str) -> Dict:
        """
        å¯¹æ¯”æ‰€æœ‰åŒºåŸŸçš„è¡¨æ ¼å·®å¼‚
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: åŒºåŸŸå¯¹æ¯”ç»“æœ
        """
        print(f"\nğŸ” å¼€å§‹åŒºåŸŸå¯¹æ¯”åˆ†æ...")
        
        available_regions = self.region_filter.get_available_regions(self.product_info["name"])
        comparison = {
            "analysis_time": datetime.now().isoformat(),
            "total_regions": len(available_regions),
            "regions": {}
        }
        
        for region in available_regions:
            if region in self.region_names:
                excluded_tables = self.region_filter.get_excluded_tables_for_region(
                    region, self.product_info["name"]
                )
                
                comparison["regions"][region] = {
                    "name": self.region_names[region],
                    "excluded_tables_count": len(excluded_tables),
                    "excluded_tables": excluded_tables
                }
        
        # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        comparison_path = self.output_dir / f"mysql_regions_comparison_{timestamp}.json"
        
        with open(comparison_path, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ åŒºåŸŸå¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {comparison_path}")
        
        # æ‰“å°å¯¹æ¯”æ‘˜è¦
        print(f"\nğŸ“Š åŒºåŸŸå¯¹æ¯”æ‘˜è¦:")
        for region, info in comparison["regions"].items():
            print(f"  {info['name']}: æ’é™¤{info['excluded_tables_count']}ä¸ªè¡¨æ ¼")
        
        return comparison


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description="MySQLå®šä»·é¡µé¢HTMLæå–å·¥å…· v2.0")
    parser.add_argument("html_file", help="HTMLæºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-r", "--region", help="ç›®æ ‡åŒºåŸŸ (å¦‚: north-china3)")
    parser.add_argument("-a", "--all-regions", action="store_true", help="æå–æ‰€æœ‰åŒºåŸŸ")
    parser.add_argument("-c", "--config", default="soft-category.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--no-validate", action="store_true", help="è·³è¿‡HTMLç»“æ„éªŒè¯")
    parser.add_argument("--save-sections", action="store_true", help="ä¿å­˜ç‹¬ç«‹çš„å†…å®¹åŒºå—æ–‡ä»¶")
    parser.add_argument("--compare-regions", action="store_true", help="å¯¹æ¯”åŒºåŸŸå·®å¼‚")
    parser.add_argument("--keep-structure", action="store_true", help="ä¿ç•™åŸå§‹ç»“æ„ï¼ˆè°ƒè¯•ç”¨ï¼‰")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.html_file):
        print(f"âŒ HTMLæ–‡ä»¶ä¸å­˜åœ¨: {args.html_file}")
        return 1
    
    # éªŒè¯é…ç½®æ–‡ä»¶
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return 1
    
    try:
        # åˆ›å»ºæå–å™¨
        extractor = MySQLHTMLExtractor(args.config, args.output)
        
        # æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
        print("ğŸš€ MySQLå®šä»·é¡µé¢HTMLæå–å·¥å…· v2.0")
        print("âœ¨ æ–°ç‰¹æ€§: tabå®¹å™¨å±•å¼€ã€æ™ºèƒ½å†…å®¹æ¸…ç†ã€å¢å¼ºéªŒè¯")
        
        if args.compare_regions:
            # åŒºåŸŸå¯¹æ¯”åˆ†æ
            extractor.compare_regions(args.html_file)
            
        elif args.all_regions:
            # æå–æ‰€æœ‰åŒºåŸŸ
            results = extractor.extract_all_regions(args.html_file, save_sections=args.save_sections)
            
        elif args.region:
            # æå–æŒ‡å®šåŒºåŸŸ
            if args.region not in extractor.region_names:
                print(f"âŒ ä¸æ”¯æŒçš„åŒºåŸŸ: {args.region}")
                print(f"æ”¯æŒçš„åŒºåŸŸ: {list(extractor.region_names.keys())}")
                return 1
            
            result = extractor.extract_for_region(
                args.html_file, 
                args.region, 
                validate=not args.no_validate,
                keep_structure=args.keep_structure
            )
            extractor.save_result(result, args.region, args.save_sections)
            
        else:
            # é»˜è®¤æå– north-china3
            print("æœªæŒ‡å®šåŒºåŸŸï¼Œé»˜è®¤æå– north-china3")
            result = extractor.extract_for_region(
                args.html_file, 
                "north-china3", 
                validate=not args.no_validate,
                keep_structure=args.keep_structure
            )
            extractor.save_result(result, "north-china3", args.save_sections)
        
        print("\nğŸ‰ æå–ä»»åŠ¡å®Œæˆï¼")
        print("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„HTMLæ–‡ä»¶ï¼Œç¡®è®¤tabå†…å®¹å·²æ­£ç¡®å±•å¼€")
        return 0
        
    except Exception as e:
        print(f"âŒ æå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())