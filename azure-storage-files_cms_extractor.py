#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Azure Storage Filesé¡µé¢CMSå¯¼å…¥HTMLæå–å™¨
åŸºäºMySQLæå–å™¨æ¶æ„ï¼Œä¸“é—¨å¤„ç†Azure Filesäº§å“é¡µé¢çš„å†…å®¹æå–å’ŒåŒºåŸŸåŒ–å¤„ç†
ä¿®æ”¹ç‰ˆï¼šä¿ç•™é‡è¦çš„sectionæ ‡é¢˜
"""

import json
import os
import argparse
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup, Comment, NavigableString, Tag

# å¯¼å…¥ç°æœ‰çš„å¤„ç†æ¨¡å—
try:
    from utils.enhanced_html_processor import (
        RegionFilterProcessor, 
        FixedHTMLProcessor, 
        verify_table_content
    )
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥HTMLå¤„ç†å™¨ï¼Œè¯·ç¡®ä¿utils/enhanced_html_processor.pyå­˜åœ¨")
    exit(1)


class AzureStorageFilesCMSExtractor:
    """Azure Storage Filesé¡µé¢CMS HTMLæå–å™¨"""
    
    def __init__(self, config_file: str = "soft-category.json", output_dir: str = "storage_files_output"):
        self.config_file = config_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åŒºåŸŸæ˜ å°„
        self.region_names = {
            "north-china": "ä¸­å›½åŒ—éƒ¨",
            "east-china": "ä¸­å›½ä¸œéƒ¨",
            "north-china2": "ä¸­å›½åŒ—éƒ¨2", 
            "east-china2": "ä¸­å›½ä¸œéƒ¨2",
            "north-china3": "ä¸­å›½åŒ—éƒ¨3",
            "east-china3": "ä¸­å›½ä¸œéƒ¨3"
        }
        
        # é‡è¦çš„sectionæ ‡é¢˜ï¼Œéœ€è¦ä¿ç•™
        self.important_section_titles = {
            "å®šä»·è¯¦ç»†ä¿¡æ¯", "å®šä»·è¯¦æƒ…", "pricing details",
            "äº†è§£å­˜å‚¨é€‰é¡¹", "å­˜å‚¨é€‰é¡¹", "storage options",
            "æ•°æ®å­˜å‚¨ä»·æ ¼", "å­˜å‚¨ä»·æ ¼", "data storage pricing", "storage pricing",
            "äº‹åŠ¡å’Œæ•°æ®ä¼ è¾“ä»·æ ¼", "äº‹åŠ¡ä»·æ ¼", "transaction pricing", "æ•°æ®ä¼ è¾“ä»·æ ¼",
            "æ–‡ä»¶åŒæ­¥ä»·æ ¼", "åŒæ­¥ä»·æ ¼", "file sync pricing",
            "å¸¸è§é—®é¢˜", "faq", "frequently asked questions"
        }
        
        # å­˜å‚¨å†—ä½™ç±»å‹æ ‡é¢˜ï¼Œä¹Ÿæ˜¯é‡è¦çš„sectionæ ‡é¢˜
        self.storage_redundancy_titles = {
            "lrs", "grs", "zrs", "ragrs", "gzrs", "ra-grs",
            "æœ¬åœ°å†—ä½™å­˜å‚¨", "åœ°ç†å†—ä½™å­˜å‚¨", "åŒºåŸŸå†—ä½™å­˜å‚¨", 
            "è¯»å–è®¿é—®åœ°ç†å†—ä½™å­˜å‚¨", "åœ°ç†åŒºåŸŸå†—ä½™å­˜å‚¨"
        }
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.region_filter = RegionFilterProcessor(config_file)
        self.html_processor = FixedHTMLProcessor(self.region_filter)
        self.original_soup = None  # ç”¨äºä¿å­˜åŸå§‹HTMLçš„soupå¯¹è±¡
        
        print(f"âœ“ Azure Storage Files CMSæå–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸŒ æ”¯æŒåŒºåŸŸ: {list(self.region_names.keys())}")
    
    def extract_cms_html_for_region(self, html_file_path: str, region: str) -> Dict[str, any]:
        """ä¸ºæŒ‡å®šåŒºåŸŸæå–CMSå‹å¥½çš„HTML"""
        
        print(f"\nğŸ”§ å¼€å§‹æå–Azure Storage Files CMS HTML")
        print(f"ğŸ“ æºæ–‡ä»¶: {html_file_path}")
        print(f"ğŸŒ ç›®æ ‡åŒºåŸŸ: {region} ({self.region_names.get(region, region)})")
        print("=" * 70)
        
        if not os.path.exists(html_file_path):
            raise FileNotFoundError(f"HTMLæ–‡ä»¶ä¸å­˜åœ¨: {html_file_path}")
        
        if region not in self.region_names:
            raise ValueError(f"ä¸æ”¯æŒçš„åŒºåŸŸ: {region}ã€‚æ”¯æŒçš„åŒºåŸŸ: {list(self.region_names.keys())}")
        
        start_time = datetime.now()
        
        try:
            # 1. åŠ è½½å’Œè§£æHTML
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            # ä¿å­˜åŸå§‹soupçš„å¼•ç”¨ï¼Œç”¨äºæå–metaä¿¡æ¯
            self.original_soup = BeautifulSoup(html_content, 'html.parser')
            print(f"âœ“ HTMLæ–‡ä»¶åŠ è½½æˆåŠŸï¼Œå¤§å°: {len(html_content):,} å­—ç¬¦")
            
            # 2. è®¾ç½®åŒºåŸŸè¿‡æ»¤å™¨
            self.region_filter.set_active_region(region, "Storage Files")
            
            # 3. æå–å’Œæ¸…æ´—å†…å®¹
            cleaned_soup = self._extract_and_clean_files_content(soup, region)
            
            # 4. åº”ç”¨åŒºåŸŸè¿‡æ»¤
            filtered_count, retained_count = self._apply_region_filtering(cleaned_soup, region)
            
            # 5. è¿›ä¸€æ­¥æ¸…æ´—ä»¥é€‚åº”CMS
            cms_ready_soup = self._prepare_for_cms(cleaned_soup, region)
            
            # 6. ç”Ÿæˆæœ€ç»ˆHTML
            final_html = self._build_final_html(cms_ready_soup, region)
            
            # 7. éªŒè¯ç»“æœ
            verification = self._verify_extraction_result(final_html)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                "success": True,
                "region": {
                    "id": region,
                    "name": self.region_names[region]
                },
                "html_content": final_html,
                "statistics": {
                    "original_size": len(html_content),
                    "final_size": len(final_html),
                    "compression_ratio": round(len(final_html) / len(html_content), 3),
                    "filtered_tables": filtered_count,
                    "retained_tables": retained_count,
                    "processing_time": processing_time
                },
                "verification": verification,
                "extraction_info": {
                    "source_file": html_file_path,
                    "extracted_at": start_time.isoformat(),
                    "extraction_method": "cms_optimized_storage_files",
                    "version": "1.2_files_cms_with_redundancy_titles"
                }
            }
            
            print(f"\nâœ… Azure Storage Files CMS HTMLæå–å®Œæˆï¼")
            print(f"ğŸ“„ å‹ç¼©æ¯”: {result['statistics']['compression_ratio']*100:.1f}%")
            print(f"ğŸ“Š ä¿ç•™è¡¨æ ¼: {retained_count} ä¸ª")
            print(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e),
                "region": {"id": region, "name": self.region_names.get(region, region)}
            }
    
    def _extract_and_clean_files_content(self, soup: BeautifulSoup, region: str) -> BeautifulSoup:
        """æå–å’Œæ¸…æ´—Azure Filesç‰¹å®šå†…å®¹ï¼Œä¿ç•™å®Œæ•´çš„é¡µé¢å†…å®¹"""
        
        print("ğŸ§¹ ç¬¬ä¸€æ­¥ï¼šæå–å’Œæ¸…æ´—Azure Fileså®Œæ•´å†…å®¹...")
        
        # å¤åˆ¶æ•´ä¸ªsoupï¼Œç„¶åè¿›è¡Œæ¸…æ´—
        cleaned_soup = BeautifulSoup(str(soup), 'html.parser')
        
        # 1. ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼ˆä½†ä¿ç•™æ‰€æœ‰å†…å®¹ï¼‰
        self._remove_unwanted_elements(cleaned_soup)
        
        # 2. æ¸…æ´—æ ·å¼å’Œè„šæœ¬
        self._clean_styles_and_scripts(cleaned_soup)
        
        # 3. ç§»é™¤å¯¼èˆªå’Œäº¤äº’å…ƒç´ 
        self._remove_navigation_elements(cleaned_soup)
        
        # 4. å±•å¼€å’Œæ¸…ç†tabç»“æ„ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹
        self._flatten_tab_structures(cleaned_soup)
        
        # 5. æ¸…ç†å±æ€§ä½†ä¿ç•™å†…å®¹
        self._clean_attributes_keep_content(cleaned_soup)
        
        # 6. æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = self._extract_files_main_content_area(cleaned_soup)
        
        print("  âœ“ Azure Fileså®Œæ•´å†…å®¹æå–å’Œæ¸…æ´—å®Œæˆ")
        
        return main_content
    
    def _remove_unwanted_elements(self, soup: BeautifulSoup):
        """ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ """
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼ç›¸å…³
        unwanted_selectors = [
            'script', 'noscript', 'style', 'link[rel="stylesheet"]',
            'meta[http-equiv]', 'meta[name="viewport"]'
        ]
        
        for selector in unwanted_selectors:
            for element in soup.select(selector):
                element.decompose()
        
        # ç§»é™¤æ³¨é‡Š
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
    
    def _clean_styles_and_scripts(self, soup: BeautifulSoup):
        """æ¸…ç†æ ·å¼å’Œè„šæœ¬"""
        
        # ç§»é™¤å†…è”æ ·å¼å±æ€§
        for tag in soup.find_all():
            if tag.get('style'):
                del tag['style']
            
            # ç§»é™¤äº‹ä»¶å¤„ç†å™¨
            attrs_to_remove = []
            for attr in tag.attrs:
                if attr.startswith('on'):  # onclick, onload, etc.
                    attrs_to_remove.append(attr)
            
            for attr in attrs_to_remove:
                del tag[attr]
    
    def _remove_navigation_elements(self, soup: BeautifulSoup):
        """ç§»é™¤å¯¼èˆªå’Œäº¤äº’å…ƒç´ """
        
        # ç§»é™¤å¯¼èˆªç›¸å…³çš„classå’Œå…ƒç´ 
        navigation_classes = [
            'bread-crumb', 'left-navigation-select', 'documentation-navigation',
            'acn-header-container', 'public_footerpage', 'region-container',
            'software-kind-container', 'dropdown-container', 'dropdown-box'
        ]
        
        for class_name in navigation_classes:
            for element in soup.find_all(class_=class_name):
                element.decompose()
        
        # ç§»é™¤å¯¼èˆªæ ‡ç­¾
        for tag in soup.find_all(['nav', 'header', 'footer']):
            tag.decompose()
        
        # ç§»é™¤è¡¨å•å…ƒç´ 
        for tag in soup.find_all(['form', 'input', 'select', 'option', 'button', 'textarea']):
            tag.decompose()
    
    def _flatten_tab_structures(self, soup: BeautifulSoup):
        """å±•å¼€tabç»“æ„ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹"""
        
        print("    ğŸ“‚ å±•å¼€Azure Files tabç»“æ„ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹...")
        
        # ç§»é™¤tabå¯¼èˆªï¼Œä½†ä¿ç•™å†…å®¹
        for tab_nav in soup.find_all('ul', class_='tab-nav'):
            tab_nav.decompose()
        
        # å±•å¼€tabå†…å®¹é¢æ¿
        tab_panels = soup.find_all('div', class_='tab-panel')
        
        for panel in tab_panels:
            # å°†é¢æ¿å†…å®¹æå‡åˆ°çˆ¶çº§
            if panel.parent:
                # è·å–é¢æ¿ä¸­çš„æ‰€æœ‰å­å…ƒç´ 
                children = list(panel.children)
                
                # å°†å­å…ƒç´ æ’å…¥åˆ°é¢æ¿çš„ä½ç½®
                for child in children:
                    if hasattr(child, 'extract'):
                        child.extract()
                        panel.insert_before(child)
                
                # ç§»é™¤ç©ºçš„é¢æ¿å®¹å™¨
                panel.decompose()
        
        # å¤„ç†å…¶ä»–å¯èƒ½çš„tabå®¹å™¨
        tab_containers = soup.find_all('div', class_=lambda x: x and any(
            keyword in ' '.join(x) for keyword in ['tab-content', 'tab-container', 'technical-azure-selector']
        ))
        
        for container in tab_containers:
            if container.parent:
                children = list(container.children)
                for child in children:
                    if hasattr(child, 'extract'):
                        child.extract()
                        container.insert_before(child)
                container.decompose()
    
    def _clean_attributes_keep_content(self, soup: BeautifulSoup):
        """æ¸…ç†å±æ€§ä½†ä¿ç•™å†…å®¹ç»“æ„"""
        
        # è¦ä¿ç•™çš„é‡è¦å±æ€§
        important_attrs = {'id', 'class', 'href', 'src', 'alt', 'title', 'colspan', 'rowspan'}
        
        # è¦ä¿ç•™çš„é‡è¦classï¼ˆä¼šè¿›ä¸€æ­¥è¿‡æ»¤ï¼‰
        important_classes = {
            'common-banner', 'common-banner-image', 'common-banner-title',
            'pricing-page-section', 'more-detail', 'storage-specific-content'
        }
        
        for tag in soup.find_all():
            if not hasattr(tag, 'attrs'):
                continue
                
            # ç§»é™¤ä¸é‡è¦çš„å±æ€§
            attrs_to_remove = []
            for attr in tag.attrs:
                if attr not in important_attrs:
                    attrs_to_remove.append(attr)
            
            for attr in attrs_to_remove:
                del tag[attr]
            
            # è¿‡æ»¤classå±æ€§
            if tag.get('class'):
                current_classes = tag['class'] if isinstance(tag['class'], list) else [tag['class']]
                filtered_classes = [cls for cls in current_classes if cls in important_classes]
                if filtered_classes:
                    tag['class'] = filtered_classes
                else:
                    if 'class' in tag.attrs:
                        del tag['class']
    
    def _is_important_section_title(self, element: Tag) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡è¦çš„sectionæ ‡é¢˜ï¼Œéœ€è¦ä¿ç•™"""
        
        if not element or not hasattr(element, 'get_text'):
            return False
        
        title_text = element.get_text(strip=True).lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…é‡è¦çš„sectionæ ‡é¢˜
        for important_title in self.important_section_titles:
            if important_title.lower() in title_text:
                print(f"    âœ“ ä¿ç•™é‡è¦sectionæ ‡é¢˜: {title_text}")
                return True
        
        # ç‰¹åˆ«æ£€æŸ¥å­˜å‚¨å†—ä½™ç±»å‹æ ‡é¢˜ï¼ˆå¦‚ LRS, GRS ç­‰ï¼‰
        # è¿™äº›é€šå¸¸æ˜¯å•ç‹¬çš„çŸ­æ ‡é¢˜ï¼Œéœ€è¦ç²¾ç¡®åŒ¹é…
        if len(title_text) <= 10:  # å¾ˆçŸ­çš„æ ‡é¢˜
            for redundancy_type in self.storage_redundancy_titles:
                if title_text == redundancy_type.lower():
                    print(f"    âœ“ ä¿ç•™å­˜å‚¨å†—ä½™ç±»å‹æ ‡é¢˜: {title_text}")
                    return True
        
        return False
    
    def _extract_files_main_content_area(self, soup: BeautifulSoup) -> BeautifulSoup:
        """æå–Azure Filesä¸»è¦å†…å®¹åŒºåŸŸï¼Œå‡å°‘divåµŒå¥—ï¼Œä¿ç•™é‡è¦sectionæ ‡é¢˜"""
        
        print("    ğŸ¯ æå–Azure Filesä¸»è¦å†…å®¹åŒºåŸŸ...")
        
        # åˆ›å»ºæ–°çš„å†…å®¹å®¹å™¨
        content_soup = BeautifulSoup("", 'html.parser')
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content_areas = [
            soup.find('main'),
            soup.find('div', class_='main-content'),
            soup.find('div', class_='content'),
            soup.find('body')
        ]
        
        main_area = None
        for area in main_content_areas:
            if area:
                main_area = area
                break
        
        if not main_area:
            main_area = soup
        
        # ç›´æ¥æå–å†…å®¹å…ƒç´ ï¼Œä¸æ·»åŠ é¢å¤–åŒ…è£…
        content_elements = []
        
        # 1. æå–äº§å“æ¨ªå¹…ï¼ˆAzure Filesç‰¹å®šï¼‰
        banner = main_area.find('div', class_='common-banner')
        if banner:
            # ç›´æ¥æå–æ¨ªå¹…æ–‡æœ¬å†…å®¹ï¼Œä¸åˆ›å»ºå¤æ‚ç»“æ„
            h2 = banner.find('h2')
            h4 = banner.find('h4')
            
            if h2:
                title_h1 = content_soup.new_tag('h1')
                title_h1.string = h2.get_text(strip=True)
                content_elements.append(title_h1)
            
            if h4:
                desc_p = content_soup.new_tag('p')
                desc_p.string = h4.get_text(strip=True)
                content_elements.append(desc_p)
        
        # 2. æå–Azure Filesç‰¹å®šå†…å®¹
        content_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol', 'table', 'a']
        
        processed_elements = set()
        
        for element in main_area.find_all(content_tags):
            # é¿å…é‡å¤å¤„ç†
            if id(element) in processed_elements:
                continue
            
            # è·³è¿‡ç©ºå…ƒç´ 
            if not element.get_text(strip=True):
                continue
            
            # è·³è¿‡å¯¼èˆªç›¸å…³å…ƒç´ 
            if self._is_navigation_element(element):
                continue
            
            # è·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ¨ªå¹…å†…å®¹
            if element.find_parent('div', class_='common-banner'):
                continue
            
            # è·³è¿‡è¡¨æ ¼å†…éƒ¨çš„å…ƒç´ ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
            if element.find_parent('table'):
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé‡è¦çš„sectionæ ‡é¢˜ - ä¼˜å…ˆä¿ç•™
            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                if self._is_important_section_title(element):
                    # é‡è¦æ ‡é¢˜ï¼Œç›´æ¥ä¿ç•™
                    clean_element = self._create_simple_element(element, content_soup)
                    if clean_element:
                        content_elements.append(clean_element)
                    processed_elements.add(id(element))
                    continue
                elif self._is_storage_table_related_title(element):
                    # æ™®é€šè¡¨æ ¼æ ‡é¢˜ï¼Œè·³è¿‡ä»¥é¿å…é‡å¤
                    continue
            
            # åªå¤„ç†ç‹¬ç«‹çš„é“¾æ¥ï¼Œè·³è¿‡åµŒå¥—åœ¨å…¶ä»–å…ƒç´ ä¸­çš„é“¾æ¥
            if element.name == 'a' and element.find_parent(['p', 'li', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                continue
            
            # ç›´æ¥å¤åˆ¶æœ‰æ„ä¹‰çš„å…ƒç´ 
            clean_element = self._create_simple_element(element, content_soup)
            if clean_element:
                content_elements.append(clean_element)
                
            # æ ‡è®°å·²å¤„ç†
            processed_elements.add(id(element))
        
        # å°†æ‰€æœ‰å†…å®¹å…ƒç´ ç›´æ¥æ·»åŠ åˆ°soupä¸­ï¼Œä¸æ·»åŠ é¢å¤–åŒ…è£…
        for element in content_elements:
            content_soup.append(element)
        
        elements_count = len(content_elements)
        print(f"    âœ“ æå–äº† {elements_count} ä¸ªAzure Fileså†…å®¹å…ƒç´ ï¼ˆç®€åŒ–ç»“æ„ï¼Œä¿ç•™sectionæ ‡é¢˜ï¼‰")
        
        return content_soup
    
    def _is_storage_table_related_title(self, element: Tag) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå­˜å‚¨ç›¸å…³è¡¨æ ¼æ ‡é¢˜ï¼ˆé¿å…é‡å¤æå–ï¼‰ï¼Œä½†ä¸è¿‡æ»¤é‡è¦çš„sectionæ ‡é¢˜"""
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºé‡è¦çš„sectionæ ‡é¢˜ï¼Œå¦‚æœæ˜¯åˆ™ä¸è¿‡æ»¤
        if self._is_important_section_title(element):
            return False
        
        # æ£€æŸ¥æ ‡é¢˜æ–‡æœ¬æ˜¯å¦åŒ…å«å­˜å‚¨è¡¨æ ¼ç›¸å…³å…³é”®è¯
        title_text = element.get_text(strip=True).lower()
        
        # åªæœ‰éå¸¸å…·ä½“çš„è¡¨æ ¼æ ‡é¢˜æ‰è¿‡æ»¤ï¼Œé¿å…è¿‡æ»¤é‡è¦çš„sectionæ ‡é¢˜
        # ç§»é™¤äº†å­˜å‚¨å†—ä½™ç±»å‹å…³é”®è¯ï¼Œå› ä¸ºå®ƒä»¬æ˜¯é‡è¦çš„sectionæ ‡é¢˜
        specific_table_keywords = [
            'ç³»åˆ—', 'tier', 'å±‚çº§', 'å®ä¾‹', 'gen', 'v2', 'v3', 'v4'
        ]
        
        # åªæœ‰å½“æ ‡é¢˜å¾ˆçŸ­ä¸”åŒ…å«å…·ä½“è¡¨æ ¼å…³é”®è¯æ—¶æ‰è®¤ä¸ºæ˜¯è¡¨æ ¼æ ‡é¢˜
        if len(title_text) < 50 and any(keyword in title_text for keyword in specific_table_keywords):
            # æ£€æŸ¥æ˜¯å¦é è¿‘è¡¨æ ¼
            next_sibling = element.find_next_sibling()
            if next_sibling and next_sibling.name == 'table':
                return True
            
            # æ£€æŸ¥æ˜¯å¦åœ¨åŒ…å«è¡¨æ ¼çš„å®¹å™¨ä¸­
            parent_with_table = element.find_parent()
            if parent_with_table and parent_with_table.find('table'):
                return True
        
        return False
    
    def _create_simple_element(self, original_element: Tag, soup: BeautifulSoup) -> Optional[Tag]:
        """åˆ›å»ºç®€åŒ–çš„å†…å®¹å…ƒç´ ï¼Œå‡å°‘åµŒå¥—"""
        
        if not original_element or not hasattr(original_element, 'name') or not original_element.name:
            return None
        
        try:
            # åˆ›å»ºæ–°å…ƒç´ 
            new_element = soup.new_tag(original_element.name)
            
            # åªä¿ç•™æœ€é‡è¦çš„å±æ€§
            if original_element.get('id'):
                new_element['id'] = original_element['id']
            
            if original_element.name == 'table':
                new_element['class'] = 'storage-files-pricing-table'
                # ä¿ç•™colspanå’Œrowspan
                for attr in ['colspan', 'rowspan']:
                    if original_element.get(attr):
                        new_element[attr] = original_element[attr]
            
            # é’ˆå¯¹ä¸åŒå…ƒç´ ç±»å‹è¿›è¡Œå¤„ç†
            if original_element.name == 'table':
                # è¡¨æ ¼ç‰¹æ®Šå¤„ç†ï¼šä¿æŒå®Œæ•´ç»“æ„
                self._copy_table_structure(original_element, new_element, soup)
            
            elif original_element.name == 'a':
                # é“¾æ¥ç‰¹æ®Šå¤„ç†ï¼šä¿ç•™hrefå’Œæ–‡æœ¬
                href = original_element.get('href')
                if href:
                    new_element['href'] = href
                
                # ä¿ç•™aria-labelç­‰å¯è®¿é—®æ€§å±æ€§
                for attr in ['aria-label', 'title', 'target']:
                    if original_element.get(attr):
                        new_element[attr] = original_element[attr]
                
                # å¤åˆ¶é“¾æ¥æ–‡æœ¬
                link_text = original_element.get_text(strip=True)
                if link_text:
                    new_element.string = link_text
            
            elif original_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                # æ ‡é¢˜ï¼šä¿ç•™æ–‡æœ¬ï¼Œä½†ä¹Ÿæ£€æŸ¥æ˜¯å¦åŒ…å«é“¾æ¥
                if original_element.find('a'):
                    # å¦‚æœæ ‡é¢˜ä¸­åŒ…å«é“¾æ¥ï¼Œä¿æŒç»“æ„
                    for child in original_element.children:
                        if hasattr(child, 'name') and child.name == 'a':
                            link_element = self._create_simple_element(child, soup)
                            if link_element:
                                new_element.append(link_element)
                        elif hasattr(child, 'strip'):
                            text = child.strip()
                            if text:
                                new_element.append(text)
                else:
                    # æ™®é€šæ ‡é¢˜ï¼Œåªä¿ç•™æ–‡æœ¬
                    text_content = original_element.get_text(strip=True)
                    if text_content:
                        new_element.string = text_content
            
            elif original_element.name == 'p':
                # æ®µè½ï¼šå¯èƒ½åŒ…å«é“¾æ¥ï¼Œéœ€è¦ä¿æŒç»“æ„
                if original_element.find('a'):
                    # æ®µè½ä¸­åŒ…å«é“¾æ¥ï¼Œä¿æŒæ··åˆå†…å®¹
                    for child in original_element.children:
                        if hasattr(child, 'name') and child.name == 'a':
                            link_element = self._create_simple_element(child, soup)
                            if link_element:
                                new_element.append(link_element)
                        elif hasattr(child, 'strip'):
                            text = child.strip()
                            if text:
                                new_element.append(text)
                else:
                    # æ™®é€šæ®µè½ï¼Œåªä¿ç•™æ–‡æœ¬
                    text_content = original_element.get_text(strip=True)
                    if text_content:
                        new_element.string = text_content
            
            elif original_element.name in ['ul', 'ol']:
                # åˆ—è¡¨ï¼šä¿ç•™ç»“æ„ä½†ç®€åŒ–ï¼Œä¹Ÿè¦å¤„ç†é“¾æ¥
                for li in original_element.find_all('li', recursive=False):
                    new_li = soup.new_tag('li')
                    
                    # æ£€æŸ¥liä¸­æ˜¯å¦æœ‰é“¾æ¥
                    if li.find('a'):
                        for child in li.children:
                            if hasattr(child, 'name') and child.name == 'a':
                                link_element = self._create_simple_element(child, soup)
                                if link_element:
                                    new_li.append(link_element)
                            elif hasattr(child, 'strip'):
                                text = child.strip()
                                if text:
                                    new_li.append(text)
                    else:
                        li_text = li.get_text(strip=True)
                        if li_text:
                            new_li.string = li_text
                    
                    if new_li.get_text(strip=True) or new_li.find_all():
                        new_element.append(new_li)
            
            else:
                # å…¶ä»–å…ƒç´ ï¼šæå–æ–‡æœ¬å†…å®¹
                text_content = original_element.get_text(strip=True)
                if text_content:
                    new_element.string = text_content
            
            return new_element if (new_element.get_text(strip=True) or new_element.find_all()) else None
            
        except Exception as e:
            print(f"    âš  åˆ›å»ºç®€åŒ–å…ƒç´ å¤±è´¥: {e}")
            return None
    
    def _copy_table_structure(self, original_table: Tag, new_table: Tag, soup: BeautifulSoup):
        """å¤åˆ¶è¡¨æ ¼ç»“æ„ï¼Œç¡®ä¿å®Œæ•´æ€§"""
        
        # ç›´æ¥å¤åˆ¶æ‰€æœ‰è¡Œ
        for tr in original_table.find_all('tr'):
            new_tr = soup.new_tag('tr')
            
            # å¤åˆ¶æ‰€æœ‰å•å…ƒæ ¼
            for cell in tr.find_all(['th', 'td']):
                new_cell = soup.new_tag(cell.name)
                
                # ä¿ç•™é‡è¦å±æ€§
                for attr in ['colspan', 'rowspan']:
                    if cell.get(attr):
                        new_cell[attr] = cell[attr]
                
                # å¤åˆ¶å•å…ƒæ ¼æ–‡æœ¬
                cell_text = cell.get_text(strip=True)
                if cell_text:
                    new_cell.string = cell_text
                
                new_tr.append(new_cell)
            
            if new_tr.find_all():  # åªæ·»åŠ éç©ºè¡Œ
                new_table.append(new_tr)
    
    def _is_navigation_element(self, element: Tag) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¯¼èˆªå…ƒç´ """
        
        if not hasattr(element, 'get'):
            return False
        
        # æ£€æŸ¥classå±æ€§
        classes = element.get('class', [])
        if isinstance(classes, str):
            classes = [classes]
        
        navigation_keywords = [
            'nav', 'menu', 'breadcrumb', 'tab-nav', 'dropdown', 'region-container',
            'software-kind', 'header', 'footer', 'sidebar'
        ]
        
        for class_name in classes:
            if any(keyword in class_name.lower() for keyword in navigation_keywords):
                return True
        
        return False
    
    def _apply_region_filtering(self, soup: BeautifulSoup, region: str) -> Tuple[int, int]:
        """åº”ç”¨åŒºåŸŸè¿‡æ»¤"""
        
        print(f"ğŸ” ç¬¬äºŒæ­¥ï¼šåº”ç”¨åŒºåŸŸè¿‡æ»¤ (åŒºåŸŸ: {region})...")
        
        # ç»Ÿè®¡è¿‡æ»¤å‰çš„è¡¨æ ¼æ•°é‡
        all_tables = soup.find_all('table')
        total_tables = len(all_tables)
        
        # åº”ç”¨è¿‡æ»¤
        filtered_count = 0
        tables_to_remove = []
        
        for table in all_tables:
            table_id = table.get('id', '')
            if table_id and self.region_filter.should_filter_table(table_id):
                tables_to_remove.append(table)
                filtered_count += 1
        
        # ç§»é™¤è¢«è¿‡æ»¤çš„è¡¨æ ¼åŠå…¶æ ‡é¢˜
        for table in tables_to_remove:
            # ç§»é™¤å‰é¢çš„æ ‡é¢˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            prev_sibling = table.find_previous_sibling()
            if prev_sibling and prev_sibling.name in ['h2', 'h3', 'h4'] and prev_sibling.get('class') == ['table-title']:
                prev_sibling.decompose()
            
            table.decompose()
        
        retained_count = total_tables - filtered_count
        
        print(f"  ğŸ“Š è¿‡æ»¤äº† {filtered_count} ä¸ªè¡¨æ ¼ï¼Œä¿ç•™ {retained_count} ä¸ªè¡¨æ ¼")
        
        return filtered_count, retained_count
    
    def _prepare_for_cms(self, soup: BeautifulSoup, region: str) -> BeautifulSoup:
        """ä¸ºCMSåšæœ€åçš„å‡†å¤‡ï¼Œä¿æŒç®€æ´ç»“æ„"""
        
        print("âœ¨ ç¬¬ä¸‰æ­¥ï¼šCMSä¼˜åŒ–ï¼ˆç®€åŒ–ç»“æ„ï¼‰...")
        
        # ç§»é™¤ç©ºçš„å®¹å™¨
        self._remove_empty_containers(soup)
        
        # ç¡®ä¿è¡¨æ ¼æœ‰é€‚å½“çš„æ ·å¼ç±»
        for table in soup.find_all('table'):
            if 'storage-files-pricing-table' not in table.get('class', []):
                table['class'] = 'storage-files-pricing-table'
        
        # åªæ·»åŠ ä¸€ä¸ªç®€å•çš„åŒºåŸŸæ ‡è¯†ï¼Œè€Œä¸æ˜¯å¤æ‚çš„åŒ…è£…ç»“æ„
        region_p = soup.new_tag('p', **{'class': 'region-info'})
        region_p.string = f"åŒºåŸŸ: {self.region_names[region]}"
        
        # å°†åŒºåŸŸä¿¡æ¯æ’å…¥åˆ°æœ€å‰é¢
        if soup.contents:
            soup.insert(0, region_p)
        else:
            soup.append(region_p)
        
        print("  âœ“ CMSä¼˜åŒ–å®Œæˆï¼ˆç®€æ´ç»“æ„ï¼‰")
        
        return soup
    
    def _remove_empty_containers(self, soup: BeautifulSoup):
        """ç§»é™¤ç©ºçš„å®¹å™¨"""
        
        # å¤šæ¬¡æ¸…ç†ï¼Œå› ä¸ºç§»é™¤æŸäº›å…ƒç´ åå¯èƒ½äº§ç”Ÿæ–°çš„ç©ºå®¹å™¨
        for _ in range(3):
            empty_elements = []
            
            for element in soup.find_all(['div', 'section', 'article', 'span']):
                # æ£€æŸ¥å…ƒç´ æ˜¯å¦ä¸ºç©º
                if not element.get_text(strip=True) and not element.find_all(['img', 'input', 'button', 'table']):
                    empty_elements.append(element)
            
            for element in empty_elements:
                element.decompose()
                
            if not empty_elements:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç©ºå…ƒç´ ï¼Œè·³å‡ºå¾ªç¯
                break
    
    def _build_final_html(self, soup: BeautifulSoup, region: str) -> str:
        """æ„å»ºæœ€ç»ˆçš„HTMLè¾“å‡º"""
        
        print("ğŸ—ï¸ ç¬¬å››æ­¥ï¼šæ„å»ºæœ€ç»ˆHTML...")
        
        region_name = self.region_names[region]
        
        # æ„å»ºå®Œæ•´çš„HTMLæ–‡æ¡£
        html_template = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Azure Storage Files å®šä»· - {region_name}</title>
    <meta name="description" content="Azure Storage Files åœ¨{region_name}çš„å®šä»·ä¿¡æ¯">
    <style>
        /* CMSå‹å¥½çš„åŸºç¡€æ ·å¼ */
        .product-banner {{
            margin-bottom: 2rem;
            padding: 1rem;
            background-color: #f8f9fa;
            border-left: 4px solid #0078d4;
        }}
        
        .product-title {{
            font-size: 1.5rem;
            font-weight: bold;
            margin-bottom: 0.5rem;
            color: #0078d4;
        }}
        
        .product-description {{
            color: #666;
            line-height: 1.5;
        }}
        
        .region-info {{
            background-color: #e7f3ff;
            padding: 0.5rem 1rem;
            margin-bottom: 1rem;
            border-radius: 4px;
            font-size: 0.9rem;
            color: #0078d4;
        }}
        
        .pricing-content {{
            margin-bottom: 2rem;
        }}
        
        .table-title {{
            font-size: 1.2rem;
            margin: 1.5rem 0 0.5rem 0;
            color: #333;
            border-bottom: 1px solid #ddd;
            padding-bottom: 0.5rem;
        }}
        
        .storage-files-pricing-table {{
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            background-color: white;
        }}
        
        .storage-files-pricing-table th,
        .storage-files-pricing-table td {{
            border: 1px solid #ddd;
            padding: 0.75rem;
            text-align: left;
        }}
        
        .storage-files-pricing-table th {{
            background-color: #f8f9fa;
            font-weight: bold;
            color: #333;
        }}
        
        .storage-files-pricing-table tr:nth-child(even) {{
            background-color: #f9f9f9;
        }}
        
        /* Sectionæ ‡é¢˜æ ·å¼ */
        h2 {{
            font-size: 1.4rem;
            color: #0078d4;
            margin: 2rem 0 1rem 0;
            border-bottom: 2px solid #0078d4;
            padding-bottom: 0.5rem;
        }}
        
        h3 {{
            font-size: 1.2rem;
            color: #333;
            margin: 1.5rem 0 1rem 0;
            border-left: 4px solid #0078d4;
            padding-left: 0.5rem;
        }}
        
        .storage-tier-section {{
            margin-top: 2rem;
            padding: 1rem;
            background-color: #f0f8ff;
            border-radius: 4px;
        }}
        
        .storage-tier-title {{
            font-size: 1.2rem;
            margin-bottom: 0.5rem;
            color: #0078d4;
        }}
        
        .hot-tier {{
            border-left: 4px solid #ff6b35;
        }}
        
        .cool-tier {{
            border-left: 4px solid #4dabf7;
        }}
        
        .transaction-section {{
            margin-top: 2rem;
        }}
        
        .transaction-title {{
            font-size: 1.3rem;
            margin-bottom: 1rem;
            color: #0078d4;
            border-bottom: 2px solid #0078d4;
            padding-bottom: 0.5rem;
        }}
        
        .bandwidth-section {{
            margin-top: 2rem;
            padding: 1rem;
            background-color: #fff8e7;
            border-radius: 4px;
        }}
        
        .bandwidth-title {{
            font-size: 1.2rem;
            margin-bottom: 0.5rem;
            color: #e67e22;
        }}
        
        .bandwidth-content {{
            margin-bottom: 0.5rem;
            color: #333;
            line-height: 1.5;
        }}
    </style>
</head>
<body>
{str(soup)}
</body>
</html>"""
        
        print("  âœ“ HTMLæ–‡æ¡£æ„å»ºå®Œæˆ")
        
        return html_template
    
    def _verify_extraction_result(self, html_content: str) -> Dict[str, any]:
        """éªŒè¯æå–ç»“æœ"""
        
        verification_soup = BeautifulSoup(html_content, 'html.parser')
        
        verification = {
            "has_main_content": bool(verification_soup.find('p', class_='region-info')),
            "has_region_info": bool(verification_soup.find('p', class_='region-info')), 
            "table_count": len(verification_soup.find_all('table', class_='storage-files-pricing-table')),
            "paragraph_count": len(verification_soup.find_all('p')),
            "heading_count": len(verification_soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])),
            "list_count": len(verification_soup.find_all(['ul', 'ol'])),
            "link_count": len(verification_soup.find_all('a')),
            "text_length": len(verification_soup.get_text(strip=True)),
            "html_size": len(html_content),
            "is_valid_html": html_content.strip().startswith('<!DOCTYPE html>')
        }
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦çš„sectionæ ‡é¢˜
        important_titles_found = []
        for title in ['å®šä»·è¯¦ç»†ä¿¡æ¯', 'äº†è§£å­˜å‚¨é€‰é¡¹', 'æ•°æ®å­˜å‚¨ä»·æ ¼', 'äº‹åŠ¡å’Œæ•°æ®ä¼ è¾“ä»·æ ¼']:
            if title in html_content:
                important_titles_found.append(title)
        
        # æ£€æŸ¥å­˜å‚¨å†—ä½™ç±»å‹æ ‡é¢˜
        redundancy_titles_found = []
        for redundancy in ['LRS', 'GRS', 'ZRS', 'GZRS', 'RA-GRS']:
            if f'>{redundancy}<' in html_content or f'>{redundancy.lower()}<' in html_content:
                redundancy_titles_found.append(redundancy)
        
        verification["important_section_titles"] = important_titles_found
        verification["redundancy_type_titles"] = redundancy_titles_found
        verification["has_section_structure"] = len(important_titles_found) > 0
        verification["has_redundancy_structure"] = len(redundancy_titles_found) > 0
        
        # éªŒè¯è¡¨æ ¼å†…å®¹
        table_verification = verify_table_content(html_content)
        verification.update(table_verification)
        
        # å†…å®¹å®Œæ•´æ€§æ£€æŸ¥
        verification["content_completeness"] = {
            "has_text_content": verification["text_length"] > 1000,  # è‡³å°‘1000å­—ç¬¦
            "has_structured_content": verification["table_count"] > 0 and verification["paragraph_count"] > 0,
            "has_navigation_structure": verification["heading_count"] > 0,
            "has_interactive_content": verification["link_count"] > 0,
            "has_section_titles": verification["has_section_structure"],
            "has_redundancy_titles": verification["has_redundancy_structure"]
        }
        
        return verification
    
    def save_cms_html(self, result: Dict[str, any], region: str, custom_filename: str = "") -> str:
        """ä¿å­˜CMS HTMLæ–‡ä»¶"""
        
        if not result["success"]:
            print(f"âŒ æ— æ³•ä¿å­˜å¤±è´¥çš„æå–ç»“æœ")
            return ""
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if custom_filename:
            filename = custom_filename
        else:
            filename = f"storage_files_{region}_cms_{timestamp}.html"
        
        file_path = self.output_dir / filename
        
        # ä¿å­˜HTMLæ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(result["html_content"])
        
        print(f"\nğŸ’¾ CMS HTMLå·²ä¿å­˜: {file_path}")
        print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {result['statistics']['final_size']:,} å­—èŠ‚")
        print(f"ğŸ“Š å‹ç¼©æ¯”: {result['statistics']['compression_ratio']*100:.1f}%")
        
        # æ˜¾ç¤ºæ‰¾åˆ°çš„é‡è¦sectionæ ‡é¢˜
        if "important_section_titles" in result["verification"]:
            titles = result["verification"]["important_section_titles"]
            print(f"ğŸ“‹ ä¿ç•™çš„sectionæ ‡é¢˜: {titles}")
        
        # æ˜¾ç¤ºæ‰¾åˆ°çš„å­˜å‚¨å†—ä½™ç±»å‹æ ‡é¢˜
        if "redundancy_type_titles" in result["verification"]:
            redundancy_titles = result["verification"]["redundancy_type_titles"]
            if redundancy_titles:
                print(f"ğŸ”§ ä¿ç•™çš„å­˜å‚¨ç±»å‹æ ‡é¢˜: {redundancy_titles}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = file_path.with_suffix('.stats.json')
        stats_data = {
            "region": result["region"],
            "statistics": result["statistics"],
            "verification": result["verification"],
            "extraction_info": result["extraction_info"]
        }
        
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
        
        return str(file_path)
    
    def extract_all_regions_cms(self, html_file_path: str, 
                               regions: Optional[List[str]] = None) -> Dict[str, Dict]:
        """æ‰¹é‡æå–æ‰€æœ‰åŒºåŸŸçš„CMS HTML"""
        
        if regions is None:
            regions = list(self.region_names.keys())
        
        print(f"\nğŸŒ å¼€å§‹æ‰¹é‡Azure Storage Files CMS HTMLæå– {len(regions)} ä¸ªåŒºåŸŸ")
        print(f"åŒºåŸŸåˆ—è¡¨: {[self.region_names.get(r, r) for r in regions]}")
        
        batch_results = {}
        successful_count = 0
        total_size = 0
        
        for i, region in enumerate(regions, 1):
            print(f"\n{'='*70}")
            print(f"å¤„ç†åŒºåŸŸ {i}/{len(regions)}: {self.region_names.get(region, region)} ({region})")
            print(f"{'='*70}")
            
            try:
                result = self.extract_cms_html_for_region(html_file_path, region)
                
                if result["success"]:
                    output_file = self.save_cms_html(result, region)
                    result["output_file"] = output_file
                    successful_count += 1
                    total_size += result["statistics"]["final_size"]
                    
                    print(f"âœ… {self.region_names.get(region, region)} CMS HTMLæå–å®Œæˆ")
                else:
                    print(f"âŒ {self.region_names.get(region, region)} æå–å¤±è´¥")
                
                batch_results[region] = result
                
            except Exception as e:
                print(f"âŒ {self.region_names.get(region, region)} å¤„ç†å¼‚å¸¸: {e}")
                batch_results[region] = {
                    "success": False,
                    "error": str(e),
                    "region": {"id": region, "name": self.region_names.get(region, region)}
                }
        
        # ç”Ÿæˆæ‰¹é‡å¤„ç†æ€»ç»“
        self._generate_batch_cms_summary(batch_results, successful_count, len(regions), total_size)
        
        return batch_results
    
    def _generate_batch_cms_summary(self, results: Dict, successful_count: int, 
                                   total_count: int, total_size: int):
        """ç”Ÿæˆæ‰¹é‡CMSå¤„ç†æ€»ç»“"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_path = self.output_dir / f"storage_files_cms_batch_summary_{timestamp}.json"
        
        summary = {
            "batch_info": {
                "processed_at": datetime.now().isoformat(),
                "extraction_version": "1.2_files_cms_with_redundancy_titles",
                "product": "Azure Storage Files",
                "total_regions": total_count,
                "successful_regions": successful_count,
                "failed_regions": total_count - successful_count,
                "total_output_size": total_size
            },
            "regions": {}
        }
        
        total_tables = 0
        
        for region, result in results.items():
            region_name = self.region_names.get(region, region)
            
            if result.get("success", False):
                verification = result["verification"]
                statistics = result["statistics"]
                
                total_tables += verification.get("table_count", 0)
                
                summary["regions"][region] = {
                    "name": region_name,
                    "status": "success",
                    "file_size": statistics["final_size"],
                    "compression_ratio": statistics["compression_ratio"],
                    "table_count": verification.get("table_count", 0),
                    "section_titles": verification.get("important_section_titles", []),
                    "redundancy_titles": verification.get("redundancy_type_titles", []),
                    "has_section_structure": verification.get("has_section_structure", False),
                    "has_redundancy_structure": verification.get("has_redundancy_structure", False),
                    "output_file": result.get("output_file", "")
                }
            else:
                summary["regions"][region] = {
                    "name": region_name,
                    "status": "failed",
                    "error": result.get("error", "æœªçŸ¥é”™è¯¯")
                }
        
        summary["aggregate_stats"] = {
            "total_tables": total_tables,
            "average_file_size": round(total_size / successful_count, 0) if successful_count > 0 else 0,
            "average_tables_per_region": round(total_tables / successful_count, 1) if successful_count > 0 else 0
        }
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“‹ Azure Storage Files CMSæ‰¹é‡å¤„ç†æ€»ç»“:")
        print(f"âœ… æˆåŠŸ: {successful_count}/{total_count} ä¸ªåŒºåŸŸ")
        print(f"ğŸ“Š æ€»å®šä»·è¡¨: {total_tables} ä¸ª")
        print(f"ğŸ“„ æ€»æ–‡ä»¶å¤§å°: {total_size:,} å­—èŠ‚")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“Š æ€»ç»“æŠ¥å‘Š: {summary_path}")


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description="Azure Storage Filesé¡µé¢CMS HTMLæå–å™¨")
    parser.add_argument("html_file", help="Azure Storage Files HTMLæºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-r", "--region", default="north-china3", help="ç›®æ ‡åŒºåŸŸ")
    parser.add_argument("-a", "--all-regions", action="store_true", help="æå–æ‰€æœ‰åŒºåŸŸ")
    parser.add_argument("-c", "--config", default="soft-category.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", default="storage_files_output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--regions", nargs="+", help="æŒ‡å®šè¦æå–çš„åŒºåŸŸåˆ—è¡¨")
    parser.add_argument("--filename", help="æŒ‡å®šè¾“å‡ºæ–‡ä»¶å")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.html_file):
        print(f"âŒ HTMLæ–‡ä»¶ä¸å­˜åœ¨: {args.html_file}")
        return 1
    
    # éªŒè¯é…ç½®æ–‡ä»¶
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return 1
    
    try:
        # åˆ›å»ºCMSæå–å™¨
        extractor = AzureStorageFilesCMSExtractor(args.config, args.output)
        
        # æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
        print("ğŸš€ Azure Storage Filesé¡µé¢CMS HTMLæå–å™¨ v1.2")
        print("ğŸ“„ ä¸“é—¨ç”Ÿæˆé€‚åˆCMSå¯¼å…¥çš„å¹²å‡€HTMLæ–‡ä»¶")
        print("ğŸ¯ ç‰¹æ€§: åŒºåŸŸè¿‡æ»¤ã€å†…å®¹æ¸…æ´—ã€CMSä¼˜åŒ–ã€ä¿ç•™sectionæ ‡é¢˜ã€ä¿ç•™å­˜å‚¨å†—ä½™ç±»å‹æ ‡é¢˜")
        
        if args.all_regions:
            # æ‰¹é‡æå–æ‰€æœ‰åŒºåŸŸ
            regions = args.regions or list(extractor.region_names.keys())
            results = extractor.extract_all_regions_cms(args.html_file, regions)
            
        else:
            # æå–å•ä¸ªåŒºåŸŸ
            if args.region not in extractor.region_names:
                print(f"âŒ ä¸æ”¯æŒçš„åŒºåŸŸ: {args.region}")
                print(f"æ”¯æŒçš„åŒºåŸŸ: {list(extractor.region_names.keys())}")
                return 1
            
            result = extractor.extract_cms_html_for_region(args.html_file, args.region)
            
            if result["success"]:
                output_file = extractor.save_cms_html(result, args.region, args.filename)
                print(f"âœ… å•ä¸ªåŒºåŸŸCMS HTMLæå–å®Œæˆ: {output_file}")
            else:
                print(f"âŒ æå–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return 1
        
        print("\nğŸ‰ Azure Storage Files CMS HTMLæå–ä»»åŠ¡å®Œæˆï¼")
        print("ğŸ“„ ç”Ÿæˆçš„HTMLæ–‡ä»¶å¯ç›´æ¥å¯¼å…¥CMSç³»ç»Ÿ")
        print("ğŸ“‹ ç°åœ¨åŒ…å«å®Œæ•´çš„sectionæ ‡é¢˜ç»“æ„")
        print("ğŸ”§ ç°åœ¨åŒ…å«å­˜å‚¨å†—ä½™ç±»å‹æ ‡é¢˜ï¼ˆLRSã€GRSç­‰ï¼‰")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ç¡®è®¤è´¨é‡")
        return 0
        
    except Exception as e:
        print(f"âŒ æå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())