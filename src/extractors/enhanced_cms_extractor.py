#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºå‹CMSæå–å™¨
æ•´åˆæ‰€æœ‰æœ€ä½³åŠŸèƒ½çš„ä¸»åŠ›æå–å™¨ï¼Œæ”¯æŒå¤§å‹HTMLæ–‡ä»¶å¤„ç†
"""

import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.core.product_manager import ProductManager
from src.core.region_processor import RegionProcessor
from src.core.config_manager import ConfigManager
from src.utils.html.element_creator import create_simple_element
from src.utils.html.cleaner import clean_html_content
from src.utils.media.image_processor import preprocess_image_paths
from src.utils.content.content_utils import (
    extract_qa_content, find_main_content_area, extract_banner_text_content, 
    extract_structured_content
)
from src.utils.data.validation_utils import validate_extracted_data
from src.utils.common.large_html_utils import LargeHTMLProcessor


class EnhancedCMSExtractor:
    """å¢å¼ºå‹CMSæå–å™¨ - æ•´åˆæ‰€æœ‰æœ€ä½³åŠŸèƒ½"""

    def __init__(self, output_dir: str, config_file: str = ""):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.config_file = config_file

        # åŒºåŸŸæ˜ å°„
        self.region_mapping = {
            "north-china": "NorthChinaContent",
            "north-china2": "NorthChina2Content",
            "north-china3": "NorthChina3Content",
            "east-china": "EastChinaContent",
            "east-china2": "EastChina2Content",
            "east-china3": "EastChina3Content"
        }

        # åŒºåŸŸä¸­æ–‡åæ˜ å°„
        self.region_names = {
            "north-china": "ä¸­å›½åŒ—éƒ¨",
            "north-china2": "ä¸­å›½åŒ—éƒ¨2",
            "north-china3": "ä¸­å›½åŒ—éƒ¨3",
            "east-china": "ä¸­å›½ä¸œéƒ¨",
            "east-china2": "ä¸­å›½ä¸œéƒ¨2",
            "east-china3": "ä¸­å›½ä¸œéƒ¨3"
        }

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.product_manager = ProductManager()
        self.region_processor = RegionProcessor(config_file)
        self.config_manager = ConfigManager()
        self.large_html_processor = LargeHTMLProcessor()

        print(f"ğŸš€ å¢å¼ºå‹CMSæå–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

    def extract_cms_content(self, html_file_path: str, url: str = "") -> Dict[str, Any]:
        """æå–CMSå†…å®¹ - æ™ºèƒ½å¤„ç†å¤§å‹æ–‡ä»¶"""
        
        print(f"\nğŸ”§ å¼€å§‹æå–å¢å¼ºå‹CMSå†…å®¹")
        print(f"ğŸ“ æºæ–‡ä»¶: {html_file_path}")

        # æ£€æµ‹äº§å“ç±»å‹
        product_key = self._detect_product_key_from_path(html_file_path)
        
        if not product_key:
            print("âš  æ— æ³•æ£€æµ‹äº§å“ç±»å‹ï¼Œä½¿ç”¨é€šç”¨æå–é€»è¾‘")
            product_key = "unknown"

        # è·å–å¤„ç†ç­–ç•¥
        try:
            strategy = self.product_manager.get_processing_strategy(html_file_path, product_key)
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {strategy['size_mb']:.2f} MB")
            print(f"ğŸš€ å¤„ç†ç­–ç•¥: {strategy['strategy']}")
        except Exception as e:
            print(f"âš  æ— æ³•è·å–å¤„ç†ç­–ç•¥: {e}")
            strategy = {"strategy": "normal", "size_mb": 0}

        if strategy['strategy'] == 'streaming':
            return self._extract_with_streaming(html_file_path, url, strategy, product_key)
        elif strategy['strategy'] == 'chunked':
            return self._extract_with_chunking(html_file_path, url, strategy, product_key)
        else:
            return self._extract_normal(html_file_path, url, product_key)

    def _extract_normal(self, html_file_path: str, url: str, product_key: str) -> Dict[str, Any]:
        """æ ‡å‡†æå–æ¨¡å¼"""
        print("ğŸ“„ ä½¿ç”¨æ ‡å‡†å¤„ç†æ¨¡å¼...")
        
        try:
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
                
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # é¢„å¤„ç†å›¾ç‰‡è·¯å¾„
            soup = preprocess_image_paths(soup)
            
            # æå–å†…å®¹
            extracted_data = self._extract_content_from_soup(
                soup, html_file_path, url, product_key
            )
            
            return extracted_data
            
        except Exception as e:
            print(f"âŒ æ ‡å‡†æå–å¤±è´¥: {e}")
            return self._create_error_result(str(e))

    def _extract_with_chunking(self, html_file_path: str, url: str,
                              strategy: Dict[str, Any], product_key: str) -> Dict[str, Any]:
        """åˆ†å—å¤„ç†æ¨¡å¼"""
        print("ğŸ§© ä½¿ç”¨åˆ†å—å¤„ç†æ¨¡å¼...")
        
        try:
            # ç›‘æ§å†…å­˜ä½¿ç”¨
            initial_memory = self.large_html_processor.monitor_memory_usage()
            print(f"  ğŸ§  åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.1f}MB")
            
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
                
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # é¢„å¤„ç†å›¾ç‰‡è·¯å¾„
            soup = preprocess_image_paths(soup)
            
            # æå–å†…å®¹ï¼ˆä¸æ ‡å‡†æ¨¡å¼ç›¸åŒï¼‰
            extracted_data = self._extract_content_from_soup(
                soup, html_file_path, url, product_key
            )
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            final_memory = self.large_html_processor.monitor_memory_usage()
            memory_used = final_memory - initial_memory
            print(f"  ğŸ§  å†…å­˜å¢é•¿: {memory_used:.1f}MB")
            
            extracted_data["processing_info"] = {
                "mode": "chunked",
                "file_size_mb": strategy.get('size_mb', 0),
                "memory_used_mb": memory_used,
                "processing_time": datetime.now().isoformat()
            }
            
            return extracted_data
            
        except Exception as e:
            print(f"âŒ åˆ†å—æå–å¤±è´¥: {e}")
            return self._create_error_result(str(e))

    def _extract_with_streaming(self, html_file_path: str, url: str,
                               strategy: Dict[str, Any], product_key: str) -> Dict[str, Any]:
        """æµå¼å¤„ç†æ¨¡å¼ï¼ˆä¸ºå¤§å‹æ–‡ä»¶é¢„ç•™ï¼‰"""
        print("ğŸŒŠ ä½¿ç”¨æµå¼å¤„ç†æ¨¡å¼...")
        
        # ç›®å‰ä½¿ç”¨åˆ†å—å¤„ç†çš„é€»è¾‘ï¼Œæœªæ¥å¯ä»¥å®ç°çœŸæ­£çš„æµå¼å¤„ç†
        result = self._extract_with_chunking(html_file_path, url, strategy, product_key)
        if "processing_info" in result:
            result["processing_info"]["mode"] = "streaming"
        
        print("  âš  æµå¼å¤„ç†åŠŸèƒ½å¼€å‘ä¸­ï¼Œå½“å‰ä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—å¤„ç†")
        
        return result

    def _extract_content_from_soup(self, soup: BeautifulSoup, html_file_path: str, 
                                  url: str, product_key: str) -> Dict[str, Any]:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–å†…å®¹"""
        
        print("ğŸ” å¼€å§‹å†…å®¹æå–...")
        
        # è·å–äº§å“é…ç½®
        try:
            product_config = self.product_manager.get_product_config(product_key)
            important_section_titles = self.product_manager.get_important_section_titles(product_key)
        except (ValueError, AttributeError) as e:
            print(f"âš  æ— æ³•è·å–äº§å“é…ç½®: {e}")
            product_config = {}
            important_section_titles = []
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = find_main_content_area(soup)
        
        # åˆå§‹åŒ–æå–ç»“æœï¼ˆå¯¹é½é‡æ„å‰çš„å­—æ®µç»“æ„ï¼‰
        extracted_data = {
            "product_key": product_key,
            "source_file": str(html_file_path),
            "source_url": url or self._get_default_url(product_key),
            "extraction_timestamp": datetime.now().isoformat(),
            "Title": "",
            "MetaDescription": "",
            "MetaKeywords": "",
            "MSServiceName": "",
            "Slug": "",
            "DescriptionContent": "",
            "Language": "",
            "NavigationTitle": "",
            "BannerContent": "",
            "QaContent": "",
            "HasRegion": False,
            "NoRegionContent": "",
            "NorthChinaContent": "",
            "NorthChina2Content": "",
            "NorthChina3Content": "",
            "EastChinaContent": "",
            "EastChina2Content": "",
            "EastChina3Content": "",
            "PricingTables": [],
            "RegionalContent": {},
            "ServiceTiers": [],
            "extraction_metadata": {}
        }
        
        # 1. æå–æ ‡é¢˜
        print("ğŸ·ï¸ æå–æ ‡é¢˜...")
        extracted_data["Title"] = self._extract_page_title(main_content or soup)

        # 2. æå–Metaä¿¡æ¯
        print("ğŸ“‹ æå–Metaä¿¡æ¯...")
        extracted_data["MetaDescription"] = self._extract_meta_description(soup)
        extracted_data["MetaKeywords"] = self._extract_meta_keywords(soup)
        extracted_data["MSServiceName"] = self._extract_ms_service_name(soup)
        extracted_data["Slug"] = self._extract_slug(url)
        extracted_data["Language"] = self._detect_language(soup)

        # 3. æå–Bannerå†…å®¹
        print("ğŸ¯ æå–Bannerå†…å®¹...")
        banner_content = self._extract_banner_content(main_content or soup)
        extracted_data["BannerContent"] = self._clean_html_content(banner_content)
        extracted_data["NavigationTitle"] = self._extract_navigation_title(soup)

        # 4. æå–æè¿°å†…å®¹
        print("ğŸ“ æå–æè¿°å†…å®¹...")
        description_content = self._extract_description_content(main_content or soup, important_section_titles)
        extracted_data["DescriptionContent"] = self._clean_html_content(description_content)

        # 5. æ£€æŸ¥åŒºåŸŸå¹¶æå–åŒºåŸŸå†…å®¹ï¼ˆåŒ…å«tabç»“æ„æ£€æµ‹ï¼‰
        print("ğŸŒ æ£€æŸ¥åŒºåŸŸå¹¶æå–åŒºåŸŸå†…å®¹...")
        extracted_data["HasRegion"] = self._check_has_region(soup)

        if extracted_data["HasRegion"]:
            # æå–å„åŒºåŸŸå†…å®¹ï¼ˆåŒ…å«tabç»“æ„æ£€æµ‹ï¼‰
            region_contents = self._extract_region_contents(soup, html_file_path)
            extracted_data.update(region_contents)

            # åŒæ—¶ä¿å­˜åˆ°RegionalContentå­—æ®µç”¨äºå…¼å®¹æ€§
            extracted_data["RegionalContent"] = region_contents
        else:
            # æ²¡æœ‰åŒºåŸŸé€‰æ‹©ï¼Œæå–ä¸»ä½“å†…å®¹åˆ°NoRegionContent
            # æ£€æµ‹æ˜¯å¦æœ‰ tab ç»“æ„
            tab_structure = self._detect_tab_structure(soup)
            if tab_structure:
                # æœ‰ tab ç»“æ„ï¼Œæå– tab å†…å®¹æ•°ç»„
                no_region_tabs = self._extract_no_region_content_with_tabs(soup, tab_structure)
                extracted_data["NoRegionContent"] = no_region_tabs
            else:
                # æ—  tab ç»“æ„ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
                extracted_data["NoRegionContent"] = self._extract_no_region_content(soup)

        # 6. æå–Q&Aå†…å®¹
        print("â“ æå–Q&Aå†…å®¹...")
        extracted_data["QaContent"] = self._extract_qa_content(main_content or soup)

        # 7. æå–å®šä»·è¡¨æ ¼å’Œç»“æ„åŒ–å†…å®¹
        print("ğŸ“‹ æå–å®šä»·è¡¨æ ¼...")
        extracted_data["PricingTables"] = self._extract_pricing_tables(main_content or soup)

        print("ğŸ› ï¸ æå–ç»“æ„åŒ–å†…å®¹...")
        structured_content = extract_structured_content(main_content or soup, important_section_titles)
        extracted_data["ServiceTiers"] = structured_content.get("sections", [])

        # 8. æ·»åŠ æå–å…ƒæ•°æ®
        extracted_data["extraction_metadata"] = {
            "extractor_version": "enhanced_v2.0",
            "processing_mode": "standard",
            "content_sections_found": len(structured_content.get("sections", [])),
            "pricing_tables_found": len(extracted_data["PricingTables"]),
            "regions_detected": len(extracted_data["RegionalContent"]),
            "faq_length": len(extracted_data["QaContent"]),
            "has_banner": bool(extracted_data["BannerContent"]),
            "product_config_used": product_key != "unknown"
        }
        
        # 9. éªŒè¯æå–çš„æ•°æ®
        if product_config:
            try:
                validation_result = validate_extracted_data(extracted_data, product_config)
                extracted_data["validation"] = validation_result
                
                if not validation_result["is_valid"]:
                    print(f"âš  æ•°æ®éªŒè¯å‘ç°é—®é¢˜: {validation_result['errors']}")
            except Exception as e:
                print(f"âš  æ•°æ®éªŒè¯å¤±è´¥: {e}")
        
        print(f"âœ… å†…å®¹æå–å®Œæˆ")
        print(f"  ğŸ“Š æ ‡é¢˜: {'âœ“' if extracted_data['Title'] else 'âœ—'}")
        print(f"  ğŸ¯ Banner: {'âœ“' if extracted_data['BannerContent'] else 'âœ—'}")  
        print(f"  ğŸ“ æè¿°: {'âœ“' if extracted_data['DescriptionContent'] else 'âœ—'}")
        print(f"  ğŸ“‹ å®šä»·è¡¨æ ¼: {len(extracted_data['PricingTables'])} ä¸ª")
        print(f"  â“ FAQ: {'âœ“' if extracted_data['QaContent'] else 'âœ—'}")
        print(f"  ğŸŒ åŒºåŸŸ: {len(extracted_data['RegionalContent'])} ä¸ª")
        
        return extracted_data

    def _extract_page_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        # ä¼˜å…ˆæŸ¥æ‰¾é¡µé¢titleæ ‡ç­¾
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text(strip=True)
            if title and len(title) > 0:
                return title
        
        # æŸ¥æ‰¾ä¸»è¦æ ‡é¢˜å…ƒç´ 
        main_heading = soup.find(['h1', 'h2'])
        if main_heading:
            return main_heading.get_text(strip=True)
        
        return ""

    def _extract_banner_content(self, soup: BeautifulSoup) -> str:
        """æå–Bannerå†…å®¹"""
        print("  ğŸ¯ æå–Bannerå†…å®¹...")

        banner = soup.find('div', class_='common-banner')
        if banner:
            # æ ‡å‡†åŒ–å›¾ç‰‡æ ¼å¼
            standardized_banner = self._standardize_banner_images(banner)
            return standardized_banner

        print("    âš  æœªæ‰¾åˆ°Bannerå†…å®¹")
        return ""

    def _standardize_banner_images(self, banner) -> str:
        """æ ‡å‡†åŒ–Bannerä¸­çš„å›¾ç‰‡æ ¼å¼ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹"""

        # æå–èƒŒæ™¯å›¾ç‰‡
        background_image = self._extract_background_image(banner)

        # æå–å›¾æ ‡
        icon_image = self._extract_icon_image(banner)

        # æå–æ–‡æœ¬å†…å®¹
        text_content = self._extract_banner_text_content(banner)

        # ç”Ÿæˆæ ‡å‡†åŒ–HTML
        if background_image or icon_image or text_content:
            standardized_html = ""

            # æ·»åŠ èƒŒæ™¯å›¾ç‰‡div
            if background_image:
                standardized_html += f'<div class="common-banner-image" style="background-image: url(&quot;{background_image}&quot;);">'
            else:
                standardized_html += '<div class="common-banner-image">'

            # æ·»åŠ å›¾æ ‡
            if icon_image:
                standardized_html += f'<img src="{icon_image}" alt="imgAlt">'

            # æ·»åŠ æ–‡æœ¬å†…å®¹
            if text_content:
                standardized_html += text_content

            standardized_html += '</div>'

            return standardized_html

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å†…å®¹ï¼Œè¿”å›åŸå§‹banner
        return str(banner)

    def _extract_background_image(self, banner) -> str:
        """ä»bannerä¸­æå–èƒŒæ™¯å›¾ç‰‡URL"""

        # æŸ¥æ‰¾data-configå±æ€§
        data_config = banner.get('data-config', '')
        if data_config:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–backgroundImage
            import re
            pattern = r'["\']backgroundImage["\']:\s*["\']([^"\']*)["\']'
            match = re.search(pattern, data_config)
            if match:
                return match.group(1)

        # æŸ¥æ‰¾styleå±æ€§ä¸­çš„background-image
        style = banner.get('style', '')
        if 'background-image' in style:
            import re
            pattern = r'background-image:\s*url\(["\']?([^"\']*)["\']?\)'
            match = re.search(pattern, style)
            if match:
                return match.group(1)

        return ""

    def _extract_icon_image(self, banner) -> str:
        """ä»bannerä¸­æå–å›¾æ ‡å›¾ç‰‡URL"""

        # é¦–å…ˆæŸ¥æ‰¾imgæ ‡ç­¾
        img_tag = banner.find('img')
        if img_tag:
            src = img_tag.get('src', '')
            if src:
                return src

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°imgæ ‡ç­¾ï¼ŒæŸ¥æ‰¾svgæ ‡ç­¾
        svg_tag = banner.find('svg')
        if svg_tag:
            # æ£€æŸ¥svgæ ‡ç­¾æ˜¯å¦æœ‰idå±æ€§ï¼Œé€šå¸¸åŒ…å«äº§å“ä¿¡æ¯
            svg_id = svg_tag.get('id', '')
            if svg_id:
                # ä¸ºsvgç”Ÿæˆä¸€ä¸ªå ä½ç¬¦è·¯å¾„ï¼ŒåŸºäºid
                # ä¾‹å¦‚ï¼šsvg-storage/files -> storage-filesçš„å›¾æ ‡
                if 'svg-' in svg_id:
                    product_name = svg_id.replace('svg-', '').replace('\\', '-')
                    return f"{{img_hostname}}/Images/marketing-resource/css/{product_name}_icon.svg"

            # æ£€æŸ¥svgå†…éƒ¨çš„symbolå…ƒç´ 
            symbol_tag = svg_tag.find('symbol')
            if symbol_tag:
                symbol_id = symbol_tag.get('id', '')
                if symbol_id and 'svg-' in symbol_id:
                    product_name = symbol_id.replace('svg-', '').replace('\\', '-')
                    return f"{{img_hostname}}/Images/marketing-resource/css/{product_name}_icon.svg"

        return ""

    def _extract_banner_text_content(self, banner) -> str:
        """ä»bannerä¸­æå–æ–‡æœ¬å†…å®¹ï¼ˆæ ‡é¢˜ã€æè¿°ç­‰ï¼‰"""

        # æŸ¥æ‰¾common-banner-titleå®¹å™¨
        title_container = banner.find('div', class_='common-banner-title')
        if not title_container:
            return ""

        text_content = ""

        # æå–ä¸»æ ‡é¢˜ (h2)
        h2_tag = title_container.find('h2')
        if h2_tag:
            # åˆ›å»ºh2æ ‡ç­¾çš„å‰¯æœ¬ï¼Œç§»é™¤imgæ ‡ç­¾
            from copy import copy
            h2_copy = copy(h2_tag)
            for img in h2_copy.find_all('img'):
                img.decompose()
            text_content += str(h2_copy)

        # æå–å‰¯æ ‡é¢˜ (h4)
        h4_tag = title_container.find('h4')
        if h4_tag:
            text_content += str(h4_tag)

        # æå–å…¶ä»–æ ‡é¢˜çº§åˆ« (h3, h5, h6)
        for tag_name in ['h3', 'h5', 'h6']:
            tag = title_container.find(tag_name)
            if tag:
                text_content += str(tag)

        return text_content

    def _clean_html_content(self, content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹ä¸­çš„å¤šä½™æ ‡ç­¾å’Œç¬¦å·"""

        if not content:
            return content

        import re

        # ç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºç™½ç¬¦
        content = re.sub(r'\n+', ' ', content)  # å°†å¤šä¸ªæ¢è¡Œç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        content = re.sub(r'\s+', ' ', content)  # å°†å¤šä¸ªç©ºç™½ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼

        # ç§»é™¤å¤šä½™çš„divæ ‡ç­¾åŒ…è£…ï¼ˆä¿ç•™æœ‰ç”¨çš„classå’Œidï¼‰
        # åªç§»é™¤çº¯ç²¹çš„åŒ…è£…divï¼Œä¿ç•™æœ‰æ„ä¹‰çš„div
        content = re.sub(r'<div>\s*</div>', '', content)  # ç§»é™¤ç©ºçš„divæ ‡ç­¾

        # æ¸…ç†æ ‡ç­¾é—´çš„å¤šä½™ç©ºç™½
        content = re.sub(r'>\s+<', '><', content)  # ç§»é™¤æ ‡ç­¾é—´çš„ç©ºç™½

        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½
        content = content.strip()

        return content

    def _extract_qa_content(self, soup: BeautifulSoup) -> str:
        """æå–Q&Aå†…å®¹ä»¥åŠæ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®å†…å®¹"""

        print("â“ æå–Q&Aå†…å®¹...")

        qa_content = ""

        # ç°æœ‰çš„FAQæå–é€»è¾‘
        faq_containers = [
            soup.find('div', class_='faq'),
            soup.find('div', class_='qa'),
            soup.find('section', class_='faq'),
            soup.find('section', class_='qa')
        ]

        for container in faq_containers:
            if container:
                qa_content += str(container)

        # æŸ¥æ‰¾åŒ…å«FAQç»“æ„çš„åˆ—è¡¨
        faq_lists = soup.find_all('ul', class_='faq-list')
        if faq_lists:
            # å¦‚æœæœ‰å¤šä¸ªFAQåˆ—è¡¨ï¼Œåˆå¹¶å®ƒä»¬
            for faq_list in faq_lists:
                qa_content += str(faq_list)

        # æŸ¥æ‰¾åŒ…å«icon-plusçš„åˆ—è¡¨ï¼ˆFAQå±•å¼€å›¾æ ‡ï¼‰
        for ul in soup.find_all('ul'):
            if ul.find('i', class_='icon-plus'):
                qa_content += str(ul)

        # æ–°å¢ï¼šæå–æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®å†…å®¹
        print("ğŸ› ï¸ æå–æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®å†…å®¹...")

        # æŸ¥æ‰¾æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®éƒ¨åˆ†
        support_sections = soup.find_all('div', class_='pricing-page-section')
        for section in support_sections:
            h2_tag = section.find('h2')
            if h2_tag and 'æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®' in h2_tag.get_text(strip=True):
                qa_content += str(section)
                print("  âœ“ æ‰¾åˆ°æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®éƒ¨åˆ†")
                break

        # æŸ¥æ‰¾æ³¨é‡Šä¸­çš„æ”¯æŒä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        import re
        html_content = str(soup)
        support_comment_pattern = r'<!--BEGIN: Support and service code chunk-->(.*?)<!--END: Support and service code chunk-->'
        support_matches = re.findall(support_comment_pattern, html_content, re.DOTALL)
        if support_matches:
            for match in support_matches:
                # è§£ææ³¨é‡Šä¸­çš„HTMLå†…å®¹
                if match.strip() and not match.strip().startswith('<!--'):
                    qa_content += f"<!-- æ”¯æŒä¿¡æ¯ -->{match}<!-- /æ”¯æŒä¿¡æ¯ -->"
                    print("  âœ“ æ‰¾åˆ°æ³¨é‡Šä¸­çš„æ”¯æŒä¿¡æ¯")

        return self._clean_html_content(qa_content)

    def _extract_description_content(self, soup: BeautifulSoup,
                                   important_section_titles: List[str]) -> str:
        """æå–Bannerä¸‹ç¬¬ä¸€ä¸ªsectionä½œä¸ºæè¿°å†…å®¹"""
        print("  ğŸ“ æå–æè¿°å†…å®¹...")

        # æŸ¥æ‰¾banneråçš„ç¬¬ä¸€ä¸ªå†…å®¹section
        banner = soup.find('div', class_='common-banner')
        if banner:
            # æŸ¥æ‰¾banneråçš„ç¬¬ä¸€ä¸ªpricing-page-sectionæˆ–section
            next_section = banner.find_next_sibling('div', class_='pricing-page-section')
            if next_section:
                print("    âœ“ æ‰¾åˆ°banneråçš„pricing-page-section")
                return str(next_section)

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°pricing-page-sectionï¼ŒæŸ¥æ‰¾æ™®é€šsection
            next_section = banner.find_next_sibling('section')
            if next_section:
                print("    âœ“ æ‰¾åˆ°banneråçš„section")
                return str(next_section)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°bannerï¼Œå°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªpricing-page-section
        first_pricing_section = soup.find('div', class_='pricing-page-section')
        if first_pricing_section:
            print("    âœ“ æ‰¾åˆ°ç¬¬ä¸€ä¸ªpricing-page-section")
            return str(first_pricing_section)

        # æœ€åå°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªsection
        first_section = soup.find('section')
        if first_section:
            print("    âœ“ æ‰¾åˆ°ç¬¬ä¸€ä¸ªsection")
            return str(first_section)

        print("    âš  æœªæ‰¾åˆ°æè¿°å†…å®¹")
        return ""

    def _extract_pricing_tables(self, soup: BeautifulSoup) -> List[str]:
        """æå–å®šä»·è¡¨æ ¼"""
        print("  ğŸ“‹ æå–å®šä»·è¡¨æ ¼...")
        
        pricing_tables = []
        tables = soup.find_all('table')
        
        for i, table in enumerate(tables):
            table_text = table.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå®šä»·è¡¨æ ¼
            if self._is_pricing_table(table_text):
                # åˆ›å»ºç®€åŒ–çš„è¡¨æ ¼
                simplified_table = BeautifulSoup("<table></table>", 'html.parser')
                table_elem = simplified_table.find('table')
                table_elem['class'] = ['pricing-table']
                
                # å¤åˆ¶è¡¨æ ¼ç»“æ„
                from src.utils.html.element_creator import copy_table_structure
                copy_table_structure(table, table_elem, simplified_table)
                
                pricing_tables.append(str(simplified_table))
        
        print(f"    âœ“ æ‰¾åˆ° {len(pricing_tables)} ä¸ªå®šä»·è¡¨æ ¼")
        return pricing_tables

    def _is_pricing_table(self, table_text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå®šä»·è¡¨æ ¼"""
        pricing_keywords = [
            'ä»·æ ¼', 'price', 'å®šä»·', 'pricing', 'è´¹ç”¨', 'cost', 
            'ï¿¥', '$', 'å…ƒ', 'ç¾å…ƒ', 'usd', 'rmb', 'cny'
        ]
        
        text_lower = table_text.lower()
        return any(keyword in text_lower for keyword in pricing_keywords)

    def _format_html_content(self, soup: BeautifulSoup) -> str:
        """æ ¼å¼åŒ–HTMLå†…å®¹"""
        if not soup or not soup.get_text(strip=True):
            return ""
        
        html_str = str(soup)
        return clean_html_content(html_str)

    def _detect_product_key_from_path(self, html_file_path: str) -> Optional[str]:
        """ä»æ–‡ä»¶è·¯å¾„æ£€æµ‹äº§å“ç±»å‹"""
        try:
            return self.product_manager.detect_product_from_filename(html_file_path)
        except Exception as e:
            print(f"âš  äº§å“ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
            return None

    def _get_default_url(self, product_key: str) -> str:
        """è·å–äº§å“é»˜è®¤URL"""
        try:
            return self.product_manager.get_product_url(product_key)
        except Exception:
            return f"https://www.azure.cn/pricing/details/{product_key}/"

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            "error": True,
            "error_message": error_message,
            "extraction_timestamp": datetime.now().isoformat(),
            "Title": "",
            "BannerContent": "",
            "DescriptionContent": "",
            "PricingTables": [],
            "FAQ": "",
            "RegionalContent": {},
            "ServiceTiers": [],
            "extraction_metadata": {
                "extractor_version": "enhanced_v2.0",
                "processing_failed": True
            }
        }

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æå–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "extractor_type": "EnhancedCMSExtractor",
            "version": "2.0",
            "large_file_support": True,
            "streaming_support": "development"
        }
        
        try:
            stats["supported_products"] = len(self.product_manager.get_supported_products())
            stats["cache_stats"] = self.product_manager.get_cache_stats()
        except Exception as e:
            print(f"âš  è·å–äº§å“ç®¡ç†å™¨ç»Ÿè®¡å¤±è´¥: {e}")
            stats["supported_products"] = 0
            stats["cache_stats"] = {}
        
        try:
            stats["region_processor_stats"] = self.region_processor.get_statistics()
        except Exception as e:
            print(f"âš  è·å–åŒºåŸŸå¤„ç†å™¨ç»Ÿè®¡å¤±è´¥: {e}")
            stats["region_processor_stats"] = {}
            
        return stats
    
    def _extract_meta_description(self, soup: BeautifulSoup) -> str:
        """æå–Metaæè¿°"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')
        return ""
    
    def _extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """æå–Metaå…³é”®è¯"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            return meta_keywords.get('content', '')
        return ""
    
    def _extract_ms_service_name(self, soup: BeautifulSoup) -> str:
        """æå–MSServiceNameå­—æ®µï¼Œä»pure-content divå†…çš„tagså…ƒç´ ä¸­çš„ms.serviceå±æ€§"""
        print("ğŸ·ï¸ æå–MSServiceName...")
        
        # æŸ¥æ‰¾pure-content div
        pure_content_div = soup.find('div', class_='pure-content')
        if pure_content_div:
            # åœ¨pure-content divå†…æŸ¥æ‰¾tagså…ƒç´ 
            tags_element = pure_content_div.find('tags')
            if tags_element:
                # æå–ms.serviceå±æ€§å€¼
                ms_service = tags_element.get('ms.service', '')
                if ms_service:
                    print(f"  âœ“ æ‰¾åˆ°MSServiceName: {ms_service}")
                    return ms_service
                else:
                    print("  âš  tagså…ƒç´ ä¸­æ²¡æœ‰ms.serviceå±æ€§")
            else:
                print("  âš  pure-content divä¸­æ²¡æœ‰æ‰¾åˆ°tagså…ƒç´ ")
        else:
            print("  âš  æ²¡æœ‰æ‰¾åˆ°pure-content div")
        
        return ""
    
    def _extract_slug(self, url: str) -> str:
        """ä»URLæå–slug"""
        if not url:
            return ""
        
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            path = parsed.path
            
            # æå–/details/ä¹‹ååˆ°/index.htmlä¹‹å‰çš„å†…å®¹ï¼Œç”¨-è¿æ¥
            # ä¾‹å¦‚ /pricing/details/storage/files/index.html -> storage-files
            # ä¾‹å¦‚ /pricing/details/api-management/index.html -> api-management
            if '/details/' in path:
                # æ‰¾åˆ°/details/ä¹‹åçš„éƒ¨åˆ†
                after_details = path.split('/details/')[1]
                
                # ç§»é™¤/index.htmlåç¼€
                if after_details.endswith('/index.html'):
                    after_details = after_details[:-11]  # ç§»é™¤'/index.html'
                elif after_details.endswith('/'):
                    after_details = after_details[:-1]  # ç§»é™¤æœ«å°¾çš„'/'
                
                # åˆ†å‰²è·¯å¾„å¹¶ç”¨-è¿æ¥
                path_parts = [p for p in after_details.split('/') if p]
                if path_parts:
                    return '-'.join(path_parts)
        except:
            pass
        
        return ""
    
    def _detect_language(self, soup: BeautifulSoup) -> str:
        """æ£€æµ‹é¡µé¢è¯­è¨€"""
        # æ£€æŸ¥htmlæ ‡ç­¾çš„langå±æ€§
        html_tag = soup.find('html')
        if html_tag:
            lang = html_tag.get('lang', '')
            if 'en' in lang.lower():
                return 'en-US'
        
        # é»˜è®¤è¿”å›ä¸­æ–‡
        return 'zh-CN'
    
    def _extract_navigation_title(self, soup: BeautifulSoup) -> str:
        """æå–å¯¼èˆªæ ‡é¢˜"""
        # æŸ¥æ‰¾common-banner-title > h2
        banner_title = soup.find('div', class_='common-banner-title')
        if banner_title:
            h2 = banner_title.find('h2')
            if h2:
                return h2.get_text(strip=True)
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾é¡µé¢ä¸»æ ‡é¢˜
        main_title = soup.find('h1')
        if main_title:
            return main_title.get_text(strip=True)
        
        return ""
    
    def _check_has_region(self, soup: BeautifulSoup) -> bool:
        """æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰åŒºåŸŸé€‰æ‹©"""
        print("ğŸŒ æ£€æŸ¥åŒºåŸŸé€‰æ‹©...")
        
        # æŸ¥æ‰¾åŒºåŸŸé€‰æ‹©ç›¸å…³çš„å…ƒç´ 
        region_indicators = [
            soup.find('div', class_='region-container'),
            soup.find('select', class_='region-selector'),
            soup.find('div', class_='software-kind'),
            soup.find('div', attrs={'data-region': True})
        ]
        
        for indicator in region_indicators:
            if indicator:
                print(f"  âœ“ å‘ç°åŒºåŸŸé€‰æ‹©å™¨: {indicator.name}")
                return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªåŒºåŸŸç›¸å…³çš„å±æ€§
        region_elements = soup.find_all(attrs={'data-region': True})
        if len(region_elements) > 1:
            print(f"  âœ“ å‘ç° {len(region_elements)} ä¸ªåŒºåŸŸå…ƒç´ ")
            return True

        # æ£€æŸ¥è¡¨æ ¼IDä¸­æ˜¯å¦åŒ…å«åŒºåŸŸä¿¡æ¯ï¼ˆå¦‚ east3, north3ï¼‰
        tables_with_regions = soup.find_all('table', id=lambda x: x and any(region in x for region in ['east3', 'north3', 'east-china', 'north-china']))
        if tables_with_regions:
            print(f"  âœ“ å‘ç° {len(tables_with_regions)} ä¸ªåŒ…å«åŒºåŸŸä¿¡æ¯çš„è¡¨æ ¼")
            return True

        print("  â„¹ æœªå‘ç°åŒºåŸŸé€‰æ‹©å™¨")
        return False

    def _detect_available_regions(self, soup: BeautifulSoup) -> Dict[str, str]:
        """åŠ¨æ€æ£€æµ‹HTMLä¸­å®é™…å­˜åœ¨çš„åŒºåŸŸ"""
        available_regions = {}

        # æ£€æŸ¥åŒºåŸŸé€‰æ‹©å™¨ä¸­çš„é€‰é¡¹
        region_selectors = soup.find_all('a', {'data-href': True}) + soup.find_all('option', {'data-href': True})

        for selector in region_selectors:
            data_href = selector.get('data-href', '').replace('#', '')
            if data_href and data_href.startswith(('north-china', 'east-china')):
                region_text = selector.get_text(strip=True)
                available_regions[data_href] = region_text
                print(f"    ğŸ” å‘ç°åŒºåŸŸé€‰æ‹©å™¨: {data_href} -> {region_text}")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼ ç»Ÿçš„åŒºåŸŸé€‰æ‹©å™¨ï¼Œä»è¡¨æ ¼IDä¸­æå–åŒºåŸŸä¿¡æ¯
        if not available_regions:
            tables = soup.find_all('table', id=True)
            region_patterns = {
                'east3': 'east-china3',
                'north3': 'north-china3',
                'east-china': 'east-china',
                'north-china': 'north-china',
                'east-china2': 'east-china2',
                'north-china2': 'north-china2'
            }

            detected_regions = set()
            for table in tables:
                table_id = table.get('id', '')
                for pattern, region_id in region_patterns.items():
                    if pattern in table_id:
                        detected_regions.add(region_id)

            # å°†æ£€æµ‹åˆ°çš„åŒºåŸŸæ·»åŠ åˆ°ç»“æœä¸­
            for region_id in detected_regions:
                region_name = self.region_names.get(region_id, region_id)
                available_regions[region_id] = region_name

        # å¦‚æœä»ç„¶åªæ£€æµ‹åˆ°å°‘æ•°åŒºåŸŸï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶ä¸­è·å–å®Œæ•´çš„åŒºåŸŸåˆ—è¡¨
        if len(available_regions) < 4:  # å¦‚æœå°‘äº4ä¸ªåŒºåŸŸï¼Œå¯èƒ½æ˜¯ä¸å®Œæ•´çš„
            print(f"    â„¹ åªæ£€æµ‹åˆ° {len(available_regions)} ä¸ªåŒºåŸŸï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶è·å–å®Œæ•´åˆ—è¡¨")
            config_regions = self._get_regions_from_config()
            if config_regions:
                print(f"    ğŸ“‹ ä»é…ç½®æ–‡ä»¶è·å–åˆ° {len(config_regions)} ä¸ªåŒºåŸŸ")
                available_regions.update(config_regions)

        return available_regions

    def _get_regions_from_config(self) -> Dict[str, str]:
        """ä»é…ç½®æ–‡ä»¶ä¸­è·å–åŒºåŸŸåˆ—è¡¨"""
        config_regions = {}

        # ä»åŒºåŸŸå¤„ç†å™¨çš„é…ç½®ä¸­è·å– Data Lake Storage çš„åŒºåŸŸ
        for filename, product_config in self.region_processor.region_config.items():
            if 'storage_data-lake' in filename or 'data-lake' in filename:
                for region_id in product_config.keys():
                    if region_id in self.region_mapping:
                        region_name = self.region_names.get(region_id, region_id)
                        config_regions[region_id] = region_name
                break

        return config_regions

    def _detect_tab_structure(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """æ£€æµ‹é¡µé¢ä¸­çš„ tab ç»“æ„"""
        tab_structure = []

        # æŸ¥æ‰¾ tab å¯¼èˆªå…ƒç´ 
        tab_nav = soup.find('ul', class_='os-tab-nav')
        if not tab_nav:
            # ä¹Ÿå°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ tab å¯¼èˆªç»“æ„
            tab_nav = soup.find('ul', class_=lambda x: x and 'tab-nav' in x)

        if not tab_nav:
            return tab_structure

        # éå†æ‰€æœ‰ tab é“¾æ¥
        tab_links = tab_nav.find_all('a')
        for link in tab_links:
            tab_name = link.get_text(strip=True)
            data_href = link.get('data-href', '')

            if tab_name and data_href:
                # ç§»é™¤ # å‰ç¼€è·å–å†…å®¹ ID
                content_id = data_href.lstrip('#')

                tab_info = {
                    'tabName': tab_name,
                    'contentId': content_id
                }
                tab_structure.append(tab_info)
                print(f"    ğŸ“‹ å‘ç° tab: {tab_name} -> {content_id}")

        return tab_structure

    def _extract_region_contents(self, soup: BeautifulSoup, html_file_path: str = "") -> Dict[str, Any]:
        """æå–å„åŒºåŸŸçš„å†…å®¹ï¼Œæ”¯æŒ tab ç»“æ„ï¼Œä½¿ç”¨ region_processor çš„æ ¸å¿ƒåŠŸèƒ½"""

        print("ğŸŒ æå–å„åŒºåŸŸå†…å®¹...")

        region_contents = {}

        # æ£€æµ‹ tab ç»“æ„
        tab_structure = self._detect_tab_structure(soup)
        has_tabs = len(tab_structure) > 0

        if has_tabs:
            print(f"  ğŸ“‚ æ£€æµ‹åˆ° tab ç»“æ„: {[tab['tabName'] for tab in tab_structure]}")

            # ä½¿ç”¨ region_processor æ£€æµ‹å¯ç”¨åŒºåŸŸ
            available_regions = self.region_processor.detect_available_regions(soup)
            print(f"  ğŸ” æ£€æµ‹åˆ°çš„åŒºåŸŸ: {available_regions}")

            # ä¸ºæ¯ä¸ªåŒºåŸŸæå– tab å†…å®¹
            for region_id in available_regions:
                if region_id in self.region_mapping:
                    content_key = self.region_mapping[region_id]
                    region_name = self.region_names.get(region_id, region_id)

                    # æå– tab å†…å®¹æ•°ç»„
                    content = self._extract_region_content_with_tabs(soup, region_id, tab_structure, html_file_path)
                    if content:
                        region_contents[content_key] = content
                        print(f"  âœ“ æå– {region_name} tab å†…å®¹: {len(content)} ä¸ª tab")
        else:
            print("  ğŸ“„ æœªæ£€æµ‹åˆ° tab ç»“æ„ï¼Œä½¿ç”¨ region_processor çš„æ ‡å‡†æå–æ–¹å¼")

            # ä½¿ç”¨ region_processor çš„æ ‡å‡†æå–åŠŸèƒ½
            processor_results = self.region_processor.extract_region_contents(soup, html_file_path)

            # è½¬æ¢ä¸º enhanced_cms_extractor æœŸæœ›çš„æ ¼å¼
            for region_id, region_data in processor_results.items():
                if region_id in self.region_mapping:
                    content_key = self.region_mapping[region_id]
                    # å°†ç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸º HTML å­—ç¬¦ä¸²
                    content = self._convert_region_data_to_html(region_data)
                    if content:
                        region_contents[content_key] = self._clean_html_content(content)
                        print(f"  âœ“ æå– {region_id} å†…å®¹: {len(content)} å­—ç¬¦")

        return region_contents

    def _convert_region_data_to_html(self, region_data: Dict[str, Any]) -> str:
        """å°† region_processor çš„ç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸º HTML å­—ç¬¦ä¸²"""
        if not region_data:
            return ""

        html_parts = []

        # æ·»åŠ å®šä»·è¡¨æ ¼
        pricing_tables = region_data.get('pricing_tables', [])
        if pricing_tables:
            html_parts.append("<div class='pricing-tables'>")
            for table in pricing_tables:
                if isinstance(table, dict) and 'content' in table:
                    html_parts.append(table['content'])
            html_parts.append("</div>")

        # æ·»åŠ åŠŸèƒ½å¯ç”¨æ€§ä¿¡æ¯
        features = region_data.get('feature_availability', [])
        if features:
            html_parts.append("<div class='feature-availability'>")
            for feature in features:
                if isinstance(feature, str):
                    html_parts.append(f"<p>{feature}</p>")
            html_parts.append("</div>")

        # æ·»åŠ åŒºåŸŸè¯´æ˜
        notes = region_data.get('region_notes', [])
        if notes:
            html_parts.append("<div class='region-notes'>")
            for note in notes:
                if isinstance(note, str):
                    html_parts.append(f"<p>{note}</p>")
            html_parts.append("</div>")

        return "\n".join(html_parts)

    def _extract_region_content_with_tabs(self, soup: BeautifulSoup, region_id: str,
                                        tab_structure: List[Dict[str, str]],
                                        html_file_path: str) -> List[Dict[str, str]]:
        """æå–æŒ‡å®šåŒºåŸŸçš„ tab å†…å®¹"""
        print(f"  ğŸ” æå–åŒºåŸŸ {region_id} çš„ tab å†…å®¹...")

        region_content = []

        if not tab_structure:
            # æ²¡æœ‰ tab ç»“æ„ï¼Œæå–æ•´ä½“å†…å®¹
            content = self._extract_single_region_content(soup, region_id, html_file_path)
            return [{"tabName": "å…¨éƒ¨å†…å®¹", "content": self._clean_html_content(content)}]

        # è·å–æ–‡ä»¶åç”¨äºåŒºåŸŸå¤„ç†å™¨
        filename = Path(html_file_path).stem if html_file_path else ""

        # ä¸ºæ¯ä¸ª tab æå–å†…å®¹
        for tab_info in tab_structure:
            tab_name = tab_info['tabName']
            content_id = tab_info['contentId']

            print(f"    ğŸ“‚ å¤„ç† tab: {tab_name} (ID: {content_id})")

            # æŸ¥æ‰¾å¯¹åº”çš„ tab å†…å®¹åŒºåŸŸ
            tab_content_div = soup.find('div', id=content_id)
            if not tab_content_div:
                print(f"      âš  æœªæ‰¾åˆ° tab å†…å®¹åŒºåŸŸ: {content_id}")
                continue

            # åˆ›å»ºè¯¥ tab çš„å‰¯æœ¬è¿›è¡Œå¤„ç†
            tab_soup = BeautifulSoup(str(tab_content_div), 'html.parser')

            # ä½¿ç”¨ region_processor åº”ç”¨åŒºåŸŸç­›é€‰
            try:
                filtered_content = self.region_processor.apply_region_filtering(tab_soup, region_id, filename)
                content_str = str(filtered_content) if filtered_content else ""
            except Exception as e:
                print(f"      âš  åŒºåŸŸç­›é€‰å¤±è´¥: {e}")
                content_str = str(tab_soup)

            # æå–å¤„ç†åçš„å†…å®¹å¹¶æ¸…ç†HTML
            tab_content = {
                'tabName': tab_name,
                'content': self._clean_html_content(content_str)
            }

            region_content.append(tab_content)
            print(f"      âœ“ tab {tab_name} å†…å®¹æå–å®Œæˆ")

        return region_content

    def _get_region_exclude_tables(self, region_id: str, html_file_path: str) -> List[str]:
        """è·å–æŒ‡å®šåŒºåŸŸéœ€è¦æ’é™¤çš„è¡¨æ ¼IDåˆ—è¡¨"""
        filename = Path(html_file_path).stem if html_file_path else ""
        product_config = self.region_processor.region_config.get(filename, {})
        return product_config.get(region_id, [])

    def _apply_region_filtering_to_tab(self, tab_soup: BeautifulSoup, exclude_tables: List[str]) -> BeautifulSoup:
        """å¯¹ tab å†…å®¹åº”ç”¨åŒºåŸŸç­›é€‰"""

        if not exclude_tables:
            return tab_soup

        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        tables = tab_soup.find_all('table')

        for table in tables:
            table_id = table.get('id', '')
            if table_id:
                # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼ˆæ·»åŠ  # å‰ç¼€è¿›è¡ŒåŒ¹é…ï¼‰
                table_id_with_hash = f"#{table_id}"
                if table_id_with_hash in exclude_tables:
                    # ç§»é™¤è¡¨æ ¼åŠå…¶å®¹å™¨
                    container = table.find_parent('div', class_='scroll-table')
                    if container:
                        container.decompose()
                    else:
                        table.decompose()
                    print(f"        âœ— ç§»é™¤è¡¨æ ¼: {table_id}")
                else:
                    print(f"        âœ“ ä¿ç•™è¡¨æ ¼: {table_id}")

        return tab_soup

    def _extract_single_region_content(self, soup: BeautifulSoup, region_id: str, html_file_path: str = "") -> str:
        """æå–å•ä¸ªåŒºåŸŸçš„å†…å®¹ï¼ŒåŸºäºç°æœ‰çš„åŒºåŸŸç­›é€‰é€»è¾‘"""

        try:
            # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ç”¨äºåŒºåŸŸç­›é€‰
            region_soup = BeautifulSoup(str(soup), 'html.parser')

            # è·å–äº§å“é…ç½®ä¿¡æ¯
            filename = Path(html_file_path).stem if html_file_path else ""
            product_config = self.region_processor.region_config.get(filename, {})
            region_tables = product_config.get(region_id, [])

            # åº”ç”¨åŒºåŸŸç­›é€‰ - ä½¿ç”¨åŒºåŸŸå¤„ç†å™¨
            filtered_soup = self.region_processor.apply_region_filtering(
                region_soup, region_id, filename
            )

            # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = self._extract_main_content_for_region(filtered_soup)

            # åœ¨å†…å®¹ä¸­æ·»åŠ table IDä¿¡æ¯
            content_str = str(main_content) if main_content else ""

            # æ·»åŠ åŒºåŸŸå’Œè¡¨æ ¼è¿‡æ»¤ä¿¡æ¯çš„æ³¨é‡Š
            if region_tables:
                filter_info = f"<!-- Region: {region_id}, Filtered table IDs: {', '.join(region_tables)} -->"
                content_str = filter_info + "\n" + content_str
            else:
                filter_info = f"<!-- Region: {region_id}, No table filtering applied -->"
                content_str = filter_info + "\n" + content_str

            return content_str

        except Exception as e:
            print(f"    âš  åŒºåŸŸ {region_id} å†…å®¹æå–å¤±è´¥: {e}")
            return ""

    def _extract_main_content_for_region(self, soup: BeautifulSoup) -> BeautifulSoup:
        """æå–åŒºåŸŸçš„ä¸»è¦å†…å®¹"""

        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content_selectors = [
            '.tab-content',
            '.pricing-page-section',
            '.content',
            'main'
        ]

        for selector in main_content_selectors:
            elements = soup.select(selector)
            if elements:
                # åªå–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ ï¼Œé¿å…é‡å¤
                element = elements[0]
                # ç›´æ¥è¿”å›è¯¥å…ƒç´ çš„å‰¯æœ¬
                return BeautifulSoup(str(element), 'html.parser')

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé€‰æ‹©å™¨ï¼Œè¿”å›æ•´ä¸ªbodyå†…å®¹
        body = soup.find('body')
        if body:
            return BeautifulSoup(str(body), 'html.parser')

        return BeautifulSoup("", 'html.parser')

    def _extract_no_region_content_with_tabs(self, soup: BeautifulSoup, tab_structure: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """æå–æ— åŒºåŸŸé¡µé¢çš„ tab å†…å®¹"""
        print("ğŸ“‚ æå–æ— åŒºåŸŸ tab å†…å®¹...")

        no_region_content = []

        if not tab_structure:
            # æ²¡æœ‰ tab ç»“æ„ï¼Œæå–æ•´ä½“å†…å®¹
            content = self._extract_no_region_content(soup)
            return [{"tabName": "å…¨éƒ¨å†…å®¹", "content": self._clean_html_content(content)}]

        # ä¸ºæ¯ä¸ª tab æå–å†…å®¹
        for tab_info in tab_structure:
            tab_name = tab_info['tabName']
            content_id = tab_info['contentId']

            print(f"    ğŸ“‚ å¤„ç† tab: {tab_name} (ID: {content_id})")

            # æŸ¥æ‰¾å¯¹åº”çš„ tab å†…å®¹åŒºåŸŸ
            tab_content_div = soup.find('div', id=content_id)
            if not tab_content_div:
                print(f"      âš  æœªæ‰¾åˆ° tab å†…å®¹åŒºåŸŸ: {content_id}")
                continue

            # æå– tab å†…å®¹å¹¶æ¸…ç†HTML
            tab_content = {
                'tabName': tab_name,
                'content': self._clean_html_content(str(tab_content_div))
            }

            no_region_content.append(tab_content)
            print(f"      âœ“ tab {tab_name} å†…å®¹æå–å®Œæˆ")

        return no_region_content

    def _extract_no_region_content(self, soup: BeautifulSoup) -> str:
        """æå–æ— åŒºåŸŸé¡µé¢çš„ä¸»ä½“å†…å®¹"""
        print("ğŸ“„ æå–æ— åŒºåŸŸä¸»ä½“å†…å®¹...")
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content_selectors = [
            'main',
            '.main-content',
            '.content',
            '.page-content',
            '.pricing-content'
        ]
        
        for selector in main_content_selectors:
            element = soup.select_one(selector)
            if element:
                return str(element)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„ä¸»å†…å®¹åŒºåŸŸï¼Œæå–bodyä¸­é™¤äº†headerã€navã€footerå¤–çš„å†…å®¹
        body = soup.find('body')
        if body:
            # å¤åˆ¶bodyå†…å®¹
            content_soup = BeautifulSoup(str(body), 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„éƒ¨åˆ†
            for unwanted in content_soup.find_all(['header', 'nav', 'footer']):
                unwanted.decompose()
            
            return str(content_soup)
        
        return ""