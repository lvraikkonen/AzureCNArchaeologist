#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºå‹CMSæå–å™¨
åŸºäºæ–°çš„åˆ†æ¨¡å—æå–éœ€æ±‚ï¼Œæå–Bannerã€æè¿°ã€Q&Aã€å„åŒºåŸŸå†…å®¹ç­‰æ¨¡å—
"""

import json
import re
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse
from bs4 import BeautifulSoup, Tag
from pathlib import Path
from datetime import datetime


class EnhancedCMSExtractor:
    """å¢å¼ºå‹CMSæå–å™¨ - æŒ‰æ¨¡å—æå–é¡µé¢å†…å®¹"""
    
    def __init__(self, output_dir: str = "enhanced_cms_output", config_file: str = "soft-category.json"):
        """
        åˆå§‹åŒ–å¢å¼ºå‹CMSæå–å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.config_file = config_file
        
        # åŒºåŸŸæ˜ å°„
        self.region_mapping = {
            "north-china": "NorthChinaContent",
            "north-china2": "NorthChina2Content", 
            "north-china3": "NorthChina3Content",
            "east-china": "EastChinaContent",
            "east-china2": "EastChina2Content",
            "east-china3": "EastChina3Content"
        }
        
        # åŒºåŸŸä¸­æ–‡åæ˜ å°„
        self.region_names = {
            "north-china": "ä¸­å›½åŒ—éƒ¨",
            "north-china2": "ä¸­å›½åŒ—éƒ¨2",
            "north-china3": "ä¸­å›½åŒ—éƒ¨3", 
            "east-china": "ä¸­å›½ä¸œéƒ¨",
            "east-china2": "ä¸­å›½ä¸œéƒ¨2",
            "east-china3": "ä¸­å›½ä¸œéƒ¨3"
        }
        
        # æ–‡ä»¶ååˆ°äº§å“åç§°çš„æ˜ å°„è¡¨
        self.filename_to_product_mapping = {
            "anomaly-detector-index.html": "Anomaly Detector",
            "api-management-index.html": "API Management",
            "cosmos-db-index.html": "Azure Cosmos DB",
            "machine-learning-index.html": "Machine Learning Server",
            "mysql-index.html": "Azure Database for MySQL",
            "postgresql-index.html": "Azure Database for PostgreSQL",
            "power-bi-embedded-index.html": "Power BI Embedded",
            "search-index.html": "Azure Search",
            "ssis-index.html": "Data Factory SSIS",
            "storage-files-index.html": "Storage Files",
            "sql-database-index.html": "SQL Database",
            "microsoft-entra-external-id-index.html": "Microsoft Entra External ID",
            "data-factory-index.html": "Data Factory Data Pipeline",
            "cognitive-services-index.html": "Cognitive Services"
        }
        
        print(f"âœ“ å¢å¼ºå‹CMSæå–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“‹ æ”¯æŒçš„äº§å“æ•°é‡: {len(self.filename_to_product_mapping)}")
    
    def detect_product_name(self, html_file_path: str) -> str:
        """
        ä»HTMLæ–‡ä»¶è·¯å¾„æ£€æµ‹äº§å“åç§°
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            äº§å“åç§°ï¼ˆå¦‚æœæ‰¾åˆ°æ˜ å°„ï¼‰æˆ–ç©ºå­—ç¬¦ä¸²
        """
        
        # æå–æ–‡ä»¶å
        filename = Path(html_file_path).name
        
        # æŸ¥æ‰¾ç›´æ¥æ˜ å°„
        if filename in self.filename_to_product_mapping:
            product_name = self.filename_to_product_mapping[filename]
            print(f"ğŸ” æ£€æµ‹åˆ°äº§å“: {filename} -> {product_name}")
            return product_name
        
        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆä¾‹å¦‚å¤„ç†å˜ä½“æ–‡ä»¶åï¼‰
        for mapped_filename, product_name in self.filename_to_product_mapping.items():
            # æå–æ ¸å¿ƒåç§°éƒ¨åˆ†ï¼ˆå»æ‰-index.htmlåç¼€ï¼‰
            mapped_core = mapped_filename.replace('-index.html', '')
            file_core = filename.replace('-index.html', '')
            
            if mapped_core == file_core:
                print(f"ğŸ” æ¨¡ç³ŠåŒ¹é…åˆ°äº§å“: {filename} -> {product_name}")
                return product_name
        
        # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»æ–‡ä»¶åæ¨æ–­
        inferred_name = self._infer_product_name_from_filename(filename)
        if inferred_name:
            print(f"ğŸ” æ¨æ–­äº§å“åç§°: {filename} -> {inferred_name}")
            return inferred_name
        
        print(f"âš ï¸ æœªæ‰¾åˆ°äº§å“æ˜ å°„: {filename}")
        return ""
    
    def _infer_product_name_from_filename(self, filename: str) -> str:
        """
        ä»æ–‡ä»¶åæ¨æ–­äº§å“åç§°
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            æ¨æ–­çš„äº§å“åç§°
        """
        
        # ç§»é™¤åç¼€
        core_name = filename.replace('-index.html', '').replace('.html', '')
        
        # ç‰¹æ®Šæƒ…å†µå¤„ç†
        special_cases = {
            'storage-files': 'Storage Files',
            'api-management': 'API Management',
            'power-bi-embedded': 'Power BI Embedded',
            'machine-learning': 'Machine Learning Server',
            'anomaly-detector': 'Anomaly Detector',
            'sql-database': 'SQL Database',
            'microsoft-entra-external-id': 'Microsoft Entra External ID',
            'data-factory': 'Data Factory Data Pipeline',
            'cognitive-services': 'Cognitive Services'
        }
        
        if core_name in special_cases:
            return special_cases[core_name]
        
        # é€šç”¨è§„åˆ™ï¼šå°†è¿å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œæ¯ä¸ªå•è¯é¦–å­—æ¯å¤§å†™
        words = core_name.split('-')
        title_case_words = []
        
        for word in words:
            # ç‰¹æ®Šå•è¯å¤„ç†
            if word.lower() == 'mysql':
                title_case_words.append('MySQL')
            elif word.lower() == 'postgresql':
                title_case_words.append('PostgreSQL')
            elif word.lower() == 'cosmos':
                title_case_words.append('Cosmos')
            elif word.lower() == 'db':
                title_case_words.append('DB')
            elif word.lower() == 'azure':
                title_case_words.append('Azure')
            elif word.lower() == 'ssis':
                title_case_words.append('SSIS')
            else:
                title_case_words.append(word.capitalize())
        
        result = ' '.join(title_case_words)
        
        # å¦‚æœç»“æœåŒ…å«æ•°æ®åº“ç›¸å…³è¯æ±‡ï¼Œæ·»åŠ Azureå‰ç¼€
        if any(db_word in result.lower() for db_word in ['mysql', 'postgresql', 'cosmos']):
            if not result.startswith('Azure'):
                result = 'Azure Database for ' + result
        
        return result
    
    def extract_cms_content(self, html_file_path: str, url: str = "") -> Dict[str, Any]:
        """
        æå–CMSæ‰€éœ€çš„åˆ†æ¨¡å—å†…å®¹
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            url: é¡µé¢URLï¼ˆç”¨äºæå–slugï¼‰
            
        Returns:
            åŒ…å«æ‰€æœ‰æ¨¡å—å†…å®¹çš„å­—å…¸
        """
        
        print(f"\nğŸ”§ å¼€å§‹æå–å¢å¼ºå‹CMSå†…å®¹")
        print(f"ğŸ“ æºæ–‡ä»¶: {html_file_path}")
        print(f"ğŸ”— URL: {url}")
        print("=" * 70)
        
        try:
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # é¢„å¤„ç†å›¾ç‰‡è·¯å¾„
            processed_soup = self._preprocess_image_paths(soup)
            
            # æå–å„ä¸ªæ¨¡å—
            result = {
                "Title": self._extract_title(processed_soup),
                "MetaDescription": self._extract_meta_description(processed_soup),
                "MetaKeywords": self._extract_meta_keywords(processed_soup),
                "MSServiceName": self._extract_ms_service_name(processed_soup),
                "Slug": self._extract_slug(url),
                "DescriptionContent": self._extract_description_content(processed_soup),
                "Language": self._detect_language(processed_soup),
                "NavigationTitle": self._extract_navigation_title(processed_soup),
                "BannerContent": self._extract_banner_content(processed_soup),
                "QaContent": self._extract_qa_content(processed_soup),
                "HasRegion": self._check_has_region(processed_soup),
                "NoRegionContent": ""
            }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒºåŸŸé€‰æ‹©
            if result["HasRegion"]:
                # æå–å„åŒºåŸŸå†…å®¹
                region_contents = self._extract_region_contents(processed_soup, html_file_path)
                result.update(region_contents)
            else:
                # æ²¡æœ‰åŒºåŸŸé€‰æ‹©ï¼Œæå–ä¸»ä½“å†…å®¹åˆ°NoRegionContent
                result["NoRegionContent"] = self._extract_no_region_content(processed_soup)
            
            # æ¸…ç†æ‰€æœ‰Contentå­—æ®µä¸­çš„å¤šä½™æ ‡ç­¾å’Œç¬¦å·
            content_fields = ["DescriptionContent", "BannerContent", "QaContent", "NoRegionContent"]
            
            # åŠ¨æ€æ·»åŠ å®é™…å­˜åœ¨çš„åŒºåŸŸå†…å®¹å­—æ®µ
            for field_name in result.keys():
                if field_name.endswith("Content") and field_name not in content_fields:
                    content_fields.append(field_name)
            
            for field in content_fields:
                if field in result and result[field]:
                    result[field] = self._clean_html_content(result[field])
            
            # æ·»åŠ æå–å…ƒæ•°æ®
            result["extraction_metadata"] = {
                "extracted_at": datetime.now().isoformat(),
                "source_file": html_file_path,
                "extractor_version": "enhanced_cms_v1.0"
            }
            
            print(f"\nâœ… å¢å¼ºå‹CMSå†…å®¹æå–å®Œæˆï¼")
            print(f"ğŸ“Š åŒ…å«åŒºåŸŸ: {result['HasRegion']}")
            print(f"ğŸ“„ Banneré•¿åº¦: {len(result['BannerContent'])} å­—ç¬¦")
            print(f"ğŸ“„ Q&Aé•¿åº¦: {len(result['QaContent'])} å­—ç¬¦")
            
            return result
            
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}
    
    def _preprocess_image_paths(self, soup: BeautifulSoup) -> BeautifulSoup:
        """é¢„å¤„ç†å›¾ç‰‡è·¯å¾„ï¼Œæ·»åŠ {img_hostname}å ä½ç¬¦"""
        
        print("ğŸ–¼ï¸ é¢„å¤„ç†å›¾ç‰‡è·¯å¾„...")
        
        # å¤„ç†imgæ ‡ç­¾çš„srcå±æ€§
        img_count = 0
        for img in soup.find_all('img'):
            src = img.get('src')
            if src and src.startswith('/'):
                img['src'] = f"{{img_hostname}}{src}"
                img_count += 1
        
        # å¤„ç†styleå±æ€§ä¸­çš„background-image
        style_count = 0
        for element in soup.find_all(style=True):
            style = element.get('style', '')
            if 'background-image:' in style and 'url(' in style:
                # åŒ¹é… url("/path/to/image") æˆ– url('/path/to/image')
                pattern = r'url\(["\']?(/[^"\']*)["\']?\)'
                def replace_url(match):
                    path = match.group(1)
                    return f'url("{{img_hostname}}{path}")'
                
                new_style = re.sub(pattern, replace_url, style)
                if new_style != style:
                    element['style'] = new_style
                    style_count += 1
        
        # å¤„ç†data-configå±æ€§ä¸­çš„å›¾ç‰‡è·¯å¾„
        data_config_count = 0
        for element in soup.find_all(attrs={'data-config': True}):
            data_config = element.get('data-config', '')
            if data_config and ('backgroundImage' in data_config or 'background-image' in data_config):
                # åŒ¹é… backgroundImage æˆ– background-image åé¢çš„å›¾ç‰‡è·¯å¾„
                # æ”¯æŒæ ¼å¼ï¼š'backgroundImage':'/path/to/image' æˆ– "backgroundImage":"/path/to/image"
                pattern = r'(["\'](?:backgroundImage|background-image)["\']:\s*["\'])(/[^"\']*?)(["\'])'
                def replace_bg_image(match):
                    prefix = match.group(1)
                    path = match.group(2)
                    suffix = match.group(3)
                    return f'{prefix}{{img_hostname}}{path}{suffix}'
                
                new_data_config = re.sub(pattern, replace_bg_image, data_config)
                if new_data_config != data_config:
                    element['data-config'] = new_data_config
                    data_config_count += 1
        
        print(f"  âœ“ å¤„ç†äº† {img_count} ä¸ªimgæ ‡ç­¾ã€{style_count} ä¸ªstyleå±æ€§å’Œ {data_config_count} ä¸ªdata-configå±æ€§")
        
        return soup
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text(strip=True)
        return ""
    
    def _extract_meta_description(self, soup: BeautifulSoup) -> str:
        """æå–Metaæè¿°"""
        
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')
        return ""
    
    def _extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """æå–Metaå…³é”®è¯"""
        
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            return meta_keywords.get('content', '')
        return ""
    
    def _extract_ms_service_name(self, soup: BeautifulSoup) -> str:
        """æå–MSServiceNameå­—æ®µï¼Œä»pure-content divå†…çš„tagså…ƒç´ ä¸­çš„ms.serviceå±æ€§"""
        
        print("ğŸ·ï¸ æå–MSServiceName...")
        
        # æŸ¥æ‰¾pure-content div
        pure_content_div = soup.find('div', class_='pure-content')
        if pure_content_div:
            # åœ¨pure-content divå†…æŸ¥æ‰¾tagså…ƒç´ 
            tags_element = pure_content_div.find('tags')
            if tags_element:
                # æå–ms.serviceå±æ€§å€¼
                ms_service = tags_element.get('ms.service', '')
                if ms_service:
                    print(f"  âœ“ æ‰¾åˆ°MSServiceName: {ms_service}")
                    return ms_service
                else:
                    print("  âš  tagså…ƒç´ ä¸­æ²¡æœ‰ms.serviceå±æ€§")
            else:
                print("  âš  pure-content divä¸­æ²¡æœ‰æ‰¾åˆ°tagså…ƒç´ ")
        else:
            print("  âš  æ²¡æœ‰æ‰¾åˆ°pure-content div")
        
        return ""
    
    def _extract_slug(self, url: str) -> str:
        """ä»URLæå–slug"""
        
        if not url:
            return ""
        
        try:
            parsed = urlparse(url)
            path = parsed.path
            
            # æå–/details/ä¹‹ååˆ°/index.htmlä¹‹å‰çš„å†…å®¹ï¼Œç”¨-è¿æ¥
            # ä¾‹å¦‚ /pricing/details/storage/files/index.html -> storage-files
            # ä¾‹å¦‚ /pricing/details/api-management/index.html -> api-management
            if '/details/' in path:
                # æ‰¾åˆ°/details/ä¹‹åçš„éƒ¨åˆ†
                after_details = path.split('/details/')[1]
                
                # ç§»é™¤/index.htmlåç¼€
                if after_details.endswith('/index.html'):
                    after_details = after_details[:-11]  # ç§»é™¤'/index.html'
                elif after_details.endswith('/'):
                    after_details = after_details[:-1]  # ç§»é™¤æœ«å°¾çš„'/'
                
                # åˆ†å‰²è·¯å¾„å¹¶ç”¨-è¿æ¥
                path_parts = [p for p in after_details.split('/') if p]
                if path_parts:
                    return '-'.join(path_parts)
        except:
            pass
        
        return ""
    
    def _extract_description_content(self, soup: BeautifulSoup) -> str:
        """æå–Bannerä¸‹ç¬¬ä¸€ä¸ªsectionä½œä¸ºæè¿°å†…å®¹"""
        
        print("ğŸ“ æå–æè¿°å†…å®¹...")
        
        # æŸ¥æ‰¾banneråçš„ç¬¬ä¸€ä¸ªå†…å®¹section
        banner = soup.find('div', class_='common-banner')
        if banner:
            # æŸ¥æ‰¾banneråçš„ç¬¬ä¸€ä¸ªpricing-page-sectionæˆ–section
            next_section = banner.find_next_sibling('div', class_='pricing-page-section')
            if next_section:
                return str(next_section)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°pricing-page-sectionï¼ŒæŸ¥æ‰¾æ™®é€šsection
            next_section = banner.find_next_sibling('section')
            if next_section:
                return str(next_section)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°bannerï¼Œå°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªpricing-page-section
        first_pricing_section = soup.find('div', class_='pricing-page-section')
        if first_pricing_section:
            return str(first_pricing_section)
        
        # æœ€åå°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªsection
        first_section = soup.find('section')
        if first_section:
            return str(first_section)
        
        return ""
    
    def _detect_language(self, soup: BeautifulSoup) -> str:
        """æ£€æµ‹é¡µé¢è¯­è¨€"""
        
        # æ£€æŸ¥htmlæ ‡ç­¾çš„langå±æ€§
        html_tag = soup.find('html')
        if html_tag:
            lang = html_tag.get('lang', '')
            if 'en' in lang.lower():
                return 'en-US'
        
        # é»˜è®¤è¿”å›ä¸­æ–‡
        return 'zh-CN'
    
    def _extract_navigation_title(self, soup: BeautifulSoup) -> str:
        """æå–å¯¼èˆªæ ‡é¢˜"""
        
        # æŸ¥æ‰¾common-banner-title > h2
        banner_title = soup.find('div', class_='common-banner-title')
        if banner_title:
            h2 = banner_title.find('h2')
            if h2:
                return h2.get_text(strip=True)
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾é¡µé¢ä¸»æ ‡é¢˜
        main_title = soup.find('h1')
        if main_title:
            return main_title.get_text(strip=True)
        
        return ""
    
    def _extract_banner_content(self, soup: BeautifulSoup) -> str:
        """æå–Bannerå†…å®¹"""
        
        print("ğŸ¨ æå–Bannerå†…å®¹...")
        
        banner = soup.find('div', class_='common-banner')
        if banner:
            # æ ‡å‡†åŒ–å›¾ç‰‡æ ¼å¼
            standardized_banner = self._standardize_banner_images(banner)
            return str(standardized_banner)
        
        return ""
    
    def _standardize_banner_images(self, banner: Tag) -> str:
        """æ ‡å‡†åŒ–Bannerä¸­çš„å›¾ç‰‡æ ¼å¼ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹"""
        
        # æå–èƒŒæ™¯å›¾ç‰‡
        background_image = self._extract_background_image(banner)
        
        # æå–å›¾æ ‡
        icon_image = self._extract_icon_image(banner)
        
        # æå–æ–‡æœ¬å†…å®¹
        text_content = self._extract_banner_text_content(banner)
        
        # ç”Ÿæˆæ ‡å‡†åŒ–HTML
        if background_image or icon_image or text_content:
            standardized_html = ""
            
            # æ·»åŠ èƒŒæ™¯å›¾ç‰‡div
            if background_image:
                standardized_html += f'<div class="common-banner-image" style="background-image: url(&quot;{background_image}&quot;);">'
            else:
                standardized_html += '<div class="common-banner-image">'
            
            # æ·»åŠ å›¾æ ‡
            if icon_image:
                standardized_html += f'<img src="{icon_image}" alt="imgAlt">'
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹
            if text_content:
                standardized_html += text_content
            
            standardized_html += '</div>'
            
            return standardized_html
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å†…å®¹ï¼Œè¿”å›åŸå§‹banner
        return str(banner)
    
    def _extract_background_image(self, banner: Tag) -> str:
        """ä»bannerä¸­æå–èƒŒæ™¯å›¾ç‰‡URL"""
        
        # æŸ¥æ‰¾data-configå±æ€§
        data_config = banner.get('data-config', '')
        if data_config:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–backgroundImage
            import re
            pattern = r'["\']backgroundImage["\']:\s*["\']([^"\']*)["\']'
            match = re.search(pattern, data_config)
            if match:
                return match.group(1)
        
        # æŸ¥æ‰¾styleå±æ€§ä¸­çš„background-image
        style = banner.get('style', '')
        if 'background-image' in style:
            pattern = r'background-image:\s*url\(["\']?([^"\']*)["\']?\)'
            match = re.search(pattern, style)
            if match:
                return match.group(1)
        
        return ""
    
    def _extract_icon_image(self, banner: Tag) -> str:
        """ä»bannerä¸­æå–å›¾æ ‡å›¾ç‰‡URL"""
        
        # é¦–å…ˆæŸ¥æ‰¾imgæ ‡ç­¾
        img_tag = banner.find('img')
        if img_tag:
            src = img_tag.get('src', '')
            if src:
                return src
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°imgæ ‡ç­¾ï¼ŒæŸ¥æ‰¾svgæ ‡ç­¾
        svg_tag = banner.find('svg')
        if svg_tag:
            # æ£€æŸ¥svgæ ‡ç­¾æ˜¯å¦æœ‰idå±æ€§ï¼Œé€šå¸¸åŒ…å«äº§å“ä¿¡æ¯
            svg_id = svg_tag.get('id', '')
            if svg_id:
                # ä¸ºsvgç”Ÿæˆä¸€ä¸ªå ä½ç¬¦è·¯å¾„ï¼ŒåŸºäºid
                # ä¾‹å¦‚ï¼šsvg-storage/files -> storage-filesçš„å›¾æ ‡
                if 'svg-' in svg_id:
                    product_name = svg_id.replace('svg-', '').replace('\\', '-')
                    return f"{{img_hostname}}/Images/marketing-resource/css/{product_name}_icon.svg"
            
            # æ£€æŸ¥svgå†…éƒ¨çš„symbolå…ƒç´ 
            symbol_tag = svg_tag.find('symbol')
            if symbol_tag:
                symbol_id = symbol_tag.get('id', '')
                if symbol_id and 'svg-' in symbol_id:
                    product_name = symbol_id.replace('svg-', '').replace('\\', '-')
                    return f"{{img_hostname}}/Images/marketing-resource/css/{product_name}_icon.svg"
        
        return ""
    
    def _extract_banner_text_content(self, banner: Tag) -> str:
        """ä»bannerä¸­æå–æ–‡æœ¬å†…å®¹ï¼ˆæ ‡é¢˜ã€æè¿°ç­‰ï¼‰"""
        
        # æŸ¥æ‰¾common-banner-titleå®¹å™¨
        title_container = banner.find('div', class_='common-banner-title')
        if not title_container:
            return ""
        
        text_content = ""
        
        # æå–ä¸»æ ‡é¢˜ (h2)
        h2_tag = title_container.find('h2')
        if h2_tag:
            # åˆ›å»ºh2æ ‡ç­¾çš„å‰¯æœ¬ï¼Œç§»é™¤imgæ ‡ç­¾
            h2_copy = h2_tag.__copy__()
            for img in h2_copy.find_all('img'):
                img.decompose()
            text_content += str(h2_copy)
        
        # æå–å‰¯æ ‡é¢˜ (h4)
        h4_tag = title_container.find('h4')
        if h4_tag:
            text_content += str(h4_tag)
        
        # æå–å…¶ä»–æ ‡é¢˜çº§åˆ« (h3, h5, h6)
        for tag_name in ['h3', 'h5', 'h6']:
            tag = title_container.find(tag_name)
            if tag:
                text_content += str(tag)
        
        return text_content
    
    def _clean_html_content(self, content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹ä¸­çš„å¤šä½™æ ‡ç­¾å’Œç¬¦å·"""
        
        if not content:
            return content
        
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºç™½ç¬¦
        content = re.sub(r'\n+', ' ', content)  # å°†å¤šä¸ªæ¢è¡Œç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        content = re.sub(r'\s+', ' ', content)  # å°†å¤šä¸ªç©ºç™½ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        
        # ç§»é™¤å¤šä½™çš„divæ ‡ç­¾åŒ…è£…ï¼ˆä¿ç•™æœ‰ç”¨çš„classå’Œidï¼‰
        # åªç§»é™¤çº¯ç²¹çš„åŒ…è£…divï¼Œä¿ç•™æœ‰æ„ä¹‰çš„div
        content = re.sub(r'<div>\s*</div>', '', content)  # ç§»é™¤ç©ºçš„divæ ‡ç­¾
        
        # æ¸…ç†æ ‡ç­¾é—´çš„å¤šä½™ç©ºç™½
        content = re.sub(r'>\s+<', '><', content)  # ç§»é™¤æ ‡ç­¾é—´çš„ç©ºç™½
        
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½
        content = content.strip()
        
        return content
    
    def _extract_qa_content(self, soup: BeautifulSoup) -> str:
        """æå–Q&Aå†…å®¹ä»¥åŠæ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®å†…å®¹"""
        
        print("â“ æå–Q&Aå†…å®¹...")
        
        qa_content = ""
        
        # ç°æœ‰çš„FAQæå–é€»è¾‘
        faq_containers = [
            soup.find('div', class_='faq'),
            soup.find('div', class_='qa'),
            soup.find('section', class_='faq'),
            soup.find('section', class_='qa')
        ]
        
        for container in faq_containers:
            if container:
                qa_content += str(container)
        
        # æŸ¥æ‰¾åŒ…å«FAQç»“æ„çš„åˆ—è¡¨
        faq_lists = soup.find_all('ul', class_='faq-list')
        if faq_lists:
            # å¦‚æœæœ‰å¤šä¸ªFAQåˆ—è¡¨ï¼Œåˆå¹¶å®ƒä»¬
            for faq_list in faq_lists:
                qa_content += str(faq_list)
        
        # æŸ¥æ‰¾åŒ…å«icon-plusçš„åˆ—è¡¨ï¼ˆFAQå±•å¼€å›¾æ ‡ï¼‰
        for ul in soup.find_all('ul'):
            if ul.find('i', class_='icon-plus'):
                qa_content += str(ul)
        
        # æ–°å¢ï¼šæå–æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®å†…å®¹
        print("ğŸ› ï¸ æå–æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®å†…å®¹...")
        
        # æŸ¥æ‰¾æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®éƒ¨åˆ†
        support_sections = soup.find_all('div', class_='pricing-page-section')
        for section in support_sections:
            h2_tag = section.find('h2')
            if h2_tag and 'æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®' in h2_tag.get_text(strip=True):
                qa_content += str(section)
                print("  âœ“ æ‰¾åˆ°æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®éƒ¨åˆ†")
                break
        
        # æŸ¥æ‰¾æ³¨é‡Šä¸­çš„æ”¯æŒä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        html_content = str(soup)
        support_comment_pattern = r'<!--BEGIN: Support and service code chunk-->(.*?)<!--END: Support and service code chunk-->'
        support_matches = re.findall(support_comment_pattern, html_content, re.DOTALL)
        if support_matches:
            for match in support_matches:
                # è§£ææ³¨é‡Šä¸­çš„HTMLå†…å®¹
                if match.strip() and not match.strip().startswith('<!--'):
                    qa_content += f"<!-- æ”¯æŒä¿¡æ¯ -->{match}<!-- /æ”¯æŒä¿¡æ¯ -->"
                    print("  âœ“ æ‰¾åˆ°æ³¨é‡Šä¸­çš„æ”¯æŒä¿¡æ¯")
        
        return qa_content
    
    def _check_has_region(self, soup: BeautifulSoup) -> bool:
        """æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰åŒºåŸŸé€‰æ‹©"""
        
        print("ğŸŒ æ£€æŸ¥åŒºåŸŸé€‰æ‹©...")
        
        # æŸ¥æ‰¾åŒºåŸŸé€‰æ‹©ç›¸å…³çš„å…ƒç´ 
        region_indicators = [
            soup.find('div', class_='region-container'),
            soup.find('select', class_='region-selector'),
            soup.find('div', class_='software-kind'),
            soup.find('div', attrs={'data-region': True})
        ]
        
        for indicator in region_indicators:
            if indicator:
                print(f"  âœ“ å‘ç°åŒºåŸŸé€‰æ‹©å™¨: {indicator.name}")
                return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªåŒºåŸŸç›¸å…³çš„å±æ€§
        region_elements = soup.find_all(attrs={'data-region': True})
        if len(region_elements) > 1:
            print(f"  âœ“ å‘ç° {len(region_elements)} ä¸ªåŒºåŸŸå…ƒç´ ")
            return True
        
        return False
    
    def _detect_available_regions(self, soup: BeautifulSoup) -> Dict[str, str]:
        """åŠ¨æ€æ£€æµ‹HTMLä¸­å®é™…å­˜åœ¨çš„åŒºåŸŸ"""
        available_regions = {}
        
        # æ£€æŸ¥åŒºåŸŸé€‰æ‹©å™¨ä¸­çš„é€‰é¡¹
        region_selectors = soup.find_all('a', {'data-href': True}) + soup.find_all('option', {'data-href': True})
        
        for selector in region_selectors:
            data_href = selector.get('data-href', '').replace('#', '')
            if data_href and data_href.startswith(('north-china', 'east-china')):
                region_text = selector.get_text(strip=True)
                available_regions[data_href] = region_text
        
        return available_regions
    
    def _extract_region_contents(self, soup: BeautifulSoup, html_file_path: str = "") -> Dict[str, str]:
        """æå–å„åŒºåŸŸçš„å†…å®¹"""
        
        print("ğŸŒ æå–å„åŒºåŸŸå†…å®¹...")
        
        region_contents = {}
        
        # åŠ¨æ€æ£€æµ‹HTMLä¸­å®é™…å­˜åœ¨çš„åŒºåŸŸ
        available_regions = self._detect_available_regions(soup)
        print(f"  ğŸ” æ£€æµ‹åˆ°çš„åŒºåŸŸ: {list(available_regions.keys())}")
        
        # åªå¤„ç†å®é™…å­˜åœ¨çš„åŒºåŸŸ
        for region_id, region_name in available_regions.items():
            if region_id in self.region_mapping:
                content_key = self.region_mapping[region_id]
                content = self._extract_single_region_content(soup, region_id, html_file_path)
                if content:
                    region_contents[content_key] = content
                    print(f"  âœ“ æå– {region_name} å†…å®¹: {len(content)} å­—ç¬¦")
        
        return region_contents
    
    def _extract_single_region_content(self, soup: BeautifulSoup, region_id: str, html_file_path: str = "") -> str:
        """æå–å•ä¸ªåŒºåŸŸçš„å†…å®¹ï¼ŒåŸºäºç°æœ‰çš„åŒºåŸŸç­›é€‰é€»è¾‘"""
        
        try:
            # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ç”¨äºåŒºåŸŸç­›é€‰
            region_soup = BeautifulSoup(str(soup), 'html.parser')
            
            # å¯¼å…¥ç°æœ‰çš„åŒºåŸŸç­›é€‰é€»è¾‘
            from ..core.config_manager import ConfigManager
            
            # åˆ›å»ºé…ç½®ç®¡ç†å™¨
            config_manager = ConfigManager(self.config_file)
            
            # æ£€æµ‹äº§å“åç§°
            product_name = self.detect_product_name(html_file_path)
            if not product_name:
                product_name = "API Management"  # é»˜è®¤å€¼
            
            # è®¾ç½®æ´»è·ƒåŒºåŸŸå¹¶åº”ç”¨ç­›é€‰
            config_manager.region_filter.set_active_region(region_id, product_name)
            
            # åº”ç”¨åŒºåŸŸç­›é€‰ - éšè—ä¸å±äºå½“å‰åŒºåŸŸçš„è¡¨æ ¼
            filtered_count, retained_count, retained_table_ids = self._apply_region_filtering_to_soup(
                region_soup, region_id, config_manager
            )
            
            # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = self._extract_main_content_for_region(region_soup)
            
            return str(main_content) if main_content else ""
            
        except Exception as e:
            print(f"    âš  åŒºåŸŸ {region_id} å†…å®¹æå–å¤±è´¥: {e}")
            return ""
    
    def _extract_no_region_content(self, soup: BeautifulSoup) -> str:
        """æå–æ— åŒºåŸŸé¡µé¢çš„ä¸»ä½“å†…å®¹"""
        
        print("ğŸ“„ æå–æ— åŒºåŸŸä¸»ä½“å†…å®¹...")
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content_selectors = [
            'main',
            '.main-content',
            '.content',
            '.page-content',
            '.pricing-content'
        ]
        
        for selector in main_content_selectors:
            element = soup.select_one(selector)
            if element:
                return str(element)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„ä¸»å†…å®¹åŒºåŸŸï¼Œæå–bodyä¸­é™¤äº†headerã€navã€footerå¤–çš„å†…å®¹
        body = soup.find('body')
        if body:
            # å¤åˆ¶bodyå†…å®¹
            content_soup = BeautifulSoup(str(body), 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„éƒ¨åˆ†
            for unwanted in content_soup.find_all(['header', 'nav', 'footer']):
                unwanted.decompose()
            
            return str(content_soup)
        
        return ""
    
    def save_cms_content(self, content: Dict[str, Any], filename: str = "") -> str:
        """ä¿å­˜CMSå†…å®¹åˆ°JSONæ–‡ä»¶"""
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"enhanced_cms_content_{timestamp}.json"
        
        if not filename.endswith('.json'):
            filename += '.json'
        
        file_path = self.output_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(content, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ CMSå†…å®¹å·²ä¿å­˜: {file_path}")
        print(f"ğŸ“Š å†…å®¹å¤§å°: {len(json.dumps(content, ensure_ascii=False)):,} å­—ç¬¦")
        
        return str(file_path)
    
    def _apply_region_filtering_to_soup(self, soup: BeautifulSoup, region_id: str, config_manager) -> tuple:
        """åº”ç”¨åŒºåŸŸç­›é€‰åˆ°soupå¯¹è±¡"""
        
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        tables = soup.find_all('table')
        filtered_count = 0
        retained_count = 0
        retained_table_ids = []
        
        for table in tables:
            table_id = table.get('id', '')
            if table_id:
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿‡æ»¤æ­¤è¡¨æ ¼ï¼ˆåŒºåŸŸä¿¡æ¯å·²ç»åœ¨set_active_regionä¸­è®¾ç½®ï¼‰
                should_filter = config_manager.region_filter.should_filter_table(table_id)
                
                if should_filter:
                    # éšè—è¡¨æ ¼
                    table.decompose()
                    filtered_count += 1
                else:
                    # ä¿ç•™è¡¨æ ¼
                    retained_count += 1
                    retained_table_ids.append(table_id)
        
        return filtered_count, retained_count, retained_table_ids
    
    def _extract_main_content_for_region(self, soup: BeautifulSoup) -> BeautifulSoup:
        """æå–åŒºåŸŸçš„ä¸»è¦å†…å®¹"""
        
        # åˆ›å»ºæ–°çš„å†…å®¹å®¹å™¨
        content_soup = BeautifulSoup("", 'html.parser')
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content_selectors = [
            '.tab-content',
            '.pricing-page-section',
            '.content',
            'main'
        ]
        
        for selector in main_content_selectors:
            elements = soup.select(selector)
            if elements:
                # åªå–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ ï¼Œé¿å…é‡å¤
                element = elements[0]
                # ç›´æ¥è¿”å›è¯¥å…ƒç´ çš„å‰¯æœ¬
                return BeautifulSoup(str(element), 'html.parser')
                break
        
        return content_soup
    
    def process_html_file(self, html_file_path: str, url: str = "", 
                         output_filename: str = "") -> Dict[str, Any]:
        """
        å¤„ç†HTMLæ–‡ä»¶ï¼Œæå–å¹¶ä¿å­˜CMSå†…å®¹
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            url: é¡µé¢URL
            output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æå–çš„å†…å®¹å­—å…¸
        """
        
        # æå–å†…å®¹
        content = self.extract_cms_content(html_file_path, url)
        
        if "error" not in content:
            # ä¿å­˜å†…å®¹
            output_path = self.save_cms_content(content, output_filename)
            content["output_file"] = output_path
        
        return content