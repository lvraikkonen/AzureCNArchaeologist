#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºå‹CMSæå–å™¨
æ•´åˆæ‰€æœ‰æœ€ä½³åŠŸèƒ½çš„ä¸»åŠ›æå–å™¨ï¼Œæ”¯æŒå¤§å‹HTMLæ–‡ä»¶å¤„ç†
"""

import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.core.product_manager import ProductManager
from src.core.region_processor import RegionProcessor
from src.core.config_manager import ConfigManager
from src.utils.html_utils import (
    create_simple_element, preprocess_image_paths, clean_html_content
)
from src.utils.faq_utils import extract_qa_content
from src.utils.content_utils import (
    find_main_content_area, extract_banner_text_content, 
    extract_structured_content
)
from src.utils.validation_utils import validate_extracted_data
from src.utils.large_html_utils import LargeHTMLProcessor


class EnhancedCMSExtractor:
    """å¢å¼ºå‹CMSæå–å™¨ - æ•´åˆæ‰€æœ‰æœ€ä½³åŠŸèƒ½"""

    def __init__(self, output_dir: str, config_file: str = ""):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.product_manager = ProductManager()
        self.region_processor = RegionProcessor(config_file)
        self.config_manager = ConfigManager()
        self.large_html_processor = LargeHTMLProcessor()
        
        print(f"ğŸš€ å¢å¼ºå‹CMSæå–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

    def extract_cms_content(self, html_file_path: str, url: str = "") -> Dict[str, Any]:
        """æå–CMSå†…å®¹ - æ™ºèƒ½å¤„ç†å¤§å‹æ–‡ä»¶"""
        
        print(f"\nğŸ”§ å¼€å§‹æå–å¢å¼ºå‹CMSå†…å®¹")
        print(f"ğŸ“ æºæ–‡ä»¶: {html_file_path}")

        # æ£€æµ‹äº§å“ç±»å‹
        product_key = self._detect_product_key_from_path(html_file_path)
        
        if not product_key:
            print("âš  æ— æ³•æ£€æµ‹äº§å“ç±»å‹ï¼Œä½¿ç”¨é€šç”¨æå–é€»è¾‘")
            product_key = "unknown"

        # è·å–å¤„ç†ç­–ç•¥
        try:
            strategy = self.product_manager.get_processing_strategy(html_file_path, product_key)
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {strategy['size_mb']:.2f} MB")
            print(f"ğŸš€ å¤„ç†ç­–ç•¥: {strategy['strategy']}")
        except Exception as e:
            print(f"âš  æ— æ³•è·å–å¤„ç†ç­–ç•¥: {e}")
            strategy = {"strategy": "normal", "size_mb": 0}

        if strategy['strategy'] == 'streaming':
            return self._extract_with_streaming(html_file_path, url, strategy, product_key)
        elif strategy['strategy'] == 'chunked':
            return self._extract_with_chunking(html_file_path, url, strategy, product_key)
        else:
            return self._extract_normal(html_file_path, url, product_key)

    def _extract_normal(self, html_file_path: str, url: str, product_key: str) -> Dict[str, Any]:
        """æ ‡å‡†æå–æ¨¡å¼"""
        print("ğŸ“„ ä½¿ç”¨æ ‡å‡†å¤„ç†æ¨¡å¼...")
        
        try:
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
                
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # é¢„å¤„ç†å›¾ç‰‡è·¯å¾„
            soup = preprocess_image_paths(soup)
            
            # æå–å†…å®¹
            extracted_data = self._extract_content_from_soup(
                soup, html_file_path, url, product_key
            )
            
            return extracted_data
            
        except Exception as e:
            print(f"âŒ æ ‡å‡†æå–å¤±è´¥: {e}")
            return self._create_error_result(str(e))

    def _extract_with_chunking(self, html_file_path: str, url: str,
                              strategy: Dict[str, Any], product_key: str) -> Dict[str, Any]:
        """åˆ†å—å¤„ç†æ¨¡å¼"""
        print("ğŸ§© ä½¿ç”¨åˆ†å—å¤„ç†æ¨¡å¼...")
        
        try:
            # ç›‘æ§å†…å­˜ä½¿ç”¨
            initial_memory = self.large_html_processor.monitor_memory_usage()
            print(f"  ğŸ§  åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.1f}MB")
            
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
                
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # é¢„å¤„ç†å›¾ç‰‡è·¯å¾„
            soup = preprocess_image_paths(soup)
            
            # æå–å†…å®¹ï¼ˆä¸æ ‡å‡†æ¨¡å¼ç›¸åŒï¼‰
            extracted_data = self._extract_content_from_soup(
                soup, html_file_path, url, product_key
            )
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            final_memory = self.large_html_processor.monitor_memory_usage()
            memory_used = final_memory - initial_memory
            print(f"  ğŸ§  å†…å­˜å¢é•¿: {memory_used:.1f}MB")
            
            extracted_data["processing_info"] = {
                "mode": "chunked",
                "file_size_mb": strategy.get('size_mb', 0),
                "memory_used_mb": memory_used,
                "processing_time": datetime.now().isoformat()
            }
            
            return extracted_data
            
        except Exception as e:
            print(f"âŒ åˆ†å—æå–å¤±è´¥: {e}")
            return self._create_error_result(str(e))

    def _extract_with_streaming(self, html_file_path: str, url: str,
                               strategy: Dict[str, Any], product_key: str) -> Dict[str, Any]:
        """æµå¼å¤„ç†æ¨¡å¼ï¼ˆä¸ºå¤§å‹æ–‡ä»¶é¢„ç•™ï¼‰"""
        print("ğŸŒŠ ä½¿ç”¨æµå¼å¤„ç†æ¨¡å¼...")
        
        # ç›®å‰ä½¿ç”¨åˆ†å—å¤„ç†çš„é€»è¾‘ï¼Œæœªæ¥å¯ä»¥å®ç°çœŸæ­£çš„æµå¼å¤„ç†
        result = self._extract_with_chunking(html_file_path, url, strategy, product_key)
        if "processing_info" in result:
            result["processing_info"]["mode"] = "streaming"
        
        print("  âš  æµå¼å¤„ç†åŠŸèƒ½å¼€å‘ä¸­ï¼Œå½“å‰ä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—å¤„ç†")
        
        return result

    def _extract_content_from_soup(self, soup: BeautifulSoup, html_file_path: str, 
                                  url: str, product_key: str) -> Dict[str, Any]:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–å†…å®¹"""
        
        print("ğŸ” å¼€å§‹å†…å®¹æå–...")
        
        # è·å–äº§å“é…ç½®
        try:
            product_config = self.product_manager.get_product_config(product_key)
            important_section_titles = self.product_manager.get_important_section_titles(product_key)
        except (ValueError, AttributeError) as e:
            print(f"âš  æ— æ³•è·å–äº§å“é…ç½®: {e}")
            product_config = {}
            important_section_titles = []
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = find_main_content_area(soup)
        
        # åˆå§‹åŒ–æå–ç»“æœï¼ˆå¯¹é½é‡æ„å‰çš„å­—æ®µç»“æ„ï¼‰
        extracted_data = {
            "product_key": product_key,
            "source_file": str(html_file_path),
            "source_url": url or self._get_default_url(product_key),
            "extraction_timestamp": datetime.now().isoformat(),
            "Title": "",
            "MetaDescription": "",
            "MetaKeywords": "",
            "MSServiceName": "",
            "Slug": "",
            "DescriptionContent": "",
            "Language": "",
            "NavigationTitle": "",
            "BannerContent": "",
            "QaContent": "",
            "HasRegion": False,
            "NoRegionContent": "",
            "NorthChinaContent": "",
            "NorthChina2Content": "",
            "NorthChina3Content": "",
            "EastChinaContent": "",
            "EastChina2Content": "",
            "EastChina3Content": "",
            "PricingTables": [],
            "RegionalContent": {},
            "ServiceTiers": [],
            "extraction_metadata": {}
        }
        
        # 1. æå–æ ‡é¢˜
        extracted_data["Title"] = self._extract_page_title(main_content or soup)
        
        # 2. æå–Metaä¿¡æ¯
        extracted_data["MetaDescription"] = self._extract_meta_description(soup)
        extracted_data["MetaKeywords"] = self._extract_meta_keywords(soup)
        extracted_data["MSServiceName"] = self._extract_ms_service_name(soup)
        extracted_data["Slug"] = self._extract_slug(url)
        extracted_data["Language"] = self._detect_language(soup)
        
        # 3. æå–Bannerå†…å®¹
        banner_content = self._extract_banner_content(main_content or soup)
        extracted_data["BannerContent"] = self._format_html_content(banner_content)
        extracted_data["NavigationTitle"] = self._extract_navigation_title(soup)
        
        # 4. æå–æè¿°å†…å®¹
        description_content = self._extract_description_content(main_content or soup, important_section_titles)
        extracted_data["DescriptionContent"] = self._format_html_content(description_content)
        
        # 5. æå–å®šä»·è¡¨æ ¼
        extracted_data["PricingTables"] = self._extract_pricing_tables(main_content or soup)
        
        # 6. æå–Q&Aå†…å®¹ï¼ˆå¯¹é½é‡æ„å‰çš„QaContentå­—æ®µï¼‰
        extracted_data["QaContent"] = extract_qa_content(main_content or soup)
        
        # 7. æ£€æŸ¥åŒºåŸŸå¹¶æå–åŒºåŸŸå†…å®¹
        extracted_data["HasRegion"] = self._check_has_region(soup)
        
        if extracted_data["HasRegion"]:
            # æå–å„åŒºåŸŸå†…å®¹
            regional_content = self.region_processor.extract_region_contents(
                soup, html_file_path
            )
            extracted_data["RegionalContent"] = regional_content
            
            # æ˜ å°„åˆ°å…·ä½“çš„åŒºåŸŸå­—æ®µ
            region_mapping = {
                "china-north": "NorthChinaContent",
                "china-north-2": "NorthChina2Content",
                "china-north-3": "NorthChina3Content",
                "china-east": "EastChinaContent",
                "china-east-2": "EastChina2Content",
                "china-east-3": "EastChina3Content"
            }
            
            for region_key, field_name in region_mapping.items():
                if region_key in regional_content:
                    extracted_data[field_name] = regional_content[region_key]
        else:
            # æ²¡æœ‰åŒºåŸŸé€‰æ‹©ï¼Œæå–ä¸»ä½“å†…å®¹åˆ°NoRegionContent
            extracted_data["NoRegionContent"] = self._extract_no_region_content(soup)
        
        # 8. æå–ç»“æ„åŒ–å†…å®¹
        structured_content = extract_structured_content(main_content or soup, important_section_titles)
        extracted_data["ServiceTiers"] = structured_content.get("sections", [])
        
        # 8. æ·»åŠ æå–å…ƒæ•°æ®
        extracted_data["extraction_metadata"] = {
            "extractor_version": "enhanced_v2.0",
            "processing_mode": "standard",
            "content_sections_found": len(structured_content.get("sections", [])),
            "pricing_tables_found": len(extracted_data["PricingTables"]),
            "regions_detected": len(extracted_data["RegionalContent"]),
            "faq_length": len(extracted_data["FAQ"]),
            "has_banner": bool(extracted_data["BannerContent"]),
            "product_config_used": product_key != "unknown"
        }
        
        # 9. éªŒè¯æå–çš„æ•°æ®
        if product_config:
            try:
                validation_result = validate_extracted_data(extracted_data, product_config)
                extracted_data["validation"] = validation_result
                
                if not validation_result["is_valid"]:
                    print(f"âš  æ•°æ®éªŒè¯å‘ç°é—®é¢˜: {validation_result['errors']}")
            except Exception as e:
                print(f"âš  æ•°æ®éªŒè¯å¤±è´¥: {e}")
        
        print(f"âœ… å†…å®¹æå–å®Œæˆ")
        print(f"  ğŸ“Š æ ‡é¢˜: {'âœ“' if extracted_data['Title'] else 'âœ—'}")
        print(f"  ğŸ¯ Banner: {'âœ“' if extracted_data['BannerContent'] else 'âœ—'}")  
        print(f"  ğŸ“ æè¿°: {'âœ“' if extracted_data['DescriptionContent'] else 'âœ—'}")
        print(f"  ğŸ“‹ å®šä»·è¡¨æ ¼: {len(extracted_data['PricingTables'])} ä¸ª")
        print(f"  â“ FAQ: {'âœ“' if extracted_data['FAQ'] else 'âœ—'}")
        print(f"  ğŸŒ åŒºåŸŸ: {len(extracted_data['RegionalContent'])} ä¸ª")
        
        return extracted_data

    def _extract_page_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        # ä¼˜å…ˆæŸ¥æ‰¾é¡µé¢titleæ ‡ç­¾
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text(strip=True)
            if title and len(title) > 0:
                return title
        
        # æŸ¥æ‰¾ä¸»è¦æ ‡é¢˜å…ƒç´ 
        main_heading = soup.find(['h1', 'h2'])
        if main_heading:
            return main_heading.get_text(strip=True)
        
        return ""

    def _extract_banner_content(self, soup: BeautifulSoup) -> BeautifulSoup:
        """æå–Bannerå†…å®¹"""
        print("  ğŸ¯ æå–Bannerå†…å®¹...")
        
        # æŸ¥æ‰¾bannerç›¸å…³å…ƒç´ 
        banner_selectors = [
            '.banner', '.hero', '.jumbotron', '.page-header', 
            'header', '.product-banner', '.intro-section', '.common-banner'
        ]
        
        for selector in banner_selectors:
            banner = soup.select_one(selector)
            if banner:
                # åˆ›å»ºç®€åŒ–çš„bannerå†…å®¹
                simplified_banner = BeautifulSoup("<div class='banner-content'></div>", 'html.parser')
                banner_div = simplified_banner.find('div')
                
                # æå–banneræ–‡æœ¬å†…å®¹
                banner_info = extract_banner_text_content(banner)
                
                if banner_info.get('title'):
                    title_elem = simplified_banner.new_tag('h2', **{'class': 'banner-title'})
                    title_elem.string = banner_info['title']
                    banner_div.append(title_elem)
                
                if banner_info.get('description'):
                    desc_elem = simplified_banner.new_tag('div', **{'class': 'banner-description'})
                    desc_elem.string = banner_info['description']
                    banner_div.append(desc_elem)
                
                if banner_info.get('features'):
                    features_ul = simplified_banner.new_tag('ul', **{'class': 'banner-features'})
                    for feature in banner_info['features'][:5]:  # æœ€å¤š5ä¸ªç‰¹æ€§
                        li = simplified_banner.new_tag('li')
                        li.string = feature
                        features_ul.append(li)
                    banner_div.append(features_ul)
                
                return simplified_banner
        
        print("    âš  æœªæ‰¾åˆ°Bannerå†…å®¹")
        return BeautifulSoup("", 'html.parser')

    def _extract_description_content(self, soup: BeautifulSoup, 
                                   important_section_titles: List[str]) -> BeautifulSoup:
        """æå–æè¿°å†…å®¹"""
        print("  ğŸ“ æå–æè¿°å†…å®¹...")
        
        simplified_content = BeautifulSoup("<div class='description-content'></div>", 'html.parser')
        content_div = simplified_content.find('div')
        
        # æŸ¥æ‰¾é‡è¦çš„section
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        
        sections_found = 0
        for heading in headings:
            # æ£€æŸ¥æ˜¯å¦ä¸ºé‡è¦æ ‡é¢˜
            heading_text = heading.get_text(strip=True).lower()
            
            is_important = any(
                title.lower() in heading_text 
                for title in important_section_titles
            ) if important_section_titles else True
            
            if is_important and sections_found < 5:  # æœ€å¤š5ä¸ªé‡è¦section
                # åˆ›å»ºsection
                section_div = simplified_content.new_tag('div', **{'class': 'content-section'})
                
                # æ·»åŠ æ ‡é¢˜
                section_title = simplified_content.new_tag(heading.name)
                section_title.string = heading.get_text(strip=True)
                section_div.append(section_title)
                
                # æ”¶é›†è¯¥sectionä¸‹çš„å†…å®¹
                next_sibling = heading.find_next_sibling()
                content_parts = []
                
                while (next_sibling and 
                       next_sibling.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'] and
                       len(content_parts) < 3):  # æœ€å¤š3ä¸ªæ®µè½
                    
                    if next_sibling.name in ['p', 'div', 'ul', 'ol']:
                        simple_element = create_simple_element(next_sibling, simplified_content)
                        if simple_element:
                            content_parts.append(simple_element)
                    
                    next_sibling = next_sibling.find_next_sibling()
                
                # æ·»åŠ å†…å®¹åˆ°section
                for part in content_parts:
                    section_div.append(part)
                
                if section_div.get_text(strip=True):  # åªæ·»åŠ æœ‰å†…å®¹çš„section
                    content_div.append(section_div)
                    sections_found += 1
        
        if sections_found == 0:
            print("    âš  æœªæ‰¾åˆ°é‡è¦çš„æè¿°å†…å®¹")
        else:
            print(f"    âœ“ æ‰¾åˆ° {sections_found} ä¸ªé‡è¦section")
        
        return simplified_content

    def _extract_pricing_tables(self, soup: BeautifulSoup) -> List[str]:
        """æå–å®šä»·è¡¨æ ¼"""
        print("  ğŸ“‹ æå–å®šä»·è¡¨æ ¼...")
        
        pricing_tables = []
        tables = soup.find_all('table')
        
        for i, table in enumerate(tables):
            table_text = table.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå®šä»·è¡¨æ ¼
            if self._is_pricing_table(table_text):
                # åˆ›å»ºç®€åŒ–çš„è¡¨æ ¼
                simplified_table = BeautifulSoup("<table></table>", 'html.parser')
                table_elem = simplified_table.find('table')
                table_elem['class'] = ['pricing-table']
                
                # å¤åˆ¶è¡¨æ ¼ç»“æ„
                from src.utils.html_utils import copy_table_structure
                copy_table_structure(table, table_elem, simplified_table)
                
                pricing_tables.append(str(simplified_table))
        
        print(f"    âœ“ æ‰¾åˆ° {len(pricing_tables)} ä¸ªå®šä»·è¡¨æ ¼")
        return pricing_tables

    def _is_pricing_table(self, table_text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå®šä»·è¡¨æ ¼"""
        pricing_keywords = [
            'ä»·æ ¼', 'price', 'å®šä»·', 'pricing', 'è´¹ç”¨', 'cost', 
            'ï¿¥', '$', 'å…ƒ', 'ç¾å…ƒ', 'usd', 'rmb', 'cny'
        ]
        
        text_lower = table_text.lower()
        return any(keyword in text_lower for keyword in pricing_keywords)

    def _format_html_content(self, soup: BeautifulSoup) -> str:
        """æ ¼å¼åŒ–HTMLå†…å®¹"""
        if not soup or not soup.get_text(strip=True):
            return ""
        
        html_str = str(soup)
        return clean_html_content(html_str)

    def _detect_product_key_from_path(self, html_file_path: str) -> Optional[str]:
        """ä»æ–‡ä»¶è·¯å¾„æ£€æµ‹äº§å“ç±»å‹"""
        try:
            return self.product_manager.detect_product_from_filename(html_file_path)
        except Exception as e:
            print(f"âš  äº§å“ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
            return None

    def _get_default_url(self, product_key: str) -> str:
        """è·å–äº§å“é»˜è®¤URL"""
        try:
            return self.product_manager.get_product_url(product_key)
        except Exception:
            return f"https://www.azure.cn/pricing/details/{product_key}/"

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            "error": True,
            "error_message": error_message,
            "extraction_timestamp": datetime.now().isoformat(),
            "Title": "",
            "BannerContent": "",
            "DescriptionContent": "",
            "PricingTables": [],
            "FAQ": "",
            "RegionalContent": {},
            "ServiceTiers": [],
            "extraction_metadata": {
                "extractor_version": "enhanced_v2.0",
                "processing_failed": True
            }
        }

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æå–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "extractor_type": "EnhancedCMSExtractor",
            "version": "2.0",
            "large_file_support": True,
            "streaming_support": "development"
        }
        
        try:
            stats["supported_products"] = len(self.product_manager.get_supported_products())
            stats["cache_stats"] = self.product_manager.get_cache_stats()
        except Exception as e:
            print(f"âš  è·å–äº§å“ç®¡ç†å™¨ç»Ÿè®¡å¤±è´¥: {e}")
            stats["supported_products"] = 0
            stats["cache_stats"] = {}
        
        try:
            stats["region_processor_stats"] = self.region_processor.get_statistics()
        except Exception as e:
            print(f"âš  è·å–åŒºåŸŸå¤„ç†å™¨ç»Ÿè®¡å¤±è´¥: {e}")
            stats["region_processor_stats"] = {}
            
        return stats
    
    def _extract_meta_description(self, soup: BeautifulSoup) -> str:
        """æå–Metaæè¿°"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')
        return ""
    
    def _extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """æå–Metaå…³é”®è¯"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            return meta_keywords.get('content', '')
        return ""
    
    def _extract_ms_service_name(self, soup: BeautifulSoup) -> str:
        """æå–MSServiceNameå­—æ®µï¼Œä»pure-content divå†…çš„tagså…ƒç´ ä¸­çš„ms.serviceå±æ€§"""
        print("ğŸ·ï¸ æå–MSServiceName...")
        
        # æŸ¥æ‰¾pure-content div
        pure_content_div = soup.find('div', class_='pure-content')
        if pure_content_div:
            # åœ¨pure-content divå†…æŸ¥æ‰¾tagså…ƒç´ 
            tags_element = pure_content_div.find('tags')
            if tags_element:
                # æå–ms.serviceå±æ€§å€¼
                ms_service = tags_element.get('ms.service', '')
                if ms_service:
                    print(f"  âœ“ æ‰¾åˆ°MSServiceName: {ms_service}")
                    return ms_service
                else:
                    print("  âš  tagså…ƒç´ ä¸­æ²¡æœ‰ms.serviceå±æ€§")
            else:
                print("  âš  pure-content divä¸­æ²¡æœ‰æ‰¾åˆ°tagså…ƒç´ ")
        else:
            print("  âš  æ²¡æœ‰æ‰¾åˆ°pure-content div")
        
        return ""
    
    def _extract_slug(self, url: str) -> str:
        """ä»URLæå–slug"""
        if not url:
            return ""
        
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            path = parsed.path
            
            # æå–/details/ä¹‹ååˆ°/index.htmlä¹‹å‰çš„å†…å®¹ï¼Œç”¨-è¿æ¥
            # ä¾‹å¦‚ /pricing/details/storage/files/index.html -> storage-files
            # ä¾‹å¦‚ /pricing/details/api-management/index.html -> api-management
            if '/details/' in path:
                # æ‰¾åˆ°/details/ä¹‹åçš„éƒ¨åˆ†
                after_details = path.split('/details/')[1]
                
                # ç§»é™¤/index.htmlåç¼€
                if after_details.endswith('/index.html'):
                    after_details = after_details[:-11]  # ç§»é™¤'/index.html'
                elif after_details.endswith('/'):
                    after_details = after_details[:-1]  # ç§»é™¤æœ«å°¾çš„'/'
                
                # åˆ†å‰²è·¯å¾„å¹¶ç”¨-è¿æ¥
                path_parts = [p for p in after_details.split('/') if p]
                if path_parts:
                    return '-'.join(path_parts)
        except:
            pass
        
        return ""
    
    def _detect_language(self, soup: BeautifulSoup) -> str:
        """æ£€æµ‹é¡µé¢è¯­è¨€"""
        # æ£€æŸ¥htmlæ ‡ç­¾çš„langå±æ€§
        html_tag = soup.find('html')
        if html_tag:
            lang = html_tag.get('lang', '')
            if 'en' in lang.lower():
                return 'en-US'
        
        # é»˜è®¤è¿”å›ä¸­æ–‡
        return 'zh-CN'
    
    def _extract_navigation_title(self, soup: BeautifulSoup) -> str:
        """æå–å¯¼èˆªæ ‡é¢˜"""
        # æŸ¥æ‰¾common-banner-title > h2
        banner_title = soup.find('div', class_='common-banner-title')
        if banner_title:
            h2 = banner_title.find('h2')
            if h2:
                return h2.get_text(strip=True)
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾é¡µé¢ä¸»æ ‡é¢˜
        main_title = soup.find('h1')
        if main_title:
            return main_title.get_text(strip=True)
        
        return ""
    
    def _check_has_region(self, soup: BeautifulSoup) -> bool:
        """æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰åŒºåŸŸé€‰æ‹©"""
        print("ğŸŒ æ£€æŸ¥åŒºåŸŸé€‰æ‹©...")
        
        # æŸ¥æ‰¾åŒºåŸŸé€‰æ‹©ç›¸å…³çš„å…ƒç´ 
        region_indicators = [
            soup.find('div', class_='region-container'),
            soup.find('select', class_='region-selector'),
            soup.find('div', class_='software-kind'),
            soup.find('div', attrs={'data-region': True})
        ]
        
        for indicator in region_indicators:
            if indicator:
                print(f"  âœ“ å‘ç°åŒºåŸŸé€‰æ‹©å™¨: {indicator.name}")
                return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªåŒºåŸŸç›¸å…³çš„å±æ€§
        region_elements = soup.find_all(attrs={'data-region': True})
        if len(region_elements) > 1:
            print(f"  âœ“ å‘ç° {len(region_elements)} ä¸ªåŒºåŸŸå…ƒç´ ")
            return True
        
        return False
    
    def _extract_no_region_content(self, soup: BeautifulSoup) -> str:
        """æå–æ— åŒºåŸŸé¡µé¢çš„ä¸»ä½“å†…å®¹"""
        print("ğŸ“„ æå–æ— åŒºåŸŸä¸»ä½“å†…å®¹...")
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content_selectors = [
            'main',
            '.main-content',
            '.content',
            '.page-content',
            '.pricing-content'
        ]
        
        for selector in main_content_selectors:
            element = soup.select_one(selector)
            if element:
                return str(element)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„ä¸»å†…å®¹åŒºåŸŸï¼Œæå–bodyä¸­é™¤äº†headerã€navã€footerå¤–çš„å†…å®¹
        body = soup.find('body')
        if body:
            # å¤åˆ¶bodyå†…å®¹
            content_soup = BeautifulSoup(str(body), 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„éƒ¨åˆ†
            for unwanted in content_soup.find_all(['header', 'nav', 'footer']):
                unwanted.decompose()
            
            return str(content_soup)
        
        return ""