#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•é™æ€ç­–ç•¥
å¤„ç†Type Aé¡µé¢ï¼šç®€å•é™æ€é¡µé¢ï¼Œå¦‚Event Gridã€Service Busç­‰
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.strategies.base_strategy import BaseStrategy
from src.core.logging import get_logger

logger = get_logger(__name__)


class SimpleStaticStrategy(BaseStrategy):
    """
    ç®€å•é™æ€ç­–ç•¥
    Type A: ç®€å•é™æ€é¡µé¢å¤„ç† - Event Gridç±»å‹
    
    ç‰¹ç‚¹ï¼š
    - æ²¡æœ‰åŒºåŸŸç­›é€‰æˆ–å¤æ‚äº¤äº’
    - å†…å®¹ç›¸å¯¹å›ºå®šï¼Œä¸éœ€è¦åŠ¨æ€ç­›é€‰
    - ä¸»è¦åŒ…å«ï¼šBannerã€æè¿°ã€ä¸»è¦å†…å®¹ã€FAQ
    - Event Gridã€Service Busæ˜¯æ­¤ç­–ç•¥çš„å…¸å‹ä»£è¡¨
    """

    def __init__(self, product_config: Dict[str, Any], html_file_path: str = ""):
        """
        åˆå§‹åŒ–ç®€å•é™æ€ç­–ç•¥
        
        Args:
            product_config: äº§å“é…ç½®ä¿¡æ¯
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
        """
        super().__init__(product_config, html_file_path)
        self.strategy_name = "simple_static"
        logger.info(f"ğŸ“„ åˆå§‹åŒ–ç®€å•é™æ€ç­–ç•¥: {self._get_product_key()}")

    def extract(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œç®€å•é™æ€ç­–ç•¥çš„æå–é€»è¾‘
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            æå–çš„CMSå†…å®¹æ•°æ®
        """
        logger.info("ğŸš€ å¼€å§‹ç®€å•é™æ€ç­–ç•¥æå–...")
        
        # 1. æå–åŸºç¡€å†…å®¹ï¼ˆTitle, Banner, Description, QAç­‰ï¼‰
        data = self._extract_base_content(soup, url)
        
        # 2. è®¾ç½®ç®€å•é¡µé¢æ ‡è¯†
        data["HasRegion"] = False
        data["NoRegionContent"] = self._extract_main_content(soup)
        
        # 3. æ¸…ç©ºåŒºåŸŸå†…å®¹å­—æ®µï¼ˆç®€å•é¡µé¢ä¸éœ€è¦ï¼‰
        region_fields = [
            "NorthChinaContent", "NorthChina2Content", "NorthChina3Content",
            "EastChinaContent", "EastChina2Content", "EastChina3Content"
        ]
        for field in region_fields:
            data[field] = ""
        
        # 4. éªŒè¯æå–ç»“æœ
        data = self._validate_extraction_result(data)
        
        logger.info("âœ… ç®€å•é™æ€ç­–ç•¥æå–å®Œæˆ")
        return data

    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """
        æå–ä¸»è¦å†…å®¹ä½œä¸ºNoRegionContent
        
        åŸºäºHTMLç»“æ„åˆ†æï¼š
        1. ä¼˜å…ˆé€‰æ‹©ï¼š<tab-control-container> å±‚å†…çš„æ‰€æœ‰å†…å®¹
        2. å¤‡é€‰æ–¹æ¡ˆï¼šDescriptionContentåé¢çš„ <pricing-page-section> å±‚å†…å®¹
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            ä¸»è¦å†…å®¹HTMLå­—ç¬¦ä¸²
        """
        logger.info("ğŸ“ æå–ä¸»è¦å†…å®¹...")
        
        try:
            # æ–¹æ¡ˆ1: æŸ¥æ‰¾ tab-control-container 
            logger.info("ğŸ” æ–¹æ¡ˆ1: æŸ¥æ‰¾ tab-control-container...")
            tab_containers = soup.find_all(class_='tab-control-container')
            if tab_containers:
                main_content = ""
                for container in tab_containers:
                    main_content += self._clean_html_content(str(container))
                    print(f"âœ“ æ‰¾åˆ°tab-control-containerå†…å®¹")
                return main_content
            
            # æ–¹æ¡ˆ2: æŸ¥æ‰¾ DescriptionContent åé¢çš„ pricing-page-section
            logger.info("ğŸ” æ–¹æ¡ˆ2: æŸ¥æ‰¾ pricing-page-section...")
            pricing_sections = soup.find_all(class_='pricing-page-section')
            
            if pricing_sections:
                main_content = ""
                # è·³è¿‡ç¬¬ä¸€ä¸ªpricing-page-sectionï¼ˆé€šå¸¸æ˜¯DescriptionContentï¼‰
                # æå–åé¢çš„pricing-page-sectionä½œä¸ºä¸»è¦å†…å®¹
                for i, section in enumerate(pricing_sections):
                    if i > 0:  # è·³è¿‡ç¬¬ä¸€ä¸ª
                        section_text = section.get_text().lower()
                        # ç¡®ä¿ä¸æ˜¯banneræˆ–å¯¼èˆªå†…å®¹
                        if not any(skip_text in section_text for skip_text in [
                            'banner', 'navigation', 'nav'
                        ]):
                            main_content += self._clean_html_content(str(section))
                            print(f"âœ“ æ‰¾åˆ°ç¬¬{i+1}ä¸ªpricing-page-sectionå†…å®¹")
                
                if main_content:
                    return main_content
            
            # æ–¹æ¡ˆ3: å¤‡ç”¨æ–¹æ¡ˆ - æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„å†…å®¹å®¹å™¨
            logger.info("ğŸ” æ–¹æ¡ˆ3: ä½¿ç”¨å¤‡ç”¨å†…å®¹æå–...")
            fallback_selectors = [
                '.main-content',
                '.content-area', 
                '.primary-content',
                'main',
                '.page-content'
            ]
            
            for selector in fallback_selectors:
                elements = soup.select(selector)
                for element in elements:
                    if len(element.get_text(strip=True)) > 100:  # è‡³å°‘100ä¸ªå­—ç¬¦
                        main_content = self._clean_html_content(str(element))
                        print(f"âœ“ ä½¿ç”¨å¤‡ç”¨å†…å®¹ï¼Œé€‰æ‹©å™¨: {selector}")
                        return main_content
            
            logger.info("âš  æœªæ‰¾åˆ°åˆé€‚çš„ä¸»è¦å†…å®¹")
            return ""
            
        except Exception as e:
            logger.info(f"âš  ä¸»è¦å†…å®¹æå–å¤±è´¥: {e}")
            return ""