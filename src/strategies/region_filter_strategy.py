#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒºåŸŸç­›é€‰ç­–ç•¥ - é€‚é…æ–°æ¶æ„
å¤„ç†Type Bé¡µé¢ï¼šå…·æœ‰åŒºåŸŸç­›é€‰åŠŸèƒ½çš„é¡µé¢ï¼Œå¦‚API Management
é›†æˆæ–°å·¥å…·ç±»ä¸ç°æœ‰RegionProcessor
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.strategies.base_strategy import BaseStrategy
from src.core.region_processor import RegionProcessor
from src.utils.content.content_extractor import ContentExtractor
from src.utils.content.section_extractor import SectionExtractor
from src.utils.content.flexible_builder import FlexibleBuilder
from src.utils.data.extraction_validator import ExtractionValidator
from src.detectors.filter_detector import FilterDetector
from src.utils.content.content_utils import classify_pricing_section, filter_sections_by_type
from src.utils.html.cleaner import clean_html_content

from src.core.logging import get_logger

logger = get_logger(__name__)


class RegionFilterStrategy(BaseStrategy):
    """
    åŒºåŸŸç­›é€‰ç­–ç•¥ - æ–°æ¶æ„é€‚é…
    Type B: åŒºåŸŸç­›é€‰é¡µé¢å¤„ç† - API Managementç±»å‹
    
    ç‰¹ç‚¹ï¼š
    - å…·æœ‰åŒºåŸŸç­›é€‰æ§ä»¶ (å¦‚ä¸­å›½åŒ—éƒ¨ã€ä¸­å›½ä¸œéƒ¨ç­‰)
    - ç­›é€‰å™¨å˜åŒ–ä¼šæ”¹å˜å†…å®¹æ˜¾ç¤º
    - éœ€è¦æå–æ¯ä¸ªåŒºåŸŸçš„ä¸“é—¨å†…å®¹
    - ä½¿ç”¨æ–°å·¥å…·ç±»æ¶æ„ï¼šContentExtractor + SectionExtractor + FlexibleBuilder + RegionProcessor
    """

    def __init__(self, product_config: Dict[str, Any], html_file_path: str = ""):
        """
        åˆå§‹åŒ–åŒºåŸŸç­›é€‰ç­–ç•¥
        
        Args:
            product_config: äº§å“é…ç½®ä¿¡æ¯
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
        """
        super().__init__(product_config, html_file_path)
        self.strategy_name = "region_filter"
        
        # åˆå§‹åŒ–å·¥å…·ç±»
        self.content_extractor = ContentExtractor()
        self.section_extractor = SectionExtractor()
        self.flexible_builder = FlexibleBuilder()
        self.extraction_validator = ExtractionValidator()
        
        # ä¿æŒç°æœ‰åŒºåŸŸå¤„ç†é€»è¾‘
        self.region_processor = RegionProcessor()
        self.filter_detector = FilterDetector()
        
        logger.info(f"ğŸŒ åˆå§‹åŒ–åŒºåŸŸç­›é€‰ç­–ç•¥: {self._get_product_key()}")

    def extract_flexible_content(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œflexible JSONæ ¼å¼æå–é€»è¾‘
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            flexible JSONæ ¼å¼çš„æå–æ•°æ®
        """
        logger.info("ğŸŒ æ‰§è¡ŒåŒºåŸŸç­›é€‰ç­–ç•¥æå–ï¼ˆflexible JSONæ ¼å¼ï¼‰...")
        
        # 1. ä½¿ç”¨ContentExtractoræå–åŸºç¡€å…ƒæ•°æ®
        base_metadata = self.content_extractor.extract_base_metadata(soup, url, self.html_file_path)
        
        # 2. ä½¿ç”¨SectionExtractoræå–commonSections
        common_sections = self.section_extractor.extract_all_sections(soup)
        
        # 3. ä½¿ç”¨FilterDetectorè·å–ç­›é€‰å™¨ä¿¡æ¯
        filter_analysis = self.filter_detector.detect_filters(soup)
        
        # 4. ä½¿ç”¨RegionProcessoræå–åŒºåŸŸå†…å®¹ï¼ˆä¼ é€’ç­›é€‰å™¨ä¿¡æ¯å’Œäº§å“é…ç½®ï¼‰
        try:
            region_content = self.region_processor.extract_region_contents(
                soup, 
                self.html_file_path,
                filter_analysis=filter_analysis,
                product_config=self.product_config
            )
            logger.info(f"âœ… åŒºåŸŸå†…å®¹æå–å®Œæˆ: {len(region_content)} ä¸ªåŒºåŸŸ")
        except Exception as e:
            logger.warning(f"âš  åŒºåŸŸå†…å®¹æå–å¤±è´¥: {e}")
            region_content = {}
        
        # 5. ä½¿ç”¨FlexibleBuilderæ„å»ºåœ°åŒºå†…å®¹ç»„
        content_groups = self.flexible_builder.build_region_content_groups(region_content)
        
        # 6. æ„å»ºç­–ç•¥ç‰¹å®šå†…å®¹ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ†ææ•°æ®
        # å¯¹äºåŒºåŸŸç­›é€‰ç­–ç•¥ï¼Œå¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„åŒºåŸŸå†…å®¹ï¼Œå¯ä»¥æå–baseContentä½œä¸ºfallback
        base_content = ""
        if not region_content or len(region_content) == 0:
            logger.info("âš  æœªæ‰¾åˆ°åŒºåŸŸå†…å®¹ï¼Œå°è¯•æå–é€šç”¨baseContent...")
            base_content = self._extract_main_content(soup)
        
        strategy_content = {
            "baseContent": base_content,  # å¦‚æœæœ‰åŒºåŸŸå†…å®¹åˆ™ä¸ºç©ºï¼Œå¦åˆ™ä½œä¸ºfallback
            "contentGroups": content_groups,
            "strategy_type": "region_filter",
            "filter_analysis": filter_analysis,  # ä¼ é€’ç­›é€‰å™¨åˆ†æç»“æœ
            "tab_analysis": {}  # åŒºåŸŸç­›é€‰ç­–ç•¥é€šå¸¸ä¸æ¶‰åŠå¤æ‚tab
        }
        
        # 7. ä½¿ç”¨FlexibleBuilderæ„å»ºå®Œæ•´çš„flexible JSON
        flexible_data = self.flexible_builder.build_flexible_page(
            base_metadata, common_sections, strategy_content
        )
        
        # 8. éªŒè¯flexible JSONç»“æœ
        flexible_data = self.extraction_validator.validate_flexible_json(flexible_data)
        
        logger.info("âœ… åŒºåŸŸç­›é€‰ç­–ç•¥æå–å®Œæˆï¼ˆflexible JSONæ ¼å¼ï¼‰")
        return flexible_data

    def extract_common_sections(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """
        æå–é€šç”¨sectionsï¼ˆBannerã€Descriptionã€QAç­‰ï¼‰
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            
        Returns:
            commonSectionsåˆ—è¡¨
        """
        return self.section_extractor.extract_all_sections(soup)

    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """
        æå–ä¸»è¦å†…å®¹ - æ™ºèƒ½åˆ†ç±»ç‰ˆæœ¬ï¼ˆç”¨äºåŒºåŸŸç­›é€‰ç­–ç•¥çš„fallbackï¼‰
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            ä¸»è¦å†…å®¹HTMLå­—ç¬¦ä¸²
        """
        logger.info("ğŸ“ æå–åŒºåŸŸç­›é€‰fallbackå†…å®¹ï¼ˆæ™ºèƒ½åˆ†ç±»æ¨¡å¼ï¼‰...")
        
        try:
            # æ–¹æ¡ˆ1: æŸ¥æ‰¾technical-azure-selectorå†…çš„pricing-page-sectionï¼Œä½¿ç”¨æ™ºèƒ½åˆ†ç±»
            logger.info("ğŸ” æŸ¥æ‰¾technical-azure-selectorå†…å®¹ï¼ˆæ™ºèƒ½åˆ†ç±»ï¼‰...")
            technical_selector = soup.find('div', class_='technical-azure-selector')
            if technical_selector:
                pricing_sections = technical_selector.find_all('div', class_='pricing-page-section')
                if pricing_sections:
                    # ä½¿ç”¨æ™ºèƒ½åˆ†ç±»è¿‡æ»¤ï¼Œåªä¿ç•™contentç±»å‹çš„section
                    content_sections = filter_sections_by_type(
                        pricing_sections, 
                        include_types=['content']
                    )
                    
                    if content_sections:
                        main_content = ""
                        for section in content_sections:
                            main_content += str(section)
                            section_type = classify_pricing_section(section)
                            logger.info(f"âœ“ æ·»åŠ åŒºåŸŸç­›é€‰fallback section (ç±»å‹: {section_type})")
                        
                        logger.info(f"âœ“ æ‰¾åˆ°åŒºåŸŸç­›é€‰fallbackå†…å®¹ï¼Œå…±{len(content_sections)}ä¸ªcontent sections")
                        return clean_html_content(main_content)
            
            # æ–¹æ¡ˆ2: æŸ¥æ‰¾æ‰€æœ‰pricing-page-sectionï¼Œæ™ºèƒ½åˆ†ç±»åå¤„ç†
            logger.info("ğŸ” æŸ¥æ‰¾æ‰€æœ‰pricing-page-sectionï¼ˆæ™ºèƒ½åˆ†ç±»ï¼‰...")
            all_pricing_sections = soup.find_all('div', class_='pricing-page-section')
            
            if all_pricing_sections:
                main_content = ""
                processed_sections = 0
                
                # è·³è¿‡ç¬¬ä¸€ä¸ªsectionï¼ˆé€šå¸¸æ˜¯Descriptionï¼‰ï¼Œä»ç¬¬äºŒä¸ªå¼€å§‹æ™ºèƒ½åˆ†ç±»
                for section in all_pricing_sections[1:]:
                    section_type = classify_pricing_section(section)
                    
                    if section_type == 'content':
                        main_content += str(section)
                        processed_sections += 1
                        logger.info(f"âœ“ æ·»åŠ åŒºåŸŸç­›é€‰fallback content section #{processed_sections}")
                    elif section_type in ['faq', 'sla']:
                        logger.info(f"â© è·³è¿‡{section_type} sectionï¼ˆå°†ç”±SectionExtractorå¤„ç†ï¼‰")
                    else:
                        logger.info(f"â© è·³è¿‡{section_type} section")
                
                if main_content:
                    logger.info(f"âœ“ åŒºåŸŸç­›é€‰fallbackæ™ºèƒ½åˆ†ç±»å®Œæˆï¼Œå¤„ç†äº†{processed_sections}ä¸ªcontent sections")
                    return clean_html_content(main_content)
            
            # æ–¹æ¡ˆ3: ä½¿ç”¨ContentExtractorçš„ä¸»è¦å†…å®¹æå–
            logger.info("ğŸ” ä½¿ç”¨ContentExtractorä¸»è¦å†…å®¹æå–...")
            main_content = self.content_extractor.extract_main_content(soup)
            if main_content:
                return clean_html_content(main_content)
            
            logger.info("âš  æœªæ‰¾åˆ°åˆé€‚çš„fallbackä¸»è¦å†…å®¹")
            return ""
            
        except Exception as e:
            logger.info(f"âš  åŒºåŸŸç­›é€‰fallbackå†…å®¹æå–å¤±è´¥: {e}")
            return ""

    def _get_product_key(self) -> str:
        """è·å–äº§å“é”®"""
        if hasattr(self, 'product_config') and 'product_key' in self.product_config:
            return self.product_config['product_key']
        
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­
        if self.html_file_path:
            file_name = Path(self.html_file_path).stem
            if file_name.endswith('-index'):
                return file_name[:-6]
        
        return "unknown"