#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤æ‚å†…å®¹ç­–ç•¥ - åŸºäºæ–°æ¶æ„åˆ›å»º
å¤„ç†å¤æ‚çš„å¤šç­›é€‰å™¨å’Œtabç»„åˆï¼Œå¦‚Cloud Servicesç±»å‹é¡µé¢
å…¨æ–°å®ç°ï¼ŒåŸºäºæ–°å·¥å…·ç±»æ¶æ„
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.strategies.base_strategy import BaseStrategy
from src.utils.content.content_extractor import ContentExtractor
from src.utils.content.section_extractor import SectionExtractor
from src.utils.content.flexible_builder import FlexibleBuilder
from src.utils.data.extraction_validator import ExtractionValidator
from src.detectors.filter_detector import FilterDetector
from src.detectors.tab_detector import TabDetector
from src.core.logging import get_logger

logger = get_logger(__name__)


class ComplexContentStrategy(BaseStrategy):
    """
    å¤æ‚å†…å®¹ç­–ç•¥ - æ–°æ¶æ„å®ç°
    Type C: å¤æ‚é¡µé¢å¤„ç† - Cloud Servicesç±»å‹
    
    ç‰¹ç‚¹ï¼š
    - å…·æœ‰å¤šç§ç­›é€‰å™¨ç»„åˆï¼šè½¯ä»¶ç±»åˆ«ã€åœ°åŒºã€category tabsç­‰
    - å¤æ‚çš„äº¤äº’å’Œå†…å®¹æ˜ å°„å…³ç³»
    - éœ€è¦å¤„ç†å¤šç»´åº¦å†…å®¹ç»„åˆ
    - ä½¿ç”¨æ–°å·¥å…·ç±»æ¶æ„ï¼šContentExtractor + SectionExtractor + FlexibleBuilder + FilterDetector + TabDetector
    """

    def __init__(self, product_config: Dict[str, Any], html_file_path: str = ""):
        """
        åˆå§‹åŒ–å¤æ‚å†…å®¹ç­–ç•¥
        
        Args:
            product_config: äº§å“é…ç½®ä¿¡æ¯
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
        """
        super().__init__(product_config, html_file_path)
        self.strategy_name = "complex_content"
        
        # åˆå§‹åŒ–å·¥å…·ç±»
        self.content_extractor = ContentExtractor()
        self.section_extractor = SectionExtractor()
        self.flexible_builder = FlexibleBuilder()
        self.extraction_validator = ExtractionValidator()
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.filter_detector = FilterDetector()
        self.tab_detector = TabDetector()
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–å¤æ‚å†…å®¹ç­–ç•¥: {self._get_product_key()}")

    def extract(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œä¼ ç»ŸCMSæ ¼å¼æå–é€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            ä¼ ç»ŸCMSæ ¼å¼çš„æå–æ•°æ®
        """
        logger.info("ğŸ”§ å¼€å§‹å¤æ‚å†…å®¹ç­–ç•¥æå–ï¼ˆä¼ ç»ŸCMSæ ¼å¼ï¼‰...")
        
        # 1. ä½¿ç”¨ContentExtractoræå–åŸºç¡€å…ƒæ•°æ®
        data = self.content_extractor.extract_base_metadata(soup, url, self.html_file_path)
        
        # 2. ä½¿ç”¨SectionExtractoræå–sectionså†…å®¹
        sections = self.section_extractor.extract_all_sections(soup)
        
        # è½¬æ¢sectionsä¸ºä¼ ç»ŸCMSæ ¼å¼
        for section in sections:
            section_type = section.get("sectionType", "")
            content = section.get("content", "")
            
            if section_type == "Banner":
                data["BannerContent"] = content
            elif section_type == "Description":
                data["DescriptionContent"] = content
            elif section_type == "Qa":
                data["QaContent"] = content
        
        # 3. åˆ†æç­›é€‰å™¨å’Œtabç»“æ„
        filter_analysis = self.filter_detector.detect_filters(soup)
        tab_analysis = self.tab_detector.detect_tabs(soup)
        
        # 4. æå–å¤æ‚å†…å®¹ï¼ˆç®€åŒ–å¤„ç†ï¼Œä¸»è¦å†…å®¹æ”¾åœ¨NoRegionContentä¸­ï¼‰
        data["HasRegion"] = False  # ä¼ ç»ŸCMSæ ¼å¼ç®€åŒ–å¤„ç†
        data["NoRegionContent"] = self._extract_complex_main_content(soup, filter_analysis, tab_analysis)
        
        # 5. è®¾ç½®ä¼ ç»ŸCMSå­—æ®µ
        data["PricingTables"] = []
        data["ServiceTiers"] = []
        data["RegionalContent"] = {}
        
        # æ¸…ç©ºåŒºåŸŸå†…å®¹å­—æ®µ
        region_fields = [
            "NorthChinaContent", "NorthChina2Content", "NorthChina3Content",
            "EastChinaContent", "EastChina2Content", "EastChina3Content"
        ]
        for field in region_fields:
            data[field] = ""
        
        # 6. æ·»åŠ å¤æ‚æ€§åˆ†æå…ƒæ•°æ®
        data["complexity_analysis"] = {
            "has_filters": filter_analysis.get("has_region", False) or filter_analysis.get("has_software", False),
            "has_tabs": tab_analysis.get("has_tabs", False),
            "filter_count": len(filter_analysis.get("region_options", [])) + len(filter_analysis.get("software_options", [])),
            "tab_count": tab_analysis.get("total_category_tabs", 0)
        }
        
        # 7. éªŒè¯æå–ç»“æœ
        data = self.extraction_validator.validate_cms_extraction(data, self.product_config)
        
        logger.info("âœ… å¤æ‚å†…å®¹ç­–ç•¥æå–å®Œæˆï¼ˆä¼ ç»ŸCMSæ ¼å¼ï¼‰")
        return data

    def extract_flexible_content(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œflexible JSONæ ¼å¼æå–é€»è¾‘
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            flexible JSONæ ¼å¼çš„æå–æ•°æ®
        """
        logger.info("ğŸ”§ å¼€å§‹å¤æ‚å†…å®¹ç­–ç•¥æå–ï¼ˆflexible JSONæ ¼å¼ï¼‰...")
        
        # 1. ä½¿ç”¨ContentExtractoræå–åŸºç¡€å…ƒæ•°æ®
        base_metadata = self.content_extractor.extract_base_metadata(soup, url, self.html_file_path)
        
        # 2. ä½¿ç”¨SectionExtractoræå–commonSections
        common_sections = self.section_extractor.extract_all_sections(soup)
        
        # 3. åˆ†æç­›é€‰å™¨å’Œtabç»“æ„
        filter_analysis = self.filter_detector.detect_filters(soup)
        tab_analysis = self.tab_detector.detect_tabs(soup)
        
        # 4. æå–å¤æ‚å†…å®¹æ˜ å°„
        content_mapping = self._extract_complex_content_mapping(soup, filter_analysis, tab_analysis)
        
        # 5. ä½¿ç”¨FlexibleBuilderæ„å»ºå¤æ‚å†…å®¹ç»„
        content_groups = self.flexible_builder.build_complex_content_groups(
            filter_analysis, tab_analysis, content_mapping
        )
        
        # 6. æ„å»ºé¡µé¢é…ç½®
        page_config = self.flexible_builder.build_page_config(filter_analysis, tab_analysis)
        
        # 7. æ„å»ºç­–ç•¥ç‰¹å®šå†…å®¹
        strategy_content = {
            "baseContent": "",  # å¤æ‚é¡µé¢ä¸»è¦å†…å®¹åœ¨contentGroupsä¸­
            "contentGroups": content_groups,
            "pageConfig": page_config,
            "strategy_type": "complex_content"
        }
        
        # 8. ä½¿ç”¨FlexibleBuilderæ„å»ºå®Œæ•´çš„flexible JSON
        flexible_data = self.flexible_builder.build_flexible_page(
            base_metadata, common_sections, strategy_content
        )
        
        # 9. éªŒè¯flexible JSONç»“æœ
        flexible_data = self.extraction_validator.validate_flexible_json(flexible_data)
        
        logger.info("âœ… å¤æ‚å†…å®¹ç­–ç•¥æå–å®Œæˆï¼ˆflexible JSONæ ¼å¼ï¼‰")
        return flexible_data

    def extract_common_sections(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """
        æå–é€šç”¨sectionsï¼ˆBannerã€Descriptionã€QAç­‰ï¼‰
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            
        Returns:
            commonSectionsåˆ—è¡¨
        """
        return self.section_extractor.extract_all_sections(soup)

    def _extract_complex_main_content(self, soup: BeautifulSoup, 
                                    filter_analysis: Dict[str, Any], 
                                    tab_analysis: Dict[str, Any]) -> str:
        """
        æå–å¤æ‚é¡µé¢çš„ä¸»è¦å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼Œç”¨äºä¼ ç»ŸCMSæ ¼å¼ï¼‰
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            filter_analysis: ç­›é€‰å™¨åˆ†æç»“æœ
            tab_analysis: Tabåˆ†æç»“æœ
            
        Returns:
            ä¸»è¦å†…å®¹HTMLå­—ç¬¦ä¸²
        """
        logger.info("ğŸ“ æå–å¤æ‚é¡µé¢ä¸»è¦å†…å®¹...")
        
        try:
            main_content = ""
            
            # 1. ä¼˜å…ˆæŸ¥æ‰¾technical-azure-selectorä¸»å®¹å™¨
            main_container = soup.find('div', class_='technical-azure-selector')
            if main_container:
                # è·å–æ‰€æœ‰å†…å®¹ç»„ï¼ˆtabContentåˆ†ç»„ï¼‰
                content_groups = tab_analysis.get("content_groups", [])
                
                if content_groups:
                    # æå–ç¬¬ä¸€ä¸ªå†…å®¹ç»„çš„å†…å®¹ä½œä¸ºç¤ºä¾‹
                    first_group = content_groups[0]
                    group_id = first_group.get("id", "")
                    
                    group_element = soup.find('div', id=group_id)
                    if group_element:
                        # è¿‡æ»¤QAç­‰é‡å¤å†…å®¹
                        content_text = group_element.get_text().lower()
                        if not any(skip_text in content_text for skip_text in [
                            'å¸¸è§é—®é¢˜', 'faq', 'æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®'
                        ]):
                            main_content = str(group_element)
                            logger.info(f"âœ“ æ‰¾åˆ°å¤æ‚é¡µé¢ä¸»å®¹å™¨å†…å®¹ï¼ˆ{group_id}ï¼‰")
                            return main_content
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†ç»„ï¼Œè¿”å›æ•´ä¸ªä¸»å®¹å™¨
                main_content = str(main_container)
                logger.info("âœ“ æ‰¾åˆ°å¤æ‚é¡µé¢ä¸»å®¹å™¨å†…å®¹ï¼ˆå®Œæ•´ï¼‰")
                return main_content
            
            # 2. å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾pricing-page-section
            pricing_sections = soup.find_all('div', class_='pricing-page-section')
            if pricing_sections:
                for i, section in enumerate(pricing_sections):
                    if i > 0:  # è·³è¿‡ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æè¿°ï¼‰
                        section_text = section.get_text().lower()
                        if not any(skip_text in section_text for skip_text in [
                            'banner', 'navigation', 'nav', 'å¸¸è§é—®é¢˜', 'faq'
                        ]):
                            main_content += str(section)
                
                if main_content:
                    logger.info("âœ“ ä½¿ç”¨pricing-page-sectionå¤‡ç”¨æ–¹æ¡ˆ")
                    return main_content
            
            # 3. æœ€åå¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ContentExtractor
            main_content = self.content_extractor.extract_main_content(soup)
            if main_content:
                logger.info("âœ“ ä½¿ç”¨ContentExtractorå¤‡ç”¨æ–¹æ¡ˆ")
                return main_content
            
            logger.info("âš  æœªæ‰¾åˆ°åˆé€‚çš„å¤æ‚é¡µé¢ä¸»è¦å†…å®¹")
            return ""
            
        except Exception as e:
            logger.info(f"âš  å¤æ‚é¡µé¢ä¸»è¦å†…å®¹æå–å¤±è´¥: {e}")
            return ""

    def _extract_complex_content_mapping(self, soup: BeautifulSoup,
                                       filter_analysis: Dict[str, Any],
                                       tab_analysis: Dict[str, Any]) -> Dict[str, str]:
        """
        æå–å¤æ‚é¡µé¢çš„å†…å®¹æ˜ å°„å…³ç³»
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            filter_analysis: ç­›é€‰å™¨åˆ†æç»“æœ
            tab_analysis: Tabåˆ†æç»“æœ
            
        Returns:
            å†…å®¹æ˜ å°„å­—å…¸
        """
        logger.info("ğŸ—ºï¸ æå–å¤æ‚é¡µé¢å†…å®¹æ˜ å°„...")
        
        content_mapping = {}
        
        try:
            # è·å–regioné€‰é¡¹
            region_options = filter_analysis.get("region_options", [])
            software_options = filter_analysis.get("software_options", [])
            category_tabs = tab_analysis.get("category_tabs", [])
            
            # å¦‚æœæ²¡æœ‰åŒºåŸŸé€‰é¡¹ï¼Œä½¿ç”¨é»˜è®¤
            if not region_options:
                region_options = [{"value": "default", "label": "é»˜è®¤"}]
            
            # æ„å»ºå¤šç»´åº¦æ˜ å°„
            for region in region_options:
                region_id = region.get("value", "")
                
                if software_options:
                    # æœ‰è½¯ä»¶ç­›é€‰å™¨çš„æƒ…å†µ
                    for software in software_options:
                        software_id = software.get("value", "")
                        
                        if category_tabs:
                            # æœ‰category tabsçš„æƒ…å†µ - ä¸‰ç»´æ˜ å°„
                            for tab in category_tabs:
                                tab_id = tab.get("href", "").replace("#", "")
                                content_key = f"{region_id}_{software_id}_{tab_id}"
                                
                                # å°è¯•æ‰¾åˆ°å¯¹åº”çš„å†…å®¹
                                content = self._find_content_by_mapping(soup, region_id, software_id, tab_id)
                                if content:
                                    content_mapping[content_key] = content
                        else:
                            # åªæœ‰region + software - äºŒç»´æ˜ å°„
                            content_key = f"{region_id}_{software_id}"
                            content = self._find_content_by_mapping(soup, region_id, software_id)
                            if content:
                                content_mapping[content_key] = content
                elif category_tabs:
                    # åªæœ‰region + category tabs - äºŒç»´æ˜ å°„
                    for tab in category_tabs:
                        tab_id = tab.get("href", "").replace("#", "")
                        content_key = f"{region_id}_{tab_id}"
                        content = self._find_content_by_mapping(soup, region_id, None, tab_id)
                        if content:
                            content_mapping[content_key] = content
                else:
                    # åªæœ‰region - ä¸€ç»´æ˜ å°„
                    content_key = region_id
                    content = self._find_content_by_mapping(soup, region_id)
                    if content:
                        content_mapping[content_key] = content
            
            logger.info(f"âœ“ æ„å»ºäº† {len(content_mapping)} ä¸ªå†…å®¹æ˜ å°„")
            return content_mapping
            
        except Exception as e:
            logger.info(f"âš  å†…å®¹æ˜ å°„æå–å¤±è´¥: {e}")
            return {}

    def _find_content_by_mapping(self, soup: BeautifulSoup, 
                               region_id: str = None,
                               software_id: str = None, 
                               tab_id: str = None) -> str:
        """
        æ ¹æ®æ˜ å°„å…³ç³»æŸ¥æ‰¾å¯¹åº”å†…å®¹
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            region_id: åŒºåŸŸID
            software_id: è½¯ä»¶ID
            tab_id: Tab ID
            
        Returns:
            æ‰¾åˆ°çš„å†…å®¹HTMLå­—ç¬¦ä¸²
        """
        try:
            # 1. å¦‚æœæœ‰tab_idï¼Œä¼˜å…ˆæŸ¥æ‰¾tabå¯¹åº”å†…å®¹
            if tab_id:
                tab_content = soup.find('div', id=tab_id)
                if tab_content:
                    return str(tab_content)
            
            # 2. å¦‚æœæœ‰software_idï¼ŒæŸ¥æ‰¾å¯¹åº”çš„tabContentåˆ†ç»„
            if software_id:
                # è½¯ä»¶ç­›é€‰å™¨é€šå¸¸å¯¹åº”tabContentåˆ†ç»„
                content_groups = soup.find_all('div', class_='tab-panel')
                for group in content_groups:
                    group_id = group.get('id', '')
                    if 'tabContent' in group_id:
                        # è¿™æ˜¯ä¸€ä¸ªå†…å®¹åˆ†ç»„ï¼Œè¿”å›å…¶å†…å®¹
                        return str(group)
            
            # 3. é»˜è®¤è¿”å›ä¸»è¦å†…å®¹åŒºåŸŸ
            main_container = soup.find('div', class_='technical-azure-selector')
            if main_container:
                return str(main_container)
            
            return ""
            
        except Exception as e:
            logger.info(f"âš  å†…å®¹æŸ¥æ‰¾å¤±è´¥: {e}")
            return ""

    def _get_product_key(self) -> str:
        """è·å–äº§å“é”®"""
        if hasattr(self, 'product_config') and 'product_key' in self.product_config:
            return self.product_config['product_key']
        
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­
        if self.html_file_path:
            file_name = Path(self.html_file_path).stem
            if file_name.endswith('-index'):
                return file_name[:-6]
        
        return "unknown"