#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤æ‚å†…å®¹ç­–ç•¥ - åŸºäºæ–°æ¶æ„åˆ›å»º
å¤„ç†å¤æ‚çš„å¤šç­›é€‰å™¨å’Œtabç»„åˆï¼Œå¦‚Cloud Servicesç±»å‹é¡µé¢
å…¨æ–°å®ç°ï¼ŒåŸºäºæ–°å·¥å…·ç±»æ¶æ„
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.strategies.base_strategy import BaseStrategy
from src.core.region_processor import RegionProcessor
from src.utils.content.content_extractor import ContentExtractor
from src.utils.content.section_extractor import SectionExtractor
from src.utils.content.flexible_builder import FlexibleBuilder
from src.utils.data.extraction_validator import ExtractionValidator
from src.detectors.filter_detector import FilterDetector
from src.detectors.tab_detector import TabDetector
from src.utils.content.content_utils import classify_pricing_section, filter_sections_by_type
from src.utils.html.cleaner import clean_html_content
from src.core.logging import get_logger

logger = get_logger(__name__)


class ComplexContentStrategy(BaseStrategy):
    """
    å¤æ‚å†…å®¹ç­–ç•¥ - æ–°æ¶æ„å®ç°
    Type C: å¤æ‚é¡µé¢å¤„ç† - Cloud Servicesç±»å‹
    
    ç‰¹ç‚¹ï¼š
    - å…·æœ‰å¤šç§ç­›é€‰å™¨ç»„åˆï¼šè½¯ä»¶ç±»åˆ«ã€åœ°åŒºã€category tabsç­‰
    - å¤æ‚çš„äº¤äº’å’Œå†…å®¹æ˜ å°„å…³ç³»
    - éœ€è¦å¤„ç†å¤šç»´åº¦å†…å®¹ç»„åˆ
    - ä½¿ç”¨æ–°å·¥å…·ç±»æ¶æ„ï¼šContentExtractor + SectionExtractor + FlexibleBuilder + FilterDetector + TabDetector
    """

    def __init__(self, product_config: Dict[str, Any], html_file_path: str = ""):
        """
        åˆå§‹åŒ–å¤æ‚å†…å®¹ç­–ç•¥
        
        Args:
            product_config: äº§å“é…ç½®ä¿¡æ¯
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
        """
        super().__init__(product_config, html_file_path)
        self.strategy_name = "complex_content"
        
        # åˆå§‹åŒ–å·¥å…·ç±»
        self.content_extractor = ContentExtractor()
        self.section_extractor = SectionExtractor()
        self.flexible_builder = FlexibleBuilder()
        self.extraction_validator = ExtractionValidator()
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.filter_detector = FilterDetector()
        self.tab_detector = TabDetector()
        
        # åˆå§‹åŒ–åŒºåŸŸå¤„ç†å™¨ï¼ˆç”¨äºè¡¨æ ¼ç­›é€‰ï¼‰
        self.region_processor = RegionProcessor()
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–å¤æ‚å†…å®¹ç­–ç•¥: {self._get_product_key()}")

    def extract_flexible_content(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œflexible JSONæ ¼å¼æå–é€»è¾‘
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            flexible JSONæ ¼å¼çš„æå–æ•°æ®
        """
        logger.info("ğŸ”§ å¼€å§‹å¤æ‚å†…å®¹ç­–ç•¥æå–ï¼ˆflexible JSONæ ¼å¼ï¼‰...")
        
        # 1. ä½¿ç”¨ContentExtractoræå–åŸºç¡€å…ƒæ•°æ®
        base_metadata = self.content_extractor.extract_base_metadata(soup, url, self.html_file_path)
        
        # 2. ä½¿ç”¨SectionExtractoræå–commonSections
        common_sections = self.section_extractor.extract_all_sections(soup)
        
        # 3. åˆ†æç­›é€‰å™¨å’Œtabç»“æ„
        filter_analysis = self.filter_detector.detect_filters(soup)
        tab_analysis = self.tab_detector.detect_tabs(soup)
        
        # 4. æå–å¤æ‚å†…å®¹æ˜ å°„
        content_mapping = self._extract_complex_content_mapping(soup, filter_analysis, tab_analysis)
        
        # 5. ä½¿ç”¨FlexibleBuilderæ„å»ºå¤æ‚å†…å®¹ç»„
        content_groups = self.flexible_builder.build_complex_content_groups(
            filter_analysis, tab_analysis, content_mapping
        )

        # 6. æ„å»ºç­–ç•¥ç‰¹å®šå†…å®¹ï¼ŒåŒ…å«æ™ºèƒ½åˆ†ç±»çš„baseContent
        # å¯¹äºå¤æ‚ç­–ç•¥ï¼Œå¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„å†…å®¹ç»„ï¼Œå¯ä»¥æå–baseContentä½œä¸ºfallback
        base_content = ""
        if not content_groups or len(content_groups) == 0:
            logger.info("âš  æœªæ‰¾åˆ°å¤æ‚å†…å®¹ç»„ï¼Œå°è¯•æå–é€šç”¨baseContent...")
            base_content = self._extract_main_content(soup)
        
        strategy_content = {
            "baseContent": base_content,  # å¦‚æœæœ‰å†…å®¹ç»„åˆ™ä¸ºç©ºï¼Œå¦åˆ™ä½œä¸ºfallback
            "contentGroups": content_groups,
            "strategy_type": "complex",
            "filter_analysis": filter_analysis,  # ä¼ é€’ç­›é€‰å™¨åˆ†æç»“æœ
            "tab_analysis": tab_analysis  # ä¼ é€’tabåˆ†æç»“æœ
        }
        
        # 7. ä½¿ç”¨FlexibleBuilderæ„å»ºå®Œæ•´çš„flexible JSON
        flexible_data = self.flexible_builder.build_flexible_page(
            base_metadata, common_sections, strategy_content
        )
        
        # 8. éªŒè¯flexible JSONç»“æœ
        flexible_data = self.extraction_validator.validate_flexible_json(flexible_data)
        
        logger.info("âœ… å¤æ‚å†…å®¹ç­–ç•¥æå–å®Œæˆï¼ˆflexible JSONæ ¼å¼ï¼‰")
        return flexible_data

    def extract_common_sections(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """
        æå–é€šç”¨sectionsï¼ˆBannerã€Descriptionã€QAç­‰ï¼‰
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            
        Returns:
            commonSectionsåˆ—è¡¨
        """
        return self.section_extractor.extract_all_sections(soup)

    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """
        æå–å¤æ‚é¡µé¢çš„ä¸»è¦å†…å®¹ - ä½¿ç”¨æ™ºèƒ½åˆ†ç±»é€»è¾‘
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            ä¸»è¦å†…å®¹HTMLå­—ç¬¦ä¸²
        """
        logger.info("ğŸ“ æå–å¤æ‚é¡µé¢ä¸»è¦å†…å®¹ï¼ˆæ™ºèƒ½åˆ†ç±»æ¨¡å¼ï¼‰...")
        
        try:
            # æ–¹æ¡ˆ1: æŸ¥æ‰¾technical-azure-selectorå†…çš„pricing-page-sectionï¼Œä½¿ç”¨æ™ºèƒ½åˆ†ç±»
            logger.info("ğŸ” æŸ¥æ‰¾technical-azure-selectorå†…å®¹ï¼ˆæ™ºèƒ½åˆ†ç±»ï¼‰...")
            technical_selector = soup.find('div', class_='technical-azure-selector')
            if technical_selector:
                pricing_sections = technical_selector.find_all('div', class_='pricing-page-section')
                if pricing_sections:
                    # ä½¿ç”¨æ™ºèƒ½åˆ†ç±»è¿‡æ»¤ï¼Œåªä¿ç•™contentç±»å‹çš„section
                    content_sections = filter_sections_by_type(
                        pricing_sections, 
                        include_types=['content']
                    )
                    
                    if content_sections:
                        main_content = ""
                        for section in content_sections:
                            main_content += str(section)
                            section_type = classify_pricing_section(section)
                            logger.info(f"âœ“ æ·»åŠ å¤æ‚ç­–ç•¥technical-azure-selector section (ç±»å‹: {section_type})")
                        
                        logger.info(f"âœ“ æ‰¾åˆ°å¤æ‚ç­–ç•¥technical-azure-selectorå†…å®¹ï¼Œå…±{len(content_sections)}ä¸ªcontent sections")
                        return clean_html_content(main_content)
                
                # å¦‚æœæ²¡æœ‰åˆ†ç±»ä¸ºcontentçš„sectionï¼Œè¿”å›æ•´ä¸ªä¸»å®¹å™¨ä½†è¿‡æ»¤FAQ/SLA
                logger.info("ğŸ” è¿”å›æ•´ä¸ªtechnical-azure-selectorå®¹å™¨...")
                all_sections = technical_selector.find_all('div', class_='pricing-page-section')
                
                filtered_main_content = ""
                for section in all_sections:
                    section_type = classify_pricing_section(section)
                    if section_type in ['content', 'other']:  # åŒ…å«otherç±»å‹ä»¥ç¡®ä¿ä¸é—æ¼å†…å®¹
                        filtered_main_content += str(section)
                        logger.info(f"âœ“ æ·»åŠ {section_type}ç±»å‹sectionåˆ°å¤æ‚ç­–ç•¥å†…å®¹")
                
                if filtered_main_content:
                    return clean_html_content(filtered_main_content)
                else:
                    # æœ€åfallbackï¼šè¿”å›å®Œæ•´ä¸»å®¹å™¨
                    return clean_html_content(str(technical_selector))
            
            # æ–¹æ¡ˆ2: æŸ¥æ‰¾æ‰€æœ‰pricing-page-sectionï¼Œæ™ºèƒ½åˆ†ç±»åå¤„ç†
            logger.info("ğŸ” æŸ¥æ‰¾æ‰€æœ‰pricing-page-sectionï¼ˆæ™ºèƒ½åˆ†ç±»ï¼‰...")
            all_pricing_sections = soup.find_all('div', class_='pricing-page-section')
            
            if all_pricing_sections:
                main_content = ""
                processed_sections = 0
                
                # è·³è¿‡ç¬¬ä¸€ä¸ªsectionï¼ˆé€šå¸¸æ˜¯Descriptionï¼‰ï¼Œä»ç¬¬äºŒä¸ªå¼€å§‹æ™ºèƒ½åˆ†ç±»
                for section in all_pricing_sections[1:]:
                    section_type = classify_pricing_section(section)
                    
                    if section_type == 'content':
                        main_content += str(section)
                        processed_sections += 1
                        logger.info(f"âœ“ æ·»åŠ å¤æ‚ç­–ç•¥content section #{processed_sections}")
                    elif section_type in ['faq', 'sla']:
                        logger.info(f"â© è·³è¿‡{section_type} sectionï¼ˆå°†ç”±SectionExtractorå¤„ç†ï¼‰")
                    else:
                        logger.info(f"â© è·³è¿‡{section_type} section")
                
                if main_content:
                    logger.info(f"âœ“ å¤æ‚ç­–ç•¥æ™ºèƒ½åˆ†ç±»å®Œæˆï¼Œå¤„ç†äº†{processed_sections}ä¸ªcontent sections")
                    return clean_html_content(main_content)
            
            # æ–¹æ¡ˆ3: ä½¿ç”¨ContentExtractorçš„ä¸»è¦å†…å®¹æå–
            logger.info("ğŸ” ä½¿ç”¨ContentExtractorä¸»è¦å†…å®¹æå–...")
            main_content = self.content_extractor.extract_main_content(soup)
            if main_content:
                return clean_html_content(main_content)
            
            logger.info("âš  æœªæ‰¾åˆ°åˆé€‚çš„å¤æ‚é¡µé¢ä¸»è¦å†…å®¹")
            return ""
            
        except Exception as e:
            logger.info(f"âš  å¤æ‚é¡µé¢ä¸»è¦å†…å®¹æå–å¤±è´¥: {e}")
            return ""

    def _extract_complex_content_mapping(self, soup: BeautifulSoup,
                                       filter_analysis: Dict[str, Any],
                                       tab_analysis: Dict[str, Any]) -> Dict[str, str]:
        """
        æå–å¤æ‚é¡µé¢çš„å†…å®¹æ˜ å°„å…³ç³»ï¼ˆå¸¦åŒºåŸŸç­›é€‰ï¼‰
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            filter_analysis: ç­›é€‰å™¨åˆ†æç»“æœ
            tab_analysis: Tabåˆ†æç»“æœ
            
        Returns:
            å†…å®¹æ˜ å°„å­—å…¸
        """
        logger.info("ğŸ—ºï¸ æå–å¤æ‚é¡µé¢å†…å®¹æ˜ å°„ï¼ˆæ”¯æŒåŒºåŸŸç­›é€‰ï¼‰...")
        
        content_mapping = {}
        
        try:
            # è·å–ç”¨äºåŒºåŸŸç­›é€‰çš„OSåç§°
            os_name = self.region_processor.get_os_name_for_region_filtering(
                product_config=self.product_config,
                filter_analysis=filter_analysis,
                html_file_path=self.html_file_path
            )
            
            if not os_name:
                logger.warning("âš  æ— æ³•è·å–æœ‰æ•ˆçš„OSåç§°ï¼Œå°†è·³è¿‡åŒºåŸŸè¡¨æ ¼ç­›é€‰")
            else:
                logger.info(f"ğŸ¯ ä½¿ç”¨OSåç§° '{os_name}' è¿›è¡ŒåŒºåŸŸè¡¨æ ¼ç­›é€‰")
            
            # è·å–regioné€‰é¡¹
            region_options = filter_analysis.get("region_options", [])
            software_options = filter_analysis.get("software_options", [])
            category_tabs = tab_analysis.get("category_tabs", [])
            
            # å¦‚æœæ²¡æœ‰åŒºåŸŸé€‰é¡¹ï¼Œä½¿ç”¨é»˜è®¤
            if not region_options:
                region_options = [{"value": "default", "label": "é»˜è®¤"}]
            
            # æ„å»ºå¤šç»´åº¦æ˜ å°„
            for region in region_options:
                region_id = region.get("value", "")
                
                if software_options:
                    # æœ‰è½¯ä»¶ç­›é€‰å™¨çš„æƒ…å†µ
                    for software in software_options:
                        software_id = software.get("value", "")
                        
                        if category_tabs:
                            # æœ‰category tabsçš„æƒ…å†µ - ä¸‰ç»´æ˜ å°„
                            for tab in category_tabs:
                                tab_id = tab.get("href", "").replace("#", "")
                                content_key = f"{region_id}_{software_id}_{tab_id}"
                                
                                # å°è¯•æ‰¾åˆ°å¯¹åº”çš„å†…å®¹å¹¶åº”ç”¨åŒºåŸŸç­›é€‰
                                content = self._find_content_by_mapping(soup, region_id, software_id, tab_id, os_name)
                                if content:
                                    content_mapping[content_key] = content
                        else:
                            # åªæœ‰region + software - äºŒç»´æ˜ å°„
                            content_key = f"{region_id}_{software_id}"
                            content = self._find_content_by_mapping(soup, region_id, software_id, None, os_name)
                            if content:
                                content_mapping[content_key] = content
                elif category_tabs:
                    # åªæœ‰region + category tabs - äºŒç»´æ˜ å°„
                    for tab in category_tabs:
                        tab_id = tab.get("href", "").replace("#", "")
                        content_key = f"{region_id}_{tab_id}"
                        content = self._find_content_by_mapping(soup, region_id, None, tab_id, os_name)
                        if content:
                            content_mapping[content_key] = content
                else:
                    # åªæœ‰region - ä¸€ç»´æ˜ å°„
                    content_key = region_id
                    content = self._find_content_by_mapping(soup, region_id, None, None, os_name)
                    if content:
                        content_mapping[content_key] = content
            
            logger.info(f"âœ“ æ„å»ºäº† {len(content_mapping)} ä¸ªå†…å®¹æ˜ å°„")
            return content_mapping
            
        except Exception as e:
            logger.info(f"âš  å†…å®¹æ˜ å°„æå–å¤±è´¥: {e}")
            return {}

    def _find_content_by_mapping(self, soup: BeautifulSoup, 
                               region_id: str = None,
                               software_id: str = None, 
                               tab_id: str = None,
                               os_name: str = None) -> str:
        """
        æ ¹æ®æ˜ å°„å…³ç³»æŸ¥æ‰¾å¯¹åº”å†…å®¹ï¼ˆæ”¯æŒåŒºåŸŸè¡¨æ ¼ç­›é€‰ï¼‰
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            region_id: åŒºåŸŸID
            software_id: è½¯ä»¶ID
            tab_id: Tab ID
            os_name: OSåç§°ï¼Œç”¨äºåŒºåŸŸç­›é€‰
            
        Returns:
            æ‰¾åˆ°çš„å†…å®¹HTMLå­—ç¬¦ä¸²ï¼ˆç»è¿‡åŒºåŸŸç­›é€‰ï¼‰
        """
        try:
            # é¦–å…ˆä»åŸå§‹soupä¸­æ‰¾åˆ°åŸºç¡€å†…å®¹
            base_content = None
            
            # 1. å¦‚æœæœ‰tab_idï¼Œä¼˜å…ˆæŸ¥æ‰¾tabå¯¹åº”å†…å®¹
            if tab_id:
                base_content = soup.find('div', id=tab_id)
                if base_content:
                    logger.info(f"âœ“ æ‰¾åˆ°tabå†…å®¹: {tab_id}")
            
            # 2. å¦‚æœæœ‰software_idï¼ŒæŸ¥æ‰¾å¯¹åº”çš„tabContentåˆ†ç»„
            if not base_content and software_id:
                content_groups = soup.find_all('div', class_='tab-panel')
                for group in content_groups:
                    group_id = group.get('id', '')
                    if 'tabContent' in group_id:
                        base_content = group
                        logger.info(f"âœ“ æ‰¾åˆ°è½¯ä»¶å†…å®¹ç»„: {group_id}")
                        break
            
            # 3. é»˜è®¤è¿”å›ä¸»è¦å†…å®¹åŒºåŸŸ
            if not base_content:
                base_content = soup.find('div', class_='technical-azure-selector')
                if base_content:
                    logger.info("âœ“ ä½¿ç”¨ä¸»è¦å†…å®¹åŒºåŸŸ")
            
            if not base_content:
                logger.warning("âš  æœªæ‰¾åˆ°ä»»ä½•åŸºç¡€å†…å®¹")
                return ""
            
            # åº”ç”¨åŒºåŸŸç­›é€‰ï¼ˆå¦‚æœæœ‰region_idå’Œos_nameï¼‰
            if region_id and os_name:
                logger.info(f"ğŸ” å¯¹å†…å®¹åº”ç”¨åŒºåŸŸç­›é€‰: region={region_id}, os={os_name}")
                # åˆ›å»ºåŒ…å«æ‰¾åˆ°å†…å®¹çš„ä¸´æ—¶soup
                temp_soup = BeautifulSoup(str(base_content), 'html.parser')
                # åº”ç”¨åŒºåŸŸç­›é€‰
                filtered_soup = self.region_processor.apply_region_filtering(temp_soup, region_id, os_name)
                return str(filtered_soup)
            else:
                # æ²¡æœ‰åŒºåŸŸä¿¡æ¯ï¼Œç›´æ¥è¿”å›åŸå§‹å†…å®¹
                if not region_id:
                    logger.info("â„¹ æ— åŒºåŸŸIDï¼Œè·³è¿‡åŒºåŸŸç­›é€‰")
                if not os_name:
                    logger.info("â„¹ æ— OSåç§°ï¼Œè·³è¿‡åŒºåŸŸç­›é€‰")
                return str(base_content)
            
        except Exception as e:
            logger.info(f"âš  å†…å®¹æŸ¥æ‰¾å¤±è´¥: {e}")
            return ""

    def _get_product_key(self) -> str:
        """è·å–äº§å“é”®"""
        if hasattr(self, 'product_config') and 'product_key' in self.product_config:
            return self.product_config['product_key']
        
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­
        if self.html_file_path:
            file_name = Path(self.html_file_path).stem
            if file_name.endswith('-index'):
                return file_name[:-6]
        
        return "unknown"