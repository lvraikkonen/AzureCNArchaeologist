#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºç¡€ç­–ç•¥æŠ½è±¡ç±»
å®šä¹‰æ‰€æœ‰æå–ç­–ç•¥çš„é€šç”¨æ¥å£å’Œå…±ç”¨æ–¹æ³•
"""

import os
import sys
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.utils.html.element_creator import create_simple_element
from src.utils.html.cleaner import clean_html_content
from src.utils.media.image_processor import preprocess_image_paths
from src.utils.content.content_utils import (
    extract_qa_content, find_main_content_area, extract_banner_text_content, 
    extract_structured_content
)
from src.utils.data.validation_utils import validate_extracted_data


class BaseStrategy(ABC):
    """åŸºç¡€ç­–ç•¥æŠ½è±¡ç±»ï¼Œæ‰€æœ‰æå–ç­–ç•¥çš„åŸºç¡€"""

    def __init__(self, product_config: Dict[str, Any], html_file_path: str = ""):
        """
        åˆå§‹åŒ–åŸºç¡€ç­–ç•¥
        
        Args:
            product_config: äº§å“é…ç½®ä¿¡æ¯
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
        """
        self.product_config = product_config
        self.html_file_path = html_file_path

    @abstractmethod
    def extract(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œå…·ä½“çš„æå–é€»è¾‘
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            æå–çš„CMSå†…å®¹æ•°æ®
        """
        pass

    def _extract_base_content(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æå–æ‰€æœ‰ç­–ç•¥å…±ç”¨çš„åŸºç¡€å†…å®¹
        åŒ…æ‹¬ï¼šTitle, Metaä¿¡æ¯, Banner, Descriptionç­‰é€šç”¨å­—æ®µ
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            åŸºç¡€å†…å®¹å­—å…¸
        """
        print("ğŸ” æå–åŸºç¡€å†…å®¹...")

        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = find_main_content_area(soup)
        
        # åˆå§‹åŒ–åŸºç¡€æ•°æ®ç»“æ„
        base_data = {
            "source_file": str(self.html_file_path),
            "source_url": url or self._get_default_url(),
            "extraction_timestamp": datetime.now().isoformat(),
            "Title": "",
            "MetaDescription": "",
            "MetaKeywords": "",
            "MSServiceName": "",
            "Slug": "",
            "BannerContent": "",
            "NavigationTitle": "",
            "DescriptionContent": "",
            "Language": "zh-cn",
            "QaContent": "",
            "PricingTables": [],
            "ServiceTiers": [],
            "LastModified": "",
            "RegionalContent": {},
        }

        # 1. æå–æ ‡é¢˜
        print("ğŸ·ï¸ æå–æ ‡é¢˜...")
        base_data["Title"] = self._extract_page_title(main_content or soup)
        
        # 2. æå–Metaä¿¡æ¯
        print("ğŸ“‹ æå–Metaä¿¡æ¯...")
        base_data["MetaDescription"] = self._extract_meta_description(soup)
        base_data["MetaKeywords"] = self._extract_meta_keywords(soup)
        base_data["MSServiceName"] = self._extract_ms_service_name(soup)
        base_data["Slug"] = self._extract_slug(url)

        # 3. æå–Bannerå†…å®¹
        print("ğŸ¨ æå–Bannerå†…å®¹...")
        banner_content = extract_banner_text_content(main_content or soup)
        base_data["BannerContent"] = self._clean_html_content(banner_content)
        base_data["NavigationTitle"] = self._extract_navigation_title(soup)

        # 4. æå–æè¿°å†…å®¹
        print("ğŸ“ æå–æè¿°å†…å®¹...")
        base_data["DescriptionContent"] = self._extract_description_content(main_content or soup)

        # 5. æå–FAQå†…å®¹
        print("â“ æå–FAQå†…å®¹...")
        base_data["QaContent"] = extract_qa_content(main_content or soup)

        # 6. æå–å®šä»·è¡¨æ ¼
        print("ğŸ’° æå–å®šä»·è¡¨æ ¼...")
        base_data["PricingTables"] = self._extract_pricing_tables(main_content or soup)

        # 7. æå–å…¶ä»–å…ƒæ•°æ®
        base_data["LastModified"] = self._extract_last_modified(soup)

        return base_data

    def _extract_page_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        # ä¼˜å…ˆæŸ¥æ‰¾é¡µé¢titleæ ‡ç­¾
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text(strip=True)
            if title and len(title) > 0:
                return title
        
        # æŸ¥æ‰¾ä¸»è¦æ ‡é¢˜å…ƒç´ 
        main_heading = soup.find(['h1', 'h2'])
        if main_heading:
            return main_heading.get_text(strip=True)
        
        return ""

    def _extract_meta_description(self, soup: BeautifulSoup) -> str:
        """æå–Metaæè¿°"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')
        return ""

    def _extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """æå–Metaå…³é”®è¯"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            return meta_keywords.get('content', '')
        return ""

    def _extract_ms_service_name(self, soup: BeautifulSoup) -> str:
        """æå–å¾®è½¯æœåŠ¡åç§°"""
        # ä»URLæˆ–é¡µé¢å†…å®¹æ¨æ–­æœåŠ¡åç§°
        if hasattr(self, 'product_config') and 'service_name' in self.product_config:
            return self.product_config['service_name']
        
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­
        if self.html_file_path:
            file_name = Path(self.html_file_path).stem
            if file_name.endswith('-index'):
                return file_name[:-6]  # ç§»é™¤'-index'åç¼€
        
        return ""

    def _extract_slug(self, url: str) -> str:
        """ä»URLæå–slug"""
        if not url:
            return ""
        
        # ä»URLæå–æœ€åä¸€ä¸ªè·¯å¾„æ®µä½œä¸ºslug
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            path_parts = [p for p in parsed.path.split('/') if p]
            if path_parts:
                return path_parts[-1]
        except:
            pass
        
        return ""

    def _extract_navigation_title(self, soup: BeautifulSoup) -> str:
        """æå–å¯¼èˆªæ ‡é¢˜"""
        # æŸ¥æ‰¾å¯¼èˆªç›¸å…³çš„æ ‡é¢˜å…ƒç´ 
        nav_selectors = [
            'nav .title',
            '.navigation-title',
            '.breadcrumb .current',
            '.page-header .title'
        ]
        
        for selector in nav_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return ""

    def _extract_description_content(self, soup: BeautifulSoup) -> str:
        """æå–æè¿°å†…å®¹"""
        # æŸ¥æ‰¾æè¿°å†…å®¹çš„å€™é€‰å…ƒç´ 
        desc_selectors = [
            '.description',
            '.product-description',
            '.intro',
            '.summary',
            'section.overview',
            '.content-section:first-of-type'
        ]
        
        for selector in desc_selectors:
            element = soup.select_one(selector)
            if element:
                return self._clean_html_content(str(element))
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„æè¿°åŒºåŸŸï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªæ®µè½
        first_paragraph = soup.find('p')
        if first_paragraph:
            return self._clean_html_content(str(first_paragraph))
        
        return ""

    def _extract_pricing_tables(self, soup: BeautifulSoup) -> List[str]:
        """æå–å®šä»·è¡¨æ ¼"""
        tables = []
        
        # æŸ¥æ‰¾å®šä»·ç›¸å…³çš„è¡¨æ ¼
        pricing_tables = soup.find_all(['table', 'div'], 
                                     class_=lambda x: x and any(
                                         keyword in x.lower() 
                                         for keyword in ['pricing', 'price', 'cost', 'tier']
                                     ))
        
        for table in pricing_tables:
            clean_table = self._clean_html_content(str(table))
            if clean_table:
                tables.append(clean_table)
        
        return tables

    def _extract_last_modified(self, soup: BeautifulSoup) -> str:
        """æå–æœ€åä¿®æ”¹æ—¶é—´"""
        # æŸ¥æ‰¾æœ€åä¿®æ”¹æ—¶é—´çš„å…ƒæ•°æ®
        modified_selectors = [
            'meta[name="last-modified"]',
            'meta[property="article:modified_time"]',
            '.last-updated',
            '.modified-date'
        ]
        
        for selector in modified_selectors:
            element = soup.select_one(selector)
            if element:
                if element.name == 'meta':
                    return element.get('content', '')
                else:
                    return element.get_text(strip=True)
        
        return ""

    def _clean_html_content(self, content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹"""
        if not content:
            return ""
        
        try:
            return clean_html_content(content)
        except Exception as e:
            print(f"âš  HTMLæ¸…ç†å¤±è´¥: {e}")
            return content

    def _get_default_url(self) -> str:
        """è·å–é»˜è®¤URL"""
        if hasattr(self, 'product_config') and 'default_url' in self.product_config:
            return self.product_config['default_url']
        
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­URL
        if self.html_file_path:
            file_name = Path(self.html_file_path).stem
            if file_name.endswith('-index'):
                service_name = file_name[:-6]
                return f"https://www.azure.cn/pricing/details/{service_name}/"
        
        return ""

    def _validate_extraction_result(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯æå–ç»“æœ
        
        Args:
            data: æå–çš„æ•°æ®
            
        Returns:
            éªŒè¯åçš„æ•°æ®ï¼ŒåŒ…å«validationå­—æ®µ
        """
        print("âœ… éªŒè¯æå–ç»“æœ...")
        
        try:
            validation_result = validate_extracted_data(data, self.product_config)
            data["validation"] = validation_result
        except Exception as e:
            print(f"âš  éªŒè¯å¤±è´¥: {e}")
            data["validation"] = {
                "is_valid": False,
                "errors": [str(e)],
                "warnings": []
            }
        
        # æ·»åŠ æå–å…ƒæ•°æ®
        data["extraction_metadata"] = {
            "extractor_version": "enhanced_v3.0",
            "extraction_timestamp": datetime.now().isoformat(),
            "strategy_used": getattr(self, 'strategy_name', 'unknown'),
            "processing_mode": "strategy_based"
        }
        
        return data

    def _get_product_key(self) -> str:
        """è·å–äº§å“é”®"""
        if hasattr(self, 'product_config') and 'product_key' in self.product_config:
            return self.product_config['product_key']
        
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­
        if self.html_file_path:
            file_name = Path(self.html_file_path).stem
            if file_name.endswith('-index'):
                return file_name[:-6]
        
        return "unknown"