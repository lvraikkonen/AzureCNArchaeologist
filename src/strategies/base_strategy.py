#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºç¡€ç­–ç•¥æŠ½è±¡ç±»
å®šä¹‰æ‰€æœ‰æå–ç­–ç•¥çš„é€šç”¨æ¥å£å’Œå…±ç”¨æ–¹æ³•
"""
import sys
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Dict, Any
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.utils.html.element_creator import create_simple_element
from src.utils.html.cleaner import clean_html_content
from src.utils.media.image_processor import preprocess_image_paths
from src.utils.content.content_utils import (
    extract_qa_content, find_main_content_area, extract_banner_text_content, 
    extract_structured_content
)
from src.utils.data.validation_utils import validate_extracted_data
from src.core.logging import get_logger

logger = get_logger(__name__)


class BaseStrategy(ABC):
    """åŸºç¡€ç­–ç•¥æŠ½è±¡ç±»ï¼Œæ‰€æœ‰æå–ç­–ç•¥çš„åŸºç¡€"""

    def __init__(self, product_config: Dict[str, Any], html_file_path: str = ""):
        """
        åˆå§‹åŒ–åŸºç¡€ç­–ç•¥
        
        Args:
            product_config: äº§å“é…ç½®ä¿¡æ¯
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
        """
        self.product_config = product_config
        self.html_file_path = html_file_path

    @abstractmethod
    def extract(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œå…·ä½“çš„æå–é€»è¾‘
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            æå–çš„CMSå†…å®¹æ•°æ®
        """
        pass

    def _extract_base_content(self, soup: BeautifulSoup, url: str = "") -> Dict[str, Any]:
        """
        æå–æ‰€æœ‰ç­–ç•¥å…±ç”¨çš„åŸºç¡€å†…å®¹
        åŒ…æ‹¬ï¼šTitle, Metaä¿¡æ¯, Banner, Descriptionç­‰é€šç”¨å­—æ®µ
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            
        Returns:
            åŸºç¡€å†…å®¹å­—å…¸
        """
        print("ğŸ” æå–åŸºç¡€å†…å®¹...")

        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = find_main_content_area(soup)
        
        # åˆå§‹åŒ–åŸºç¡€æ•°æ®ç»“æ„
        base_data = {
            "source_file": str(self.html_file_path),
            "source_url": url or self._get_default_url(),
            "extraction_timestamp": datetime.now().isoformat(),
            "Title": "",
            "MetaDescription": "",
            "MetaKeywords": "",
            "MSServiceName": "",
            "Slug": "",
            "BannerContent": "",
            "DescriptionContent": "",
            "Language": "zh-cn",
            "QaContent": "",
            "PricingTables": [],
            "ServiceTiers": [],
            "LastModified": "",
            "RegionalContent": {},
        }

        # 1. æå–æ ‡é¢˜
        logger.info("ğŸ·ï¸ æå–æ ‡é¢˜...")
        base_data["Title"] = self._extract_page_title(soup)
        
        # 2. æå–Metaä¿¡æ¯
        logger.info("ğŸ“‹ æå–Metaä¿¡æ¯...")
        base_data["MetaTitle"] = self._extract_meta_title(soup)
        base_data["MetaDescription"] = self._extract_meta_description(soup)
        base_data["MetaKeywords"] = self._extract_meta_keywords(soup)
        base_data["MSServiceName"] = self._extract_ms_service_name(soup)
        base_data["Slug"] = self._extract_slug(url)

        # 3. æå–Bannerå†…å®¹
        logger.info("ğŸ¨ æå–Bannerå†…å®¹...")
        banner_content = self._extract_banner_content(soup)
        base_data["BannerContent"] = self._clean_html_content(banner_content)

        # 4. æå–æè¿°å†…å®¹
        logger.info("ğŸ“ æå–æè¿°å†…å®¹...")
        base_data["DescriptionContent"] = self._extract_description_content(main_content or soup)

        # 5. æå–FAQå†…å®¹
        logger.info("â“ æå–FAQå†…å®¹...")
        qa_content = self._extract_qa_content(soup)
        base_data["QaContent"] = self._clean_html_content(qa_content)

        # 6. æå–å…¶ä»–å…ƒæ•°æ®
        base_data["LastModified"] = self._extract_last_modified(soup)

        return base_data

    def _extract_page_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        # ä¼˜å…ˆæŸ¥æ‰¾é¡µé¢titleæ ‡ç­¾
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text(strip=True)
            logger.info(f"Get page title: {title}")
            if title and len(title) > 0:
                return title
        #
        # # æŸ¥æ‰¾ä¸»è¦æ ‡é¢˜å…ƒç´ 
        # main_heading = soup.find(['h1', 'h2'])
        # if main_heading:
        #     return main_heading.get_text(strip=True)
        
        return ""

    def _extract_meta_title(self, soup: BeautifulSoup) -> str:
        """æå–Metaæè¿°"""
        meta_title = soup.find('meta', attrs={'name': 'title'})
        if meta_title:
            return meta_title.get('content', '')
        return ""

    def _extract_meta_description(self, soup: BeautifulSoup) -> str:
        """æå–Metaæè¿°"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')
        return ""

    def _extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """æå–Metaå…³é”®è¯"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            return meta_keywords.get('content', '')
        return ""

    def _extract_ms_service_name(self, soup: BeautifulSoup) -> str:
        """æå–å¾®è½¯æœåŠ¡åç§°"""
        """æå–MSServiceNameå­—æ®µï¼Œä»pure-content divå†…çš„tagså…ƒç´ ä¸­çš„ms.serviceå±æ€§"""
        # æŸ¥æ‰¾pure-content div
        pure_content_div = soup.find('div', class_='pure-content')
        if pure_content_div:
            # åœ¨pure-content divå†…æŸ¥æ‰¾tagså…ƒç´ 
            tags_element = pure_content_div.find('tags')
            if tags_element:
                # æå–ms.serviceå±æ€§å€¼
                ms_service = tags_element.get('ms.service', '')
                if ms_service:
                    logger.info(f"  âœ“ æ‰¾åˆ°MSServiceName: {ms_service}")
                    return ms_service
                else:
                    logger.info("  âš  tagså…ƒç´ ä¸­æ²¡æœ‰ms.serviceå±æ€§")
            else:
                logger.info("  âš  pure-content divä¸­æ²¡æœ‰æ‰¾åˆ°tagså…ƒç´ ")
        else:
            logger.info("  âš  æ²¡æœ‰æ‰¾åˆ°pure-content div")

        return ""

    def _extract_slug(self, url: str) -> str:
        """ä»URLæå–slug"""
        """ä»URLæå–slug"""
        if not url:
            return ""

        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            path = parsed.path

            logger.info(f"Extracting slug from url: {url}")

            # æå–/details/ä¹‹ååˆ°/index.htmlä¹‹å‰çš„å†…å®¹ï¼Œç”¨-è¿æ¥
            # ä¾‹å¦‚ /pricing/details/storage/files/index.html -> storage-files
            # ä¾‹å¦‚ /pricing/details/api-management/index.html -> api-management
            if '/details/' in path:
                # æ‰¾åˆ°/details/ä¹‹åçš„éƒ¨åˆ†
                after_details = path.split('/details/')[1]

                # ç§»é™¤/index.htmlåç¼€
                if after_details.endswith('/index.html'):
                    after_details = after_details[:-11]  # ç§»é™¤'/index.html'
                elif after_details.endswith('/'):
                    after_details = after_details[:-1]  # ç§»é™¤æœ«å°¾çš„'/'

                # åˆ†å‰²è·¯å¾„å¹¶ç”¨_è¿æ¥
                path_parts = [p for p in after_details.split('/') if p]
                if path_parts:
                    return '_'.join(path_parts)
        except:
            pass

        return ""

    def _extract_description_content(self, soup: BeautifulSoup) -> str:
        """æå–æè¿°å†…å®¹ - Banneråç¬¬ä¸€ä¸ªpricing-page-sectionçš„å†…å®¹ï¼Œä½†æ’é™¤FAQ"""
        
        # é¦–å…ˆæŸ¥æ‰¾Bannerå…ƒç´ 
        banner = soup.find('div', {'class': ['common-banner', 'col-top-banner']})
        if banner:
            # ä»Banneråé¢æŸ¥æ‰¾ç¬¬ä¸€ä¸ªpricing-page-section
            current = banner
            while current:
                current = current.find_next_sibling()
                if current and current.name == 'div' and 'pricing-page-section' in current.get('class', []):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯FAQå†…å®¹(åŒ…å«more-detailæˆ–æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®)
                    content_text = current.get_text().strip()
                    if ('more-detail' in str(current) or 
                        'æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®' in content_text or
                        'å¸¸è§é—®é¢˜' in content_text):
                        continue  # è·³è¿‡FAQå†…å®¹ï¼ŒæŸ¥æ‰¾ä¸‹ä¸€ä¸ªsection
                    
                    # æ‰¾åˆ°åˆé€‚çš„æè¿°sectionï¼Œè¿”å›æ¸…ç†åçš„å†…å®¹
                    return self._clean_html_content(str(current))
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä¼ ç»Ÿé€‰æ‹©å™¨
        desc_selectors = [
            '.description',
            '.product-description', 
            '.intro',
            '.summary',
            'section.overview'
        ]
        
        for selector in desc_selectors:
            element = soup.select_one(selector)
            if element:
                return self._clean_html_content(str(element))
        
        return ""

    def _extract_last_modified(self, soup: BeautifulSoup) -> str:
        """æå–æœ€åä¿®æ”¹æ—¶é—´"""
        # æŸ¥æ‰¾æœ€åä¿®æ”¹æ—¶é—´çš„å…ƒæ•°æ®
        modified_selectors = [
            'meta[name="last-modified"]',
            'meta[property="article:modified_time"]',
            '.last-updated',
            '.modified-date'
        ]
        
        for selector in modified_selectors:
            element = soup.select_one(selector)
            if element:
                if element.name == 'meta':
                    return element.get('content', '')
                else:
                    return element.get_text(strip=True)
        
        return ""

    def _clean_html_content(self, content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹"""
        if not content:
            return ""
        
        try:
            return clean_html_content(content)
        except Exception as e:
            print(f"âš  HTMLæ¸…ç†å¤±è´¥: {e}")
            return content

    def _get_default_url(self) -> str:
        """è·å–é»˜è®¤URL"""
        if hasattr(self, 'product_config') and 'default_url' in self.product_config:
            return self.product_config['default_url']
        
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­URL
        if self.html_file_path:
            file_name = Path(self.html_file_path).stem
            if file_name.endswith('-index'):
                service_name = file_name[:-6]
                return f"https://www.azure.cn/pricing/details/{service_name}/"
        
        return ""

    def _validate_extraction_result(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯æå–ç»“æœ
        
        Args:
            data: æå–çš„æ•°æ®
            
        Returns:
            éªŒè¯åçš„æ•°æ®ï¼ŒåŒ…å«validationå­—æ®µ
        """
        print("âœ… éªŒè¯æå–ç»“æœ...")
        
        try:
            validation_result = validate_extracted_data(data, self.product_config)
            data["validation"] = validation_result
        except Exception as e:
            print(f"âš  éªŒè¯å¤±è´¥: {e}")
            data["validation"] = {
                "is_valid": False,
                "errors": [str(e)],
                "warnings": []
            }
        
        # æ·»åŠ æå–å…ƒæ•°æ®
        data["extraction_metadata"] = {
            "extractor_version": "enhanced_v3.0",
            "extraction_timestamp": datetime.now().isoformat(),
            "strategy_used": getattr(self, 'strategy_name', 'unknown'),
            "processing_mode": "strategy_based"
        }
        
        return data

    def _get_product_key(self) -> str:
        """è·å–äº§å“é”®"""
        if hasattr(self, 'product_config') and 'product_key' in self.product_config:
            return self.product_config['product_key']
        
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­
        if self.html_file_path:
            file_name = Path(self.html_file_path).stem
            if file_name.endswith('-index'):
                return file_name[:-6]
        
        return "unknown"

    def _extract_banner_content(self, soup: BeautifulSoup) -> str:
        """
        æå–Bannerå†…å®¹
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            Banner HTMLå†…å®¹å­—ç¬¦ä¸²
        """
        try:
            # å¯»æ‰¾å¸¸è§çš„banneré€‰æ‹©å™¨
            banner_selectors = [
                'div.common-banner',
                'div.common-banner-image', 
                '.banner',
                '.hero',
                '.page-banner',
                '.product-banner'
            ]
            
            for selector in banner_selectors:
                banner = soup.select_one(selector)
                if banner:
                    # æ ‡å‡†åŒ–å›¾ç‰‡æ ¼å¼
                    standardized_banner = self._standardize_banner_images(banner)
                    print(f"ğŸ¨ æ‰¾åˆ°Bannerå†…å®¹ï¼Œé€‰æ‹©å™¨: {selector}")
                    return standardized_banner
            
            print("âš  æœªæ‰¾åˆ°Bannerå†…å®¹")
            return ""
            
        except Exception as e:
            print(f"âš  Bannerå†…å®¹æå–å¤±è´¥: {e}")
            return ""

    def _standardize_banner_images(self, banner) -> str:
        """
        æ ‡å‡†åŒ–Bannerä¸­çš„å›¾ç‰‡æ ¼å¼ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹
        
        Args:
            banner: Bannerå…ƒç´ 
            
        Returns:
            æ ‡å‡†åŒ–çš„HTMLå­—ç¬¦ä¸²
        """
        try:
            # åˆ›å»ºbannerçš„å‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹DOM
            import copy
            banner_copy = copy.copy(banner)
            
            # å¤„ç†imgæ ‡ç­¾
            for img in banner_copy.find_all('img'):
                src = img.get('src', '')
                if src:
                    # æ ‡å‡†åŒ–å›¾ç‰‡è·¯å¾„
                    if not src.startswith('http'):
                        img['src'] = f"{{img_hostname}}{src}"
            
            # å¤„ç†styleä¸­çš„background-image
            if banner_copy.get('style'):
                style = banner_copy['style']
                if 'background-image' in style:
                    # æ ‡å‡†åŒ–èƒŒæ™¯å›¾ç‰‡è·¯å¾„
                    import re
                    style = re.sub(r'url\(["\']?([^"\']*)["\']?\)', 
                                 lambda m: f'url("{{{img_hostname}}}{m.group(1)}")' if not m.group(1).startswith('http') else m.group(0), 
                                 style)
                    banner_copy['style'] = style
            
            return str(banner_copy)
            
        except Exception as e:
            print(f"âš  Bannerå›¾ç‰‡æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return str(banner)

    def _extract_qa_content(self, soup: BeautifulSoup) -> str:
        """
        æå–Q&Aå†…å®¹ä»¥åŠæ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®å†…å®¹
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            Q&Aå†…å®¹HTMLå­—ç¬¦ä¸²
        """
        try:
            print("ğŸ” æå–Q&Aå†…å®¹...")
            qa_content = ""
            
            # 1. æŸ¥æ‰¾æ ‡å‡†FAQå®¹å™¨
            faq_containers = [
                soup.find('div', class_='faq'),
                soup.find('div', class_='qa'),
                soup.find('section', class_='faq'),
                soup.find('section', class_='qa')
            ]
            
            for container in faq_containers:
                if container:
                    qa_content += str(container)
                    
            # 2. æŸ¥æ‰¾more-detailå®¹å™¨
            more_detail_containers = soup.find_all('div', class_='more-detail')
            for container in more_detail_containers:
                if container:
                    qa_content += str(container)
                    print(f"âœ“ æ‰¾åˆ°more-detailå®¹å™¨")
            
            # 3. æŸ¥æ‰¾åŒ…å«FAQç»“æ„çš„åˆ—è¡¨
            faq_lists = soup.find_all('ul', class_='faq-list')
            for faq_list in faq_lists:
                if faq_list:
                    qa_content += str(faq_list)
            
            # 4. æŸ¥æ‰¾pricing-page-sectionä¸­çš„æ”¯æŒå’ŒSLAå†…å®¹ï¼ˆæ’é™¤å®šä»·å†…å®¹ï¼‰
            pricing_sections = soup.find_all('div', class_='pricing-page-section')
            for section in pricing_sections:
                section_text = section.get_text().lower()
                # åªæå–æ˜ç¡®çš„æ”¯æŒå’ŒSLAéƒ¨åˆ†ï¼Œæ’é™¤åŒ…å«å®šä»·è¡¨æ ¼çš„éƒ¨åˆ†
                if ('æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®' in section_text or 'sla' in section_text) and not any(price_indicator in section_text for price_indicator in ['ï¿¥', 'ä»·æ ¼', 'æ¯å•ä½', 'å°æ—¶', 'å¼€å‘äººå‘˜åŸºæœ¬æ ‡å‡†']):
                    qa_content += str(section)
                    print(f"âœ“ æ‰¾åˆ°pricing-page-sectionæ”¯æŒ/SLAå†…å®¹")
            
            # 5. æŸ¥æ‰¾accordion-styleçš„FAQ
            accordion_items = soup.find_all(['div', 'section'], class_=['accordion-item', 'faq-item'])
            for item in accordion_items:
                if item:
                    qa_content += str(item)
            
            # 6. å¦‚æœä»¥ä¸Šéƒ½æ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾åŒ…å«ç‰¹å®šFAQé—®é¢˜çš„å…ƒç´ 
            if not qa_content:
                faq_questions = [
                    'å¼€å‘äººå‘˜å±‚çš„ç”¨é€”æ˜¯ä»€ä¹ˆ',
                    'æˆ‘æ˜¯å¦å¯ä»¥åœ¨è‡ªå·±çš„æ•°æ®ä¸­å¿ƒ',
                    'ä»€ä¹ˆæ˜¯"å•ä½"',
                    'ä»€ä¹ˆæ˜¯"ç½‘å…³éƒ¨ç½²"'
                ]
                
                for question in faq_questions:
                    elements = soup.find_all(string=lambda text: text and question in text)
                    for element in elements:
                        # æ‰¾åˆ°åŒ…å«é—®é¢˜çš„æœ€è¿‘çš„å®¹å™¨
                        parent = element.parent
                        while parent and parent.name not in ['div', 'section', 'article']:
                            parent = parent.parent
                        
                        if parent:
                            # æŸ¥æ‰¾çˆ¶çº§çš„pricing-page-sectionæˆ–more-detailå®¹å™¨
                            container = parent
                            for _ in range(5):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5å±‚
                                if container.get('class') and any(cls in ['pricing-page-section', 'more-detail'] 
                                                                 for cls in container.get('class', [])):
                                    qa_content += str(container)
                                    print(f"âœ“ æ‰¾åˆ°FAQé—®é¢˜å®¹å™¨: {question[:20]}...")
                                    break
                                if container.parent:
                                    container = container.parent
                                else:
                                    break
            
            print(f"âœ“ æå–äº† {len(qa_content)} å­—ç¬¦çš„Q&Aå†…å®¹")
            return qa_content
            
        except Exception as e:
            print(f"âš  Q&Aå†…å®¹æå–å¤±è´¥: {e}")
            return ""