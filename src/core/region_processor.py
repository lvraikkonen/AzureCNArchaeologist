#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒºåŸŸå¤„ç†å™¨
ä»BaseCMSExtractorä¸­æå–çš„åŒºåŸŸå¤„ç†é€»è¾‘ï¼Œå»é™¤å¤æ‚çš„ç»§æ‰¿å…³ç³»
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from bs4 import BeautifulSoup, Tag


class RegionProcessor:
    """åŒºåŸŸå¤„ç†é€»è¾‘ï¼Œä»BaseCMSExtractorä¸­æå–"""

    def __init__(self, config_file: str = "data/configs/soft-category.json"):
        self.config_file = config_file
        self.region_config = self._load_region_config()
        print(f"âœ“ åŒºåŸŸå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ åŒºåŸŸé…ç½®æ–‡ä»¶: {config_file}")

    def _load_region_config(self) -> Dict[str, Any]:
        """åŠ è½½åŒºåŸŸé…ç½®æ–‡ä»¶"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    raw_config = json.load(f)

                # å¦‚æœé…ç½®æ˜¯æ•°ç»„æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                if isinstance(raw_config, list):
                    config = self._convert_array_config_to_dict(raw_config)
                    print(f"ğŸ“‹ åŠ è½½åŒºåŸŸé…ç½®: {len(raw_config)} ä¸ªé…ç½®é¡¹ï¼Œè½¬æ¢ä¸º {len(config)} ä¸ªäº§å“")
                else:
                    config = raw_config
                    print(f"ğŸ“‹ åŠ è½½åŒºåŸŸé…ç½®: {len(config)} ä¸ªäº§å“")

                return config
            except Exception as e:
                print(f"âš  åŠ è½½åŒºåŸŸé…ç½®å¤±è´¥: {e}")
                return {}
        else:
            print(f"âš  åŒºåŸŸé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file}")
            return {}

    def _convert_array_config_to_dict(self, array_config: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å°†æ•°ç»„æ ¼å¼çš„é…ç½®è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        dict_config = {}

        for item in array_config:
            if not isinstance(item, dict):
                continue

            os_name = item.get('os', '')
            region = item.get('region', '')
            table_ids = item.get('tableIDs', [])

            if not os_name or not region:
                continue

            # æ ‡å‡†åŒ–äº§å“åç§°ï¼ˆè½¬æ¢ä¸ºæ–‡ä»¶åæ ¼å¼ï¼‰
            product_key = self._normalize_product_name(os_name)

            if product_key not in dict_config:
                dict_config[product_key] = {}

            dict_config[product_key][region] = table_ids

        return dict_config

    def _normalize_product_name(self, os_name: str) -> str:
        """æ ‡å‡†åŒ–äº§å“åç§°ä¸ºæ–‡ä»¶åæ ¼å¼"""
        # äº§å“åç§°æ˜ å°„è¡¨
        name_mapping = {
            'API Management': 'api-management-index',
            'Azure Database for MySQL': 'mysql-index',
            'Azure Cosmos DB': 'cosmos-db-index',
            'Storage Files': 'storage-files-index',
            'Data Factory SSIS': 'ssis-index',
            'Power BI Embedded': 'power-bi-embedded-index',
            'Cognitive Services': 'cognitive-services-index',
            'Anomaly Detector': 'anomaly-detector-index',
            'Machine Learning Server': 'machine-learning-server-index',
            'Azure_Data_Lake_Storage_Gen': 'storage_data-lake_index',
            'databricks': 'databricks-index'
        }

        return name_mapping.get(os_name, os_name.lower().replace(' ', '-'))

    def detect_available_regions(self, soup: BeautifulSoup) -> List[str]:
        """åŠ¨æ€æ£€æµ‹HTMLä¸­å®é™…å­˜åœ¨çš„åŒºåŸŸ"""
        print("ğŸ” æ£€æµ‹å¯ç”¨åŒºåŸŸ...")
        
        detected_regions = set()
        
        # æ–¹æ³•1: æ£€æŸ¥region-containerç±»
        region_containers = soup.find_all(class_='region-container')
        for container in region_containers:
            region_id = container.get('id')
            if region_id:
                detected_regions.add(region_id)
        
        # æ–¹æ³•2: æ£€æŸ¥data-regionå±æ€§
        region_elements = soup.find_all(attrs={'data-region': True})
        for element in region_elements:
            region_id = element.get('data-region')
            if region_id:
                detected_regions.add(region_id)
        
        # æ–¹æ³•3: æ£€æŸ¥å¸¸è§çš„åŒºåŸŸIDæ¨¡å¼
        common_region_patterns = [
            'china-north', 'china-east',
            'china-north2', 'china-east2',
            'china-north3', 'china-east3',
        ]
        
        for pattern in common_region_patterns:
            elements = soup.find_all(id=lambda x: x and pattern in x.lower())
            if elements:
                detected_regions.add(pattern)
        
        # æ–¹æ³•4: æ£€æŸ¥selecté€‰é¡¹ä¸­çš„åŒºåŸŸ
        region_selects = soup.find_all('select')
        for select in region_selects:
            select_id = select.get('id', '').lower()
            if 'region' in select_id or 'location' in select_id:
                options = select.find_all('option')
                for option in options:
                    value = option.get('value')
                    if value and len(value) > 2:  # è¿‡æ»¤ç©ºå€¼å’Œè¿‡çŸ­çš„å€¼
                        detected_regions.add(value)
        
        detected_list = sorted(list(detected_regions))
        print(f"  âœ“ æ£€æµ‹åˆ° {len(detected_list)} ä¸ªåŒºåŸŸ: {detected_list}")
        
        return detected_list

    def extract_region_contents(self, soup: BeautifulSoup, html_file_path: str) -> Dict[str, Any]:
        """æå–å„åŒºåŸŸçš„å†…å®¹ - ä¿æŒå®Œæ•´HTMLæ ¼å¼"""
        print("ğŸŒ æå–åŒºåŸŸå†…å®¹...")
        
        region_contents = {}
        
        # è·å–æ–‡ä»¶åç”¨äºåŒºåŸŸé…ç½®æŸ¥è¯¢
        filename = Path(html_file_path).stem
        
        # æ£€æµ‹å¯ç”¨åŒºåŸŸ
        available_regions = self.detect_available_regions(soup)
        
        if not available_regions:
            print("  â„¹ æœªæ£€æµ‹åˆ°å…·ä½“åŒºåŸŸï¼Œä½¿ç”¨å…¨å±€å†…å®¹")
            region_contents['global'] = self._extract_global_content(soup)
            return region_contents
        
        # ä¸ºæ¯ä¸ªåŒºåŸŸæå–å†…å®¹
        for region_id in available_regions:
            print(f"  ğŸ“ å¤„ç†åŒºåŸŸ: {region_id}")
            
            try:
                # åº”ç”¨åŒºåŸŸç­›é€‰
                region_soup = self.apply_region_filtering(soup, region_id, filename)
                
                # æå–å®Œæ•´çš„HTMLå†…å®¹è€Œä¸æ˜¯åˆ†è§£çš„ç»“æ„
                region_html = self._extract_region_html_content(region_soup, region_id)
                
                region_contents[region_id] = region_html
                
            except Exception as e:
                print(f"  âš  åŒºåŸŸ {region_id} å†…å®¹æå–å¤±è´¥: {e}")
                continue
        
        print(f"  âœ“ æˆåŠŸæå– {len(region_contents)} ä¸ªåŒºåŸŸçš„å†…å®¹")
        return region_contents

    def apply_region_filtering(self, soup: BeautifulSoup, region_id: str, 
                             filename: str = "") -> BeautifulSoup:
        """åº”ç”¨åŒºåŸŸç­›é€‰åˆ°soupå¯¹è±¡"""
        print(f"ğŸ”§ åº”ç”¨åŒºåŸŸç­›é€‰: {region_id}")
        
        # åˆ›å»ºsoupçš„å‰¯æœ¬
        filtered_soup = BeautifulSoup(str(soup), 'html.parser')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥äº§å“çš„åŒºåŸŸé…ç½®
        product_config = self.region_config.get(filename, {})
        
        if not product_config:
            print(f"  â„¹ äº§å“ {filename} æ— åŒºåŸŸé…ç½®ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹")
            return filtered_soup
        
        region_tables = product_config.get(region_id, [])
        
        if not region_tables:
            print(f"  â„¹ åŒºåŸŸ {region_id} æ— ç‰¹å®šè¡¨æ ¼é…ç½®ï¼Œä¿ç•™æ‰€æœ‰è¡¨æ ¼")
            return filtered_soup
        
        # ç§»é™¤æŒ‡å®šçš„è¡¨æ ¼
        tables_removed = 0
        removed_table_ids = []

        for table_id in region_tables:
            # å¤„ç†å¸¦#å·å’Œä¸å¸¦#å·çš„table_id
            clean_table_id = table_id.replace('#', '') if table_id.startswith('#') else table_id

            # æŸ¥æ‰¾å…ƒç´ ï¼ˆå…ˆå°è¯•å¸¦#çš„IDï¼Œå†å°è¯•ä¸å¸¦#çš„ï¼‰
            elements = filtered_soup.find_all(id=clean_table_id)
            if not elements and not table_id.startswith('#'):
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾å¸¦#å‰ç¼€çš„
                elements = filtered_soup.find_all(id=f"#{table_id}")

            for element in elements:
                # ç§»é™¤è¡¨æ ¼åŠå…¶ç›¸å…³çš„å‰ç½®å†…å®¹
                self._remove_table_with_related_content(element, clean_table_id)
                tables_removed += 1
                removed_table_ids.append(table_id)

        if tables_removed > 0:
            print(f"  âœ“ ç§»é™¤äº† {tables_removed} ä¸ªåŒºåŸŸç‰¹å®šè¡¨æ ¼: {removed_table_ids}")

        # åœ¨filtered_soupä¸­æ·»åŠ ä¸€ä¸ªéšè—çš„å…ƒæ•°æ®æ ‡ç­¾ï¼Œè®°å½•è¢«ç§»é™¤çš„table IDs
        if removed_table_ids:
            metadata_comment = filtered_soup.new_string(f"<!-- Removed table IDs for region {region_id}: {', '.join(removed_table_ids)} -->")
            if filtered_soup.body:
                filtered_soup.body.insert(0, metadata_comment)

        return filtered_soup

    def _analyze_pricing_section_structure(self, pricing_section):
        """åˆ†æpricing-page-sectionçš„ç»“æ„ï¼Œè¯†åˆ«å†…å®¹å—"""
        content_blocks = []
        current_block = None
        
        for element in pricing_section.children:
            if hasattr(element, 'name'):
                if element.name == 'h2':
                    # æ–°çš„æ ‡é¢˜å¼€å§‹æ–°çš„å†…å®¹å—
                    if current_block:
                        content_blocks.append(current_block)
                    current_block = {
                        'type': 'section',
                        'title': element,
                        'title_text': element.get_text(strip=True),
                        'elements': [element]
                    }
                elif current_block:
                    # å°†å…ƒç´ å½’å±åˆ°å½“å‰å†…å®¹å—
                    current_block['elements'].append(element)
                    
                    # è¯†åˆ«å…ƒç´ ç±»å‹
                    if element.name == 'table':
                        current_block['has_table'] = True
                        current_block['table_id'] = element.get('id')
                    elif element.name == 'div' and 'tags-date' in element.get('class', []):
                        current_block['has_tags_date'] = True
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_block:
            content_blocks.append(current_block)
            
        return content_blocks
    
    def _classify_content_relation(self, element, table_id: str):
        """åˆ†ç±»å†…å®¹ä¸è¡¨æ ¼çš„å…³ç³»"""
        if not hasattr(element, 'name'):
            return 'unrelated'
            
        # å¦‚æœæ˜¯è¡¨æ ¼æœ¬èº«
        if element.name == 'table' and element.get('id') == table_id.replace('#', ''):
            return 'table'
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡è¦çš„äº§å“æ ‡é¢˜ï¼ˆå…¨å±€ä¿æŠ¤ï¼‰
        if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            title_text = element.get_text(strip=True)
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„æ ‡é¢˜è¯†åˆ«ï¼Œè€Œéç¡¬ç¼–ç åˆ—è¡¨
            if self._is_global_product_title(title_text):
                return 'global_title'
            elif self._is_section_title(title_text):
                return 'section_title'
                
        # æ£€æŸ¥tags-dateçš„ç±»å‹
        if element.name == 'div' and 'tags-date' in element.get('class', []):
            return self._classify_tags_date(element)
            
        # å…¶ä»–å…ƒç´ 
        return 'content'
    
    def _is_global_product_title(self, title_text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å…¨å±€äº§å“æ ‡é¢˜ï¼ˆåº”ä¿æŠ¤ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»è¦äº§å“/æœåŠ¡åç§°
        global_patterns = [
            r'^API\s*ç®¡ç†$',
            r'^API\s*Management$', 
            r'^Azure\s+Database',
            r'^Cosmos\s*DB$',
            r'^MySQL$',
            r'^PostgreSQL$'
        ]
        
        import re
        for pattern in global_patterns:
            if re.match(pattern, title_text, re.IGNORECASE):
                return True
        return False
    
    def _is_section_title(self, title_text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯åŠŸèƒ½åŒºæ®µæ ‡é¢˜"""
        section_keywords = ['Gateway', 'ç½‘å…³', 'å®šä»·', 'Pricing', 'åŠŸèƒ½', 'Features']
        return any(keyword in title_text for keyword in section_keywords)
    
    def _classify_tags_date(self, tags_date_element) -> str:
        """åˆ†ç±»tags-dateå…ƒç´ çš„ç±»å‹"""
        text = tags_date_element.get_text(strip=True)
        
        # å…¨å±€ä»·æ ¼è¯´æ˜ï¼ˆåº”ä¿æŠ¤ï¼‰
        global_pricing_patterns = [
            '*ä»¥ä¸‹ä»·æ ¼å‡ä¸ºå«ç¨ä»·æ ¼',
            '*æ¯æœˆä»·æ ¼ä¼°ç®—åŸºäº',
            'prices are tax-inclusive',
            'monthly price estimates'
        ]
        
        for pattern in global_pricing_patterns:
            if pattern in text:
                return 'global_pricing_note'
        
        # è¡¨æ ¼æ³¨é‡Šè¯´æ˜ï¼ˆåº”ä¿ç•™ï¼‰- åŒ…å«è„šæ³¨ç¼–å·çš„è¯´æ˜
        if self._contains_footnote_references(text):
            return 'table_footnote_note'
                
        # å…¶ä»–è¡¨æ ¼ç‰¹å®šçš„è¯´æ˜ï¼ˆå¯èƒ½éœ€è¦ç§»é™¤ï¼‰
        return 'table_specific_note'
    
    def _contains_footnote_references(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«è„šæ³¨å¼•ç”¨ï¼ˆsupæ ‡ç­¾å†…å®¹ï¼‰"""
        import re
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç±»ä¼¼ "1 è¦æ±‚åœ¨ä¸¤ä¸ªæˆ–æ›´å¤šåŒºåŸŸ" æˆ– "2 ååé‡æ•°æ®ä»…ä¾›å‚è€ƒ" çš„æ¨¡å¼
        footnote_patterns = [
            r'^\s*\d+\s*[\u4e00-\u9fff]',  # æ•°å­—å¼€å¤´åè·Ÿä¸­æ–‡
            r'sup>\s*\d+\s*</sup',  # supæ ‡ç­¾åŒ…å«æ•°å­—
            r'è¦æ±‚åœ¨.*åŒºåŸŸ.*éƒ¨ç½²',  # åŒºåŸŸéƒ¨ç½²è¦æ±‚
            r'ååé‡æ•°æ®.*å‚è€ƒ',  # ååé‡è¯´æ˜
            r'å¼€å‘è€…å±‚.*ä»˜è´¹',  # å¼€å‘è€…å±‚è¯´æ˜
            r'é«˜çº§å±‚.*ä»˜è´¹',  # é«˜çº§å±‚è¯´æ˜
            r'ä»…é€‚ç”¨äº.*ç½‘å…³',  # ç½‘å…³ç›¸å…³è¯´æ˜
            r'è¯·ä½¿ç”¨.*ç¼“å­˜',  # ç¼“å­˜è¯´æ˜
        ]
        
        for pattern in footnote_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False
    
    def _remove_table_with_related_content(self, table_element, table_id: str):
        """ç²¾ç¡®ç§»é™¤è¡¨æ ¼åŠå…¶ç›´æ¥å…³è”çš„å†…å®¹ï¼Œä¿æŠ¤å…¨å±€å†…å®¹"""
        print(f"    ğŸ—‘ï¸ ç²¾ç¡®ç§»é™¤è¡¨æ ¼: {table_id}")
        
        # åˆ†ææ‰€åœ¨çš„pricing-page-sectionç»“æ„
        pricing_section = self._find_parent_pricing_section(table_element)
        if not pricing_section:
            # å›é€€åˆ°åŸæœ‰é€»è¾‘
            print(f"      âš  æœªæ‰¾åˆ°pricing-page-sectionï¼Œä½¿ç”¨å›é€€é€»è¾‘")
            self._remove_table_fallback(table_element, table_id)
            return
            
        # åˆ†æç»“æ„å¹¶ç²¾ç¡®ç§»é™¤
        content_blocks = self._analyze_pricing_section_structure(pricing_section)
        
        elements_to_remove = []
        
        # æ‰¾åˆ°åŒ…å«æ­¤è¡¨æ ¼çš„å†…å®¹å—
        target_block = None
        for block in content_blocks:
            if block.get('table_id') == table_id.replace('#', ''):
                target_block = block
                break
        
        if target_block:
            print(f"      ğŸ“ æ‰¾åˆ°è¡¨æ ¼æ‰€åœ¨å†…å®¹å—: {target_block['title_text']}")
            
            for element in target_block['elements']:
                relation = self._classify_content_relation(element, table_id)
                
                if relation == 'table':
                    elements_to_remove.append(element)
                    print(f"      ğŸ—‘ï¸ ç§»é™¤è¡¨æ ¼: {table_id}")
                elif relation == 'table_specific_note':
                    elements_to_remove.append(element)
                    print(f"      ğŸ—‘ï¸ ç§»é™¤è¡¨æ ¼ä¸“å±è¯´æ˜: {element.get_text(strip=True)[:50]}")
                elif relation == 'table_footnote_note':
                    print(f"      ğŸ›¡ï¸ ä¿æŠ¤è¡¨æ ¼è„šæ³¨è¯´æ˜: {element.get_text(strip=True)[:50]}")
                elif relation == 'global_title':
                    print(f"      ğŸ›¡ï¸ ä¿æŠ¤å…¨å±€æ ‡é¢˜: {element.get_text(strip=True)[:50]}")
                elif relation == 'global_pricing_note':
                    print(f"      ğŸ›¡ï¸ ä¿æŠ¤å…¨å±€ä»·æ ¼è¯´æ˜: {element.get_text(strip=True)[:50]}")
                elif relation == 'section_title':
                    print(f"      ğŸ›¡ï¸ ä¿æŠ¤åŒºæ®µæ ‡é¢˜: {element.get_text(strip=True)[:50]}")
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–çš„å—ï¼Œç›´æ¥ç§»é™¤è¡¨æ ¼
            elements_to_remove.append(table_element)
            print(f"      âš  æœªæ‰¾åˆ°ç»“æ„åŒ–å—ï¼Œä»…ç§»é™¤è¡¨æ ¼æœ¬èº«")
        
        # ç§»é™¤æ”¶é›†åˆ°çš„å…ƒç´ 
        for element in elements_to_remove:
            try:
                element.decompose()
            except Exception as e:
                print(f"      âš  ç§»é™¤å…ƒç´ å¤±è´¥: {e}")
    
    def _find_parent_pricing_section(self, element):
        """æŸ¥æ‰¾å…ƒç´ æ‰€åœ¨çš„pricing-page-sectionçˆ¶èŠ‚ç‚¹"""
        current = element.parent
        while current:
            if (hasattr(current, 'get') and current.get('class') and 
                'pricing-page-section' in current.get('class')):
                return current
            current = current.parent
        return None
    
    def _remove_table_fallback(self, table_element, table_id: str):
        """å›é€€çš„è¡¨æ ¼ç§»é™¤é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆåŸé€»è¾‘ï¼‰"""
        print(f"    ğŸ”„ ä½¿ç”¨å›é€€ç§»é™¤é€»è¾‘: {table_id}")
        # åªç§»é™¤è¡¨æ ¼æœ¬èº«ï¼Œä¸ç§»é™¤å…¶ä»–å†…å®¹
        try:
            table_element.decompose()
        except Exception as e:
            print(f"      âš  è¡¨æ ¼ç§»é™¤å¤±è´¥: {e}")

    def _extract_global_content(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """æå–å…¨å±€å†…å®¹ï¼ˆæ— åŒºåŸŸåŒºåˆ†ï¼‰"""
        return {
            'type': 'global',
            'pricing_tables': self._extract_pricing_tables_simple(soup),
            'content_summary': self._get_content_summary(soup)
        }

    def _extract_region_html_content(self, soup: BeautifulSoup, region_id: str) -> str:
        """æå–åŒºåŸŸçš„å®Œæ•´HTMLå†…å®¹ - é’ˆå¯¹pricing-detail-tabç»“æ„ä¼˜åŒ–"""
        print(f"    ğŸ“„ æå–åŒºåŸŸ {region_id} çš„å®Œæ•´HTMLå†…å®¹")
        
        # æ„å»ºHTMLç»“æ„ï¼ŒåŒ¹é…åŸå§‹tab-contentæ ¼å¼
        html_parts = []
        html_parts.append('<div class="tab-content">')
        html_parts.append('<div class="tab-panel" id="tabContent1">')
        
        # æŸ¥æ‰¾pricing-detail-tabç»“æ„ä¸­çš„ä¸»è¦å†…å®¹
        pricing_detail_tab = soup.find(class_='technical-azure-selector pricing-detail-tab')
        content_extracted = False
        
        if pricing_detail_tab:
            print(f"    ğŸ¯ å‘ç°pricing-detail-tabç»“æ„ï¼Œæå–å®Œæ•´å†…å®¹")
            # åœ¨pricing-detail-tabä¸­æŸ¥æ‰¾tab-content
            tab_content = pricing_detail_tab.find(class_='tab-content')
            if tab_content:
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªtab-panelä¸­çš„pricing-page-section
                tab_panel = tab_content.find('div', {'id': 'tabContent1'}) or tab_content.find(class_='tab-panel')
                if tab_panel:
                    pricing_section = tab_panel.find(class_='pricing-page-section')
                    if pricing_section:
                        # éªŒè¯æ˜¯å¦åŒ…å«å…³é”®å…ƒç´ 
                        has_h2 = pricing_section.find('h2') is not None
                        has_tags_date = pricing_section.find(class_='tags-date') is not None
                        
                        print(f"    ğŸ“‹ å†…å®¹éªŒè¯: H2={has_h2}, tags-date={has_tags_date}")
                        
                        # æå–å®Œæ•´çš„pricing-page-sectionå†…å®¹
                        section_html = self._preserve_important_content(str(pricing_section))
                        html_parts.append(section_html)
                        content_extracted = True
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°pricing-detail-tabç»“æ„ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
        if not content_extracted:
            print(f"    ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ¡ˆï¼šæŸ¥æ‰¾å…¨å±€tab-content")
            tab_content_containers = soup.find_all(class_='tab-content')
            
            for tab_content in tab_content_containers:
                pricing_sections = tab_content.find_all(class_='pricing-page-section')
                for section in pricing_sections:
                    # è·³è¿‡åŒ…å«more-detailçš„sectionï¼ˆFAQå†…å®¹ï¼‰
                    if section.find(class_='more-detail'):
                        continue
                    
                    # éªŒè¯å¹¶æå–å†…å®¹
                    has_h2 = section.find('h2') is not None
                    has_tags_date = section.find(class_='tags-date') is not None
                    print(f"    ğŸ“‹ å›é€€å†…å®¹éªŒè¯: H2={has_h2}, tags-date={has_tags_date}")
                    
                    section_html = self._preserve_important_content(str(section))
                    html_parts.append(section_html)
                    content_extracted = True
                    break
                
                if content_extracted:
                    break
        
        # å¦‚æœä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨æœ€åçš„å›é€€æ–¹æ¡ˆ
        if not content_extracted:
            print(f"    ğŸš¨ ä½¿ç”¨æœ€ç»ˆå›é€€æ–¹æ¡ˆï¼šæŸ¥æ‰¾ä»»æ„pricing-page-section")
            pricing_sections = soup.find_all(class_='pricing-page-section')
            for section in pricing_sections:
                if section.find(class_='more-detail'):
                    continue
                section_html = self._preserve_important_content(str(section))
                html_parts.append(section_html)
                content_extracted = True
                break
        
        # æ·»åŠ tabæ§åˆ¶ç»“æ„
        html_parts.append('<div class="technical-azure-selector tab-control-selector" style="min-height: 400px;">')
        html_parts.append('<div class="tab-control-container tab-active" id="tabContent1">')
        html_parts.append('<!-- Content extracted from tab-content pricing-page-section -->')
        html_parts.append('</div>')  # tab-control-container
        html_parts.append('</div>')  # technical-azure-selector
        html_parts.append('</div>')  # tab-panel
        html_parts.append('</div>')  # tab-content
        
        # ç»„åˆå¹¶æ¸…ç†HTML
        result_html = ''.join(html_parts)
        result_html = self._clean_html_content(result_html)
        
        print(f"    âœ“ æ„å»ºåŒºåŸŸHTMLå†…å®¹ï¼Œé•¿åº¦: {len(result_html)} å­—ç¬¦")
        return result_html

    def _preserve_important_content(self, content: str) -> str:
        """ä¿ç•™é‡è¦å†…å®¹çš„HTMLå¤„ç† - ç¡®ä¿H2å’Œtags-dateä¸è¢«è¯¯åˆ """
        if not content:
            return ""
        
        # è½»åº¦æ¸…ç†ï¼Œä½†ä¿ç•™é‡è¦ç»“æ„
        import re
        # åªç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦
        content = re.sub(r'\n+', ' ', content)
        content = re.sub(r'\t+', ' ', content)
        # ç§»é™¤è¿‡å¤šçš„è¿ç»­ç©ºæ ¼ï¼Œä½†ä¿ç•™åŸºæœ¬ç©ºæ ¼
        content = re.sub(r'  +', ' ', content)
        # æ¸…ç†é¦–å°¾ç©ºæ ¼
        content = content.strip()
        
        return content

    def _clean_html_content(self, content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹ï¼Œç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼"""
        if not content:
            return ""
        
        import re
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦
        content = re.sub(r'\n+', ' ', content)
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼ï¼ˆä¿ç•™å•ä¸ªç©ºæ ¼ï¼‰
        content = re.sub(r'\s+', ' ', content)
        # ç§»é™¤æ ‡ç­¾ä¹‹é—´çš„å¤šä½™ç©ºæ ¼
        content = re.sub(r'>\s+<', '><', content)
        # æ¸…ç†é¦–å°¾ç©ºæ ¼
        content = content.strip()
        
        return content

    def _extract_region_pricing_tables(self, soup: BeautifulSoup, region_id: str) -> List[Dict[str, Any]]:
        """æå–åŒºåŸŸç‰¹å®šçš„å®šä»·è¡¨æ ¼"""
        pricing_tables = []
        
        # æŸ¥æ‰¾å®šä»·ç›¸å…³çš„è¡¨æ ¼
        tables = soup.find_all('table')
        
        for i, table in enumerate(tables):
            table_text = table.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå®šä»·è¡¨æ ¼
            if self._is_pricing_table(table_text):
                table_info = {
                    'table_id': f"table_{region_id}_{i}",
                    'region': region_id,
                    'content': table_text[:1000],  # é™åˆ¶å†…å®¹é•¿åº¦
                    'row_count': len(table.find_all('tr')),
                    'has_pricing': any(keyword in table_text.lower() 
                                     for keyword in ['ï¿¥', '$', 'ä»·æ ¼', 'price', 'è´¹ç”¨'])
                }
                
                pricing_tables.append(table_info)
        
        return pricing_tables

    def _extract_region_features(self, soup: BeautifulSoup, region_id: str) -> List[str]:
        """æå–åŒºåŸŸç‰¹å®šçš„åŠŸèƒ½å¯ç”¨æ€§"""
        features = []
        
        # æŸ¥æ‰¾åŠŸèƒ½åˆ—è¡¨
        feature_lists = soup.find_all(['ul', 'ol'])
        
        for ul in feature_lists:
            # æ£€æŸ¥æ˜¯å¦ä¸ºåŠŸèƒ½åˆ—è¡¨
            ul_text = ul.get_text(strip=True).lower()
            if any(keyword in ul_text for keyword in ['åŠŸèƒ½', 'feature', 'ç‰¹æ€§', 'æ”¯æŒ']):
                items = ul.find_all('li')
                for item in items[:10]:  # é™åˆ¶æ•°é‡
                    item_text = item.get_text(strip=True)
                    if len(item_text) > 5:  # è¿‡æ»¤è¿‡çŸ­çš„å†…å®¹
                        features.append(item_text)
        
        return features

    def _extract_region_notes(self, soup: BeautifulSoup, region_id: str) -> List[str]:
        """æå–åŒºåŸŸç‰¹å®šçš„è¯´æ˜ä¿¡æ¯"""
        notes = []
        
        # æŸ¥æ‰¾åŒ…å«åŒºåŸŸä¿¡æ¯çš„æ®µè½
        paragraphs = soup.find_all('p')
        
        for p in paragraphs:
            p_text = p.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«åŒºåŸŸç›¸å…³ä¿¡æ¯
            if any(keyword in p_text.lower() for keyword in 
                  ['åŒºåŸŸ', 'region', 'åœ°åŒº', 'å¯ç”¨æ€§', 'availability']):
                if len(p_text) > 20:  # è¿‡æ»¤è¿‡çŸ­çš„å†…å®¹
                    notes.append(p_text[:200])  # é™åˆ¶é•¿åº¦
        
        return notes

    def _is_pricing_table(self, table_text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå®šä»·è¡¨æ ¼"""
        pricing_keywords = [
            'ä»·æ ¼', 'price', 'å®šä»·', 'pricing', 'è´¹ç”¨', 'cost', 
            'ï¿¥', '$', 'å…ƒ', 'ç¾å…ƒ', 'usd', 'rmb', 'cny'
        ]
        
        text_lower = table_text.lower()
        return any(keyword in text_lower for keyword in pricing_keywords)

    def _extract_pricing_tables_simple(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ç®€å•çš„å®šä»·è¡¨æ ¼æå–"""
        pricing_tables = []
        tables = soup.find_all('table')
        
        for i, table in enumerate(tables):
            table_text = table.get_text(strip=True)
            
            if self._is_pricing_table(table_text):
                pricing_tables.append({
                    'table_id': f"global_table_{i}",
                    'content': table_text[:500],  # é™åˆ¶å†…å®¹é•¿åº¦
                    'row_count': len(table.find_all('tr'))
                })
        
        return pricing_tables

    def _get_content_summary(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """è·å–å†…å®¹æ‘˜è¦"""
        return {
            'total_tables': len(soup.find_all('table')),
            'total_lists': len(soup.find_all(['ul', 'ol'])),
            'total_headings': len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])),
            'has_forms': len(soup.find_all('form')) > 0,
            'has_scripts': len(soup.find_all('script')) > 0
        }

    def get_region_mapping(self) -> Dict[str, str]:
        """è·å–åŒºåŸŸæ˜ å°„å…³ç³»"""
        # æ ‡å‡†çš„Azure ChinaåŒºåŸŸæ˜ å°„
        return {
            'north-china': 'ä¸­å›½åŒ—éƒ¨',
            'esat-china': 'ä¸­å›½ä¸œéƒ¨', 
            'north-china2': 'ä¸­å›½åŒ—éƒ¨ 2',
            'esat-china2': 'ä¸­å›½ä¸œéƒ¨ 2',
            'north-china3': 'ä¸­å›½åŒ—éƒ¨ 3',
            'esat-china3': 'ä¸­å›½ä¸œéƒ¨ 3', 
        }

    def normalize_region_id(self, region_id: str) -> str:
        """æ ‡å‡†åŒ–åŒºåŸŸID"""
        # è½¬æ¢ä¸ºå°å†™å¹¶æ›¿æ¢å¸¸è§å˜ä½“
        normalized = region_id.lower().strip()
        
        # å¤„ç†å¸¸è§çš„åŒºåŸŸåç§°å˜ä½“
        region_variants = {
            'cn-north': 'china-north',
            'cn-east': 'china-east', 
            'cn-south': 'china-south',
            'åŒ—äº¬': 'beijing',
            'ä¸Šæµ·': 'shanghai',
            'å¹¿å·': 'guangzhou',
            'æ·±åœ³': 'shenzhen'
        }
        
        return region_variants.get(normalized, normalized)

    def validate_region_config(self) -> Dict[str, Any]:
        """éªŒè¯åŒºåŸŸé…ç½®çš„å®Œæ•´æ€§"""
        validation_result = {
            'is_valid': True,
            'total_products': len(self.region_config),
            'total_regions': set(),
            'issues': []
        }
        
        for product, regions in self.region_config.items():
            if not isinstance(regions, dict):
                validation_result['issues'].append(f"äº§å“ {product} çš„é…ç½®ä¸æ˜¯å­—å…¸æ ¼å¼")
                validation_result['is_valid'] = False
                continue
                
            for region_id, table_ids in regions.items():
                validation_result['total_regions'].add(region_id)
                
                if not isinstance(table_ids, list):
                    validation_result['issues'].append(
                        f"äº§å“ {product} åŒºåŸŸ {region_id} çš„è¡¨æ ¼IDä¸æ˜¯åˆ—è¡¨æ ¼å¼"
                    )
                    validation_result['is_valid'] = False
        
        validation_result['total_regions'] = len(validation_result['total_regions'])
        
        return validation_result

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–åŒºåŸŸå¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_products_configured': len(self.region_config),
            'regions_by_product': {},
            'most_common_regions': {},
            'total_table_exclusions': 0
        }
        
        all_regions = []
        
        # æ£€æŸ¥region_configæ˜¯å¦ä¸ºå­—å…¸æ ¼å¼
        if not isinstance(self.region_config, dict):
            return stats
            
        for product, regions in self.region_config.items():
            if isinstance(regions, dict):
                product_regions = list(regions.keys())
                stats['regions_by_product'][product] = len(product_regions)
                all_regions.extend(product_regions)
                
                # ç»Ÿè®¡è¡¨æ ¼æ’é™¤æ•°é‡
                for table_list in regions.values():
                    if isinstance(table_list, list):
                        stats['total_table_exclusions'] += len(table_list)
            else:
                # å¦‚æœregionsä¸æ˜¯å­—å…¸ï¼Œè·³è¿‡
                stats['regions_by_product'][product] = 0
        
        # ç»Ÿè®¡æœ€å¸¸è§çš„åŒºåŸŸ
        from collections import Counter
        region_counts = Counter(all_regions)
        stats['most_common_regions'] = dict(region_counts.most_common(10))
        
        return stats