#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†…å®¹å¤„ç†å·¥å…·å‡½æ•°
å¤„ç†ä¸»è¦å†…å®¹åŒºåŸŸæå–ã€sectionæ ‡é¢˜è¯†åˆ«ã€bannerå¤„ç†ç­‰
"""

import re
from typing import Optional, List, Dict, Any
from bs4 import BeautifulSoup, Tag

from src.core.logging import get_logger

logger = get_logger(__name__)


def find_main_content_area(soup: BeautifulSoup) -> Optional[Tag]:
    """æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ"""
    # ä¼˜å…ˆæŸ¥æ‰¾å¸¦æœ‰ç‰¹å®šclassæˆ–idçš„ä¸»è¦å†…å®¹åŒºåŸŸ
    content_selectors = [
        'main',
        '[role="main"]',
        '.main-content',
        '.content',
        '#main-content',
        '#content',
        '.page-content'
    ]
    
    for selector in content_selectors:
        main_content = soup.select_one(selector)
        if main_content:
            return main_content
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›body
    return soup.find('body') or soup


def is_important_section_title(element: Tag, important_section_titles: List[str]) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºé‡è¦çš„sectionæ ‡é¢˜
    
    Args:
        element: HTMLå…ƒç´ 
        important_section_titles: é‡è¦sectionæ ‡é¢˜åˆ—è¡¨
        
    Returns:
        æ˜¯å¦ä¸ºé‡è¦æ ‡é¢˜
    """
    if not element or not hasattr(element, 'get_text'):
        return False
        
    element_text = element.get_text(strip=True).lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦çš„sectionæ ‡é¢˜å…³é”®è¯
    for title in important_section_titles:
        if title.lower() in element_text:
            return True
    
    return False


def extract_banner_text_content(banner: Tag) -> Dict[str, Any]:
    """
    ä»bannerä¸­æå–æ–‡æœ¬å†…å®¹ï¼ˆæ ‡é¢˜ã€æè¿°ç­‰ï¼‰
    
    Args:
        banner: banner HTMLå…ƒç´ 
        
    Returns:
        æå–çš„bannerå†…å®¹å­—å…¸
    """
    if not banner:
        return {}
    
    banner_content = {}
    
    # æå–æ ‡é¢˜ (h1, h2 ç­‰)
    title_element = banner.find(['h1', 'h2', 'h3'])
    if title_element:
        banner_content['title'] = title_element.get_text(strip=True)
    
    # æå–æè¿°æ€§æ®µè½
    paragraphs = banner.find_all('p')
    descriptions = []
    for p in paragraphs:
        text = p.get_text(strip=True)
        if text and len(text) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬
            descriptions.append(text)
    
    if descriptions:
        banner_content['description'] = ' '.join(descriptions[:3])  # æœ€å¤š3ä¸ªæ®µè½
    
    # æå–é“¾æ¥
    links = banner.find_all('a', href=True)
    if links:
        banner_content['links'] = []
        for link in links[:5]:  # æœ€å¤š5ä¸ªé“¾æ¥
            link_text = link.get_text(strip=True)
            link_href = link.get('href')
            if link_text and link_href:
                banner_content['links'].append({
                    'text': link_text,
                    'href': link_href
                })
    
    # æå–åˆ—è¡¨é¡¹
    list_items = banner.find_all('li')
    if list_items:
        banner_content['features'] = []
        for li in list_items[:10]:  # æœ€å¤š10ä¸ªç‰¹æ€§
            feature_text = li.get_text(strip=True)
            if feature_text and len(feature_text) > 5:
                banner_content['features'].append(feature_text)
    
    return banner_content


def standardize_banner_images(soup: BeautifulSoup) -> BeautifulSoup:
    """
    æ ‡å‡†åŒ–bannerä¸­çš„å›¾ç‰‡ï¼Œæ·»åŠ å ä½ç¬¦
    
    Args:
        soup: BeautifulSoupå¯¹è±¡
        
    Returns:
        å¤„ç†åçš„BeautifulSoupå¯¹è±¡
    """
    print("ğŸ–¼ï¸ æ ‡å‡†åŒ–bannerå›¾ç‰‡...")
    
    # æŸ¥æ‰¾banneråŒºåŸŸ
    banner_selectors = [
        '.banner',
        '.hero',
        '.jumbotron', 
        '.page-header',
        'header',
        '.product-banner'
    ]
    
    processed_count = 0
    
    for selector in banner_selectors:
        banners = soup.select(selector)
        
        for banner in banners:
            # å¤„ç†bannerä¸­çš„å›¾ç‰‡
            images = banner.find_all('img')
            
            for img in images:
                src = img.get('src')
                if src:
                    if src.startswith('/'):
                        img['src'] = f"{{img_hostname}}{src}"
                        processed_count += 1
                    elif not src.startswith(('http', 'https', 'data:')):
                        # ç›¸å¯¹è·¯å¾„
                        img['src'] = f"{{img_hostname}}/{src}"
                        processed_count += 1
            
            # å¤„ç†background-imageæ ·å¼
            for element in banner.find_all(style=True):
                style = element.get('style', '')
                if 'background-image' in style:
                    # æ›¿æ¢ç›¸å¯¹è·¯å¾„
                    new_style = re.sub(
                        r'background-image:\s*url\(["\']?(/[^"\']*?)["\']?\)',
                        r'background-image: url("{img_hostname}\1")',
                        style
                    )
                    if new_style != style:
                        element['style'] = new_style
                        processed_count += 1
    
    if processed_count > 0:
        print(f"  âœ“ æ ‡å‡†åŒ–äº† {processed_count} ä¸ªbannerå›¾ç‰‡")
    
    return soup


def extract_structured_content(soup: BeautifulSoup, 
                             important_section_titles: List[str]) -> Dict[str, Any]:
    """
    æå–ç»“æ„åŒ–å†…å®¹ï¼ŒåŒ…æ‹¬å„ä¸ªé‡è¦section
    
    Args:
        soup: BeautifulSoupå¯¹è±¡
        important_section_titles: é‡è¦sectionæ ‡é¢˜åˆ—è¡¨
        
    Returns:
        ç»“æ„åŒ–å†…å®¹å­—å…¸
    """
    structured_content = {
        'sections': [],
        'pricing_tables': [],
        'feature_lists': [],
        'call_to_actions': []
    }
    
    # æå–é‡è¦sections
    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    
    for heading in headings:
        if is_important_section_title(heading, important_section_titles):
            section_content = {
                'title': heading.get_text(strip=True),
                'level': heading.name,
                'content': ''
            }
            
            # æ”¶é›†è¯¥æ ‡é¢˜ä¸‹çš„å†…å®¹
            next_sibling = heading.find_next_sibling()
            content_parts = []
            
            while next_sibling and next_sibling.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                if next_sibling.get_text(strip=True):
                    content_parts.append(next_sibling.get_text(strip=True))
                next_sibling = next_sibling.find_next_sibling()
            
            section_content['content'] = ' '.join(content_parts[:3])  # é™åˆ¶å†…å®¹é•¿åº¦
            structured_content['sections'].append(section_content)
    
    # æå–å®šä»·è¡¨æ ¼
    tables = soup.find_all('table')
    for table in tables:
        # ç®€å•çš„è¡¨æ ¼å†…å®¹æå–
        table_text = table.get_text(strip=True)
        if any(keyword in table_text.lower() for keyword in ['ä»·æ ¼', 'price', 'è´¹ç”¨', 'cost']):
            structured_content['pricing_tables'].append({
                'content': table_text[:500]  # é™åˆ¶é•¿åº¦
            })
    
    # æå–ç‰¹æ€§åˆ—è¡¨
    feature_lists = soup.find_all(['ul', 'ol'])
    for ul in feature_lists:
        items = ul.find_all('li')
        if len(items) >= 2:  # è‡³å°‘2ä¸ªé¡¹ç›®æ‰ç®—ç‰¹æ€§åˆ—è¡¨
            features = [li.get_text(strip=True) for li in items[:10]]  # æœ€å¤š10ä¸ªç‰¹æ€§
            if any(len(f) > 10 for f in features):  # è¿‡æ»¤è¿‡çŸ­çš„ç‰¹æ€§
                structured_content['feature_lists'].append({
                    'items': features
                })
    
    # æå–è¡ŒåŠ¨å·å¬é“¾æ¥
    cta_keywords = ['å¼€å§‹ä½¿ç”¨', 'ç«‹å³è¯•ç”¨', 'äº†è§£æ›´å¤š', 'get started', 'learn more', 'try now']
    links = soup.find_all('a', href=True)
    
    for link in links:
        link_text = link.get_text(strip=True).lower()
        if any(keyword.lower() in link_text for keyword in cta_keywords):
            structured_content['call_to_actions'].append({
                'text': link.get_text(strip=True),
                'href': link.get('href')
            })
    
    return structured_content


# ========== FAQç›¸å…³åŠŸèƒ½ ==========
# ä»faq_utils.pyåˆå¹¶è¿‡æ¥çš„FAQå¤„ç†åŠŸèƒ½

def is_faq_item(li: Tag) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºFAQé¡¹ç»“æ„"""
    # æ£€æŸ¥æ˜¯å¦æœ‰iconå’Œdivç»“æ„
    has_icon = li.find('i', class_='icon icon-plus')
    has_div = li.find('div')

    if has_icon and has_div:
        # æ£€æŸ¥divå†…æ˜¯å¦æœ‰aæ ‡ç­¾ï¼ˆé—®é¢˜ï¼‰å’Œsectionæ ‡ç­¾ï¼ˆç­”æ¡ˆï¼‰
        div = li.find('div')
        has_question = div.find('a')
        has_answer = div.find('section')

        return bool(has_question and has_answer)

    return False


def process_faq_item(li: Tag, new_li: Tag, soup: BeautifulSoup):
    """å¤„ç†FAQé¡¹ï¼Œæå–é—®é¢˜å’Œç­”æ¡ˆï¼Œä½¿ç”¨ç¾åŒ–çš„HTMLç»“æ„"""
    div = li.find('div')
    if not div:
        return

    # æå–é—®é¢˜
    question_a = div.find('a')
    if question_a:
        question_text = question_a.get_text(strip=True)
        if question_text:
            # åˆ›å»ºé—®é¢˜divå®¹å™¨
            question_div = soup.new_tag('div', **{'class': 'faq-question'})
            question_div.string = question_text
            new_li.append(question_div)

    # æå–ç­”æ¡ˆ
    answer_section = div.find('section')
    if answer_section:
        answer_text = answer_section.get_text(strip=True)
        if answer_text:
            # åˆ›å»ºç­”æ¡ˆdivå®¹å™¨
            answer_div = soup.new_tag('div', **{'class': 'faq-answer'})
            answer_div.string = answer_text
            new_li.append(answer_div)


def extract_qa_content(soup: BeautifulSoup) -> str:
    """æå–Q&Aå†…å®¹ä»¥åŠæ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®å†…å®¹"""
    logger.info("æå–Q&Aå†…å®¹...")

    qa_sections = []
    
    # æŸ¥æ‰¾å¸¸è§é—®é¢˜éƒ¨åˆ†
    faq_keywords = ['å¸¸è§é—®é¢˜', 'faq', 'frequently asked questions', 'é—®é¢˜è§£ç­”']
    
    for keyword in faq_keywords:
        # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ ‡é¢˜
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'], 
                                string=lambda text: text and keyword.lower() in text.lower())
        
        for heading in headings:
            # æŸ¥æ‰¾æ ‡é¢˜åçš„å†…å®¹
            content_element = heading.find_next_sibling()
            
            if content_element:
                qa_text = content_element.get_text(strip=True)
                if qa_text and len(qa_text) > 50:  # è¿‡æ»¤è¿‡çŸ­çš„å†…å®¹
                    qa_sections.append({
                        'section': keyword,
                        'content': qa_text[:1000]  # é™åˆ¶é•¿åº¦
                    })
    
    # æŸ¥æ‰¾æ”¯æŒç›¸å…³å†…å®¹
    support_keywords = ['æ”¯æŒ', 'support', 'æœåŠ¡çº§åˆ«åè®®', 'sla', 'service level']
    
    for keyword in support_keywords:
        elements = soup.find_all(string=lambda text: text and keyword.lower() in text.lower())
        
        for element in elements[:3]:  # é™åˆ¶æ•°é‡
            parent = element.parent
            if parent:
                support_text = parent.get_text(strip=True)
                if support_text and len(support_text) > 30:
                    qa_sections.append({
                        'section': keyword,
                        'content': support_text[:500]
                    })
    
    # åˆå¹¶æ‰€æœ‰Q&Aå†…å®¹
    if qa_sections:
        combined_qa = '\n\n'.join([f"## {section['section']}\n{section['content']}"
                                  for section in qa_sections[:5]])  # é™åˆ¶ä¸ºå‰5ä¸ª
        logger.info(f"æå–äº† {len(qa_sections)} ä¸ªQ&Aéƒ¨åˆ†")
        return combined_qa
    else:
        logger.warning("æœªæ‰¾åˆ°Q&Aå†…å®¹")
        return ""


# ========== Sectionåˆ†ç±»åŠŸèƒ½ ==========

def classify_pricing_section(section: Tag) -> str:
    """
    æ™ºèƒ½åˆ†ç±»pricing-page-sectionï¼Œåˆ¤æ–­sectionç±»å‹
    
    Args:
        section: BeautifulSoupçš„Tagå¯¹è±¡ï¼Œä»£è¡¨ä¸€ä¸ªpricing-page-section
        
    Returns:
        sectionç±»å‹ï¼š'faq', 'sla', 'content', 'other'
    """
    if not section:
        return 'other'
    
    # è·å–sectionçš„æ–‡æœ¬å†…å®¹
    section_text = section.get_text().strip()
    section_html = str(section).lower()
    
    # FAQç›¸å…³çš„æ­£åˆ™æ¨¡å¼
    faq_patterns = [
        r'å¸¸è§é—®é¢˜',
        r'FAQs',
        r'frequently\s+asked\s+questions',
        r'q\s*&\s*a',
        r'more-detail',  # ç‰¹æ®Šclassæ ‡è¯†
    ]
    
    # SLA/æ”¯æŒç›¸å…³çš„æ­£åˆ™æ¨¡å¼
    sla_patterns = [
        r'æ”¯æŒå’ŒæœåŠ¡çº§åˆ«åè®®',
        r'Support\s*&\s*sla',
        r'service\s+level\s+agreement',
    ]
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºFAQç±»å‹
    for pattern in faq_patterns:
        if re.search(pattern, section_text, re.IGNORECASE):
            logger.debug(f"æ£€æµ‹åˆ°FAQ section: {pattern}")
            return 'faq'
        if re.search(pattern, section_html):
            logger.debug(f"æ£€æµ‹åˆ°FAQ section (HTML): {pattern}")
            return 'faq'
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºSLA/æ”¯æŒç±»å‹
    for pattern in sla_patterns:
        if re.search(pattern, section_text, re.IGNORECASE):
            logger.debug(f"æ£€æµ‹åˆ°SLA section: {pattern}")
            return 'sla'
        if re.search(pattern, section_html):
            logger.debug(f"æ£€æµ‹åˆ°SLA section (HTML): {pattern}")
            return 'sla'
    
    
    # æ£€æŸ¥sectioné•¿åº¦ï¼ŒçŸ­å†…å®¹å¯èƒ½æ˜¯å¯¼èˆªæˆ–å…¶ä»–
    if len(section_text.strip()) < 50:
        logger.debug("sectionå†…å®¹è¿‡çŸ­ï¼Œå½’ç±»ä¸ºother")
        return 'other'
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æˆ–ç»“æ„åŒ–å†…å®¹ï¼ˆé€šå¸¸æ˜¯äº§å“åŠŸèƒ½è¯´æ˜ï¼‰
    if section.find('table') or section.find('ul') or section.find('ol'):
        # å¦‚æœæœ‰ç»“æ„åŒ–å†…å®¹ä½†æ²¡æœ‰FAQ/SLAæ ‡è¯†ï¼Œå¾ˆå¯èƒ½æ˜¯äº§å“å†…å®¹
        logger.debug("æ£€æµ‹åˆ°ç»“æ„åŒ–å†…å®¹ï¼Œå½’ç±»ä¸ºcontent")
        return 'content'
    
    # é»˜è®¤å½’ç±»ä¸ºcontentï¼ˆäº§å“ç›¸å…³å†…å®¹ï¼‰
    logger.debug("é»˜è®¤å½’ç±»ä¸ºcontent")
    return 'content'


def filter_sections_by_type(sections: List[Tag], 
                          include_types: List[str] = None, 
                          exclude_types: List[str] = None) -> List[Tag]:
    """
    æ ¹æ®ç±»å‹è¿‡æ»¤sections
    
    Args:
        sections: sectionåˆ—è¡¨
        include_types: åŒ…å«çš„ç±»å‹åˆ—è¡¨
        exclude_types: æ’é™¤çš„ç±»å‹åˆ—è¡¨
        
    Returns:
        è¿‡æ»¤åçš„sectionåˆ—è¡¨
    """
    if not sections:
        return []
    
    filtered_sections = []
    
    for section in sections:
        section_type = classify_pricing_section(section)
        
        # æ£€æŸ¥åŒ…å«æ¡ä»¶
        if include_types and section_type not in include_types:
            continue
            
        # æ£€æŸ¥æ’é™¤æ¡ä»¶  
        if exclude_types and section_type in exclude_types:
            continue
            
        filtered_sections.append(section)
    
    return filtered_sections