#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨å†…å®¹æå–å™¨
æŠ½ç¦»BaseStrategyä¸­çš„Titleã€Metaã€ä¸»å†…å®¹æå–é€»è¾‘ï¼Œæ”¯æŒä¼ ç»ŸCMSå’Œflexible JSONåŒæ ¼å¼éœ€æ±‚
"""

import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from src.core.logging import get_logger
from src.utils.content.content_utils import find_main_content_area

logger = get_logger(__name__)


class ContentExtractor:
    """é€šç”¨å†…å®¹æå–å™¨ - æå–æ ‡é¢˜ã€Metaä¿¡æ¯å’Œä¸»è¦å†…å®¹"""

    def __init__(self):
        """åˆå§‹åŒ–å†…å®¹æå–å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ–ContentExtractor")

    def extract_base_metadata(self, soup: BeautifulSoup, url: str = "", html_file_path: str = "") -> Dict[str, Any]:
        """
        æå–åŸºç¡€å…ƒæ•°æ®ä¿¡æ¯
        
        Args:
            soup: BeautifulSoupè§£æçš„HTMLå¯¹è±¡
            url: æºURL
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            åŸºç¡€å…ƒæ•°æ®å­—å…¸
        """
        logger.info("ğŸ” æå–åŸºç¡€å…ƒæ•°æ®...")

        # åˆå§‹åŒ–åŸºç¡€æ•°æ®ç»“æ„
        metadata = {
            "source_file": str(html_file_path),
            "source_url": url or self._get_default_url(html_file_path),
            "extraction_timestamp": datetime.now().isoformat(),
            "Title": "",
            "MetaTitle": "",
            "MetaDescription": "",
            "MetaKeywords": "", 
            "MSServiceName": "",
            "Slug": "",
            "Language": "zh-cn",
            "LastModified": ""
        }

        # 1. æå–æ ‡é¢˜
        metadata["Title"] = self.extract_title(soup)
        
        # 2. æå–Metaä¿¡æ¯
        metadata["MetaTitle"] = self.extract_meta_title(soup)
        metadata["MetaDescription"] = self.extract_meta_description(soup)
        metadata["MetaKeywords"] = self.extract_meta_keywords(soup)
        metadata["MSServiceName"] = self.extract_ms_service_name(soup)
        metadata["Slug"] = self.extract_slug(url)
        metadata["Language"] = self.extract_language(soup)
        
        # 3. æå–å…¶ä»–å…ƒæ•°æ®
        metadata["LastModified"] = self.extract_last_modified(soup)

        return metadata

    def extract_title(self, soup: BeautifulSoup) -> str:
        """
        æå–é¡µé¢æ ‡é¢˜
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            é¡µé¢æ ‡é¢˜å­—ç¬¦ä¸²
        """
        # ä¼˜å…ˆæŸ¥æ‰¾é¡µé¢titleæ ‡ç­¾
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text(strip=True)
            logger.info(f"âœ“ æå–é¡µé¢æ ‡é¢˜: {title}")
            if title and len(title) > 0:
                return title
        
        logger.info("âš  æœªæ‰¾åˆ°é¡µé¢æ ‡é¢˜")
        return ""

    def extract_meta_title(self, soup: BeautifulSoup) -> str:
        """
        æå–Metaæ ‡é¢˜
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            Metaæ ‡é¢˜å­—ç¬¦ä¸²
        """
        meta_title = soup.find('meta', attrs={'name': 'title'})
        if meta_title:
            title = meta_title.get('content', '')
            logger.info(f"âœ“ æå–Metaæ ‡é¢˜: {title}")
            return title
        return ""

    def extract_meta_description(self, soup: BeautifulSoup) -> str:
        """
        æå–Metaæè¿°
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            Metaæè¿°å­—ç¬¦ä¸²
        """
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            desc = meta_desc.get('content', '')
            logger.info(f"âœ“ æå–Metaæè¿°: {desc[:50]}...")
            return desc
        return ""

    def extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """
        æå–Metaå…³é”®è¯
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            Metaå…³é”®è¯å­—ç¬¦ä¸²
        """
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            keywords = meta_keywords.get('content', '')
            logger.info(f"âœ“ æå–Metaå…³é”®è¯: {keywords}")
            return keywords
        return ""

    def extract_ms_service_name(self, soup: BeautifulSoup) -> str:
        """
        æå–å¾®è½¯æœåŠ¡åç§°
        ä»pure-content divå†…çš„tagså…ƒç´ ä¸­çš„ms.serviceå±æ€§
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            MSServiceNameå­—ç¬¦ä¸²
        """
        # æŸ¥æ‰¾pure-content div
        pure_content_div = soup.find('div', class_='pure-content')
        if pure_content_div:
            # åœ¨pure-content divå†…æŸ¥æ‰¾tagså…ƒç´ 
            tags_element = pure_content_div.find('tags')
            if tags_element:
                # æå–ms.serviceå±æ€§å€¼
                ms_service = tags_element.get('ms.service', '')
                if ms_service:
                    logger.info(f"âœ“ æå–MSServiceName: {ms_service}")
                    return ms_service
                else:
                    logger.info("âš  tagså…ƒç´ ä¸­æ²¡æœ‰ms.serviceå±æ€§")
            else:
                logger.info("âš  pure-content divä¸­æ²¡æœ‰æ‰¾åˆ°tagså…ƒç´ ")
        else:
            logger.info("âš  æ²¡æœ‰æ‰¾åˆ°pure-content div")

        return ""

    def extract_slug(self, url: str) -> str:
        """
        ä»URLæå–slug
        
        Args:
            url: æºURL
            
        Returns:
            slugå­—ç¬¦ä¸²
        """
        if not url:
            return ""

        try:
            parsed = urlparse(url)
            path = parsed.path

            logger.info(f"ä»URLæå–slug: {url}")

            # æå–/details/ä¹‹ååˆ°/index.htmlä¹‹å‰çš„å†…å®¹ï¼Œç”¨-è¿æ¥
            # ä¾‹å¦‚ /pricing/details/storage/files/index.html -> storage-files
            # ä¾‹å¦‚ /pricing/details/api-management/index.html -> api-management
            if '/details/' in path:
                # æ‰¾åˆ°/details/ä¹‹åçš„éƒ¨åˆ†
                after_details = path.split('/details/')[1]

                # ç§»é™¤/index.htmlåç¼€
                if after_details.endswith('/index.html'):
                    after_details = after_details[:-11]  # ç§»é™¤'/index.html'
                elif after_details.endswith('/'):
                    after_details = after_details[:-1]  # ç§»é™¤æœ«å°¾çš„'/'

                # åˆ†å‰²è·¯å¾„å¹¶ç”¨_è¿æ¥
                path_parts = [p for p in after_details.split('/') if p]
                if path_parts:
                    slug = '_'.join(path_parts)
                    logger.info(f"âœ“ æå–slug: {slug}")
                    return slug
        except Exception as e:
            logger.info(f"âš  slugæå–å¤±è´¥: {e}")

        return ""
    
    def extract_language(self, soup: BeautifulSoup) -> str:
        """
        æå–é¡µé¢è¯­è¨€
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            é¡µé¢è¯­è¨€å­—ç¬¦ä¸²
        """
        body = soup.find("body")
        if body and body.has_attr("class"):
            return body["class"][0]
        return "zh-cn"

    def extract_last_modified(self, soup: BeautifulSoup) -> str:
        """
        æå–æœ€åä¿®æ”¹æ—¶é—´
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            æœ€åä¿®æ”¹æ—¶é—´å­—ç¬¦ä¸²
        """
        # æŸ¥æ‰¾æœ€åä¿®æ”¹æ—¶é—´çš„å…ƒæ•°æ®
        modified_selectors = [
            'meta[name="last-modified"]',
            'meta[property="article:modified_time"]',
            '.last-updated',
            '.modified-date'
        ]
        
        for selector in modified_selectors:
            element = soup.select_one(selector)
            if element:
                if element.name == 'meta':
                    modified = element.get('content', '')
                    logger.info(f"âœ“ æå–æœ€åä¿®æ”¹æ—¶é—´: {modified}")
                    return modified
                else:
                    modified = element.get_text(strip=True)
                    logger.info(f"âœ“ æå–æœ€åä¿®æ”¹æ—¶é—´: {modified}")
                    return modified
        
        return ""

    def extract_main_content(self, soup: BeautifulSoup) -> str:
        """
        æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            ä¸»è¦å†…å®¹HTMLå­—ç¬¦ä¸²
        """
        logger.info("ğŸ“ æå–ä¸»è¦å†…å®¹åŒºåŸŸ...")
        
        try:
            # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = find_main_content_area(soup)
            if main_content:
                content_html = str(main_content)
                logger.info(f"âœ“ æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œé•¿åº¦: {len(content_html)}")
                return content_html
            
            logger.info("âš  æœªæ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ")
            return ""
            
        except Exception as e:
            logger.info(f"âš  ä¸»è¦å†…å®¹æå–å¤±è´¥: {e}")
            return ""

    def _get_default_url(self, html_file_path: str) -> str:
        """
        è·å–é»˜è®¤URL
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            é»˜è®¤URLå­—ç¬¦ä¸²
        """
        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­URL
        if html_file_path:
            file_name = Path(html_file_path).stem
            if file_name.endswith('-index'):
                service_name = file_name[:-6]
                return f"https://www.azure.cn/pricing/details/{service_name}/"
        
        return ""