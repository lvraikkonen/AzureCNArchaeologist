#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†…å®¹å¤„ç†å·¥å…·å‡½æ•°
å¤„ç†ä¸»è¦å†…å®¹åŒºåŸŸæå–ã€sectionæ ‡é¢˜è¯†åˆ«ã€bannerå¤„ç†ç­‰
"""

import re
from typing import Optional, List, Dict, Any
from bs4 import BeautifulSoup, Tag


def find_main_content_area(soup: BeautifulSoup) -> Optional[Tag]:
    """æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ"""
    # ä¼˜å…ˆæŸ¥æ‰¾å¸¦æœ‰ç‰¹å®šclassæˆ–idçš„ä¸»è¦å†…å®¹åŒºåŸŸ
    content_selectors = [
        'main',
        '[role="main"]',
        '.main-content',
        '.content',
        '#main-content',
        '#content',
        '.page-content'
    ]
    
    for selector in content_selectors:
        main_content = soup.select_one(selector)
        if main_content:
            return main_content
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›body
    return soup.find('body') or soup


def is_important_section_title(element: Tag, important_section_titles: List[str]) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºé‡è¦çš„sectionæ ‡é¢˜
    
    Args:
        element: HTMLå…ƒç´ 
        important_section_titles: é‡è¦sectionæ ‡é¢˜åˆ—è¡¨
        
    Returns:
        æ˜¯å¦ä¸ºé‡è¦æ ‡é¢˜
    """
    if not element or not hasattr(element, 'get_text'):
        return False
        
    element_text = element.get_text(strip=True).lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦çš„sectionæ ‡é¢˜å…³é”®è¯
    for title in important_section_titles:
        if title.lower() in element_text:
            return True
    
    return False


def extract_banner_text_content(banner: Tag) -> Dict[str, Any]:
    """
    ä»bannerä¸­æå–æ–‡æœ¬å†…å®¹ï¼ˆæ ‡é¢˜ã€æè¿°ç­‰ï¼‰
    
    Args:
        banner: banner HTMLå…ƒç´ 
        
    Returns:
        æå–çš„bannerå†…å®¹å­—å…¸
    """
    if not banner:
        return {}
    
    banner_content = {}
    
    # æå–æ ‡é¢˜ (h1, h2 ç­‰)
    title_element = banner.find(['h1', 'h2', 'h3'])
    if title_element:
        banner_content['title'] = title_element.get_text(strip=True)
    
    # æå–æè¿°æ€§æ®µè½
    paragraphs = banner.find_all('p')
    descriptions = []
    for p in paragraphs:
        text = p.get_text(strip=True)
        if text and len(text) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬
            descriptions.append(text)
    
    if descriptions:
        banner_content['description'] = ' '.join(descriptions[:3])  # æœ€å¤š3ä¸ªæ®µè½
    
    # æå–é“¾æ¥
    links = banner.find_all('a', href=True)
    if links:
        banner_content['links'] = []
        for link in links[:5]:  # æœ€å¤š5ä¸ªé“¾æ¥
            link_text = link.get_text(strip=True)
            link_href = link.get('href')
            if link_text and link_href:
                banner_content['links'].append({
                    'text': link_text,
                    'href': link_href
                })
    
    # æå–åˆ—è¡¨é¡¹
    list_items = banner.find_all('li')
    if list_items:
        banner_content['features'] = []
        for li in list_items[:10]:  # æœ€å¤š10ä¸ªç‰¹æ€§
            feature_text = li.get_text(strip=True)
            if feature_text and len(feature_text) > 5:
                banner_content['features'].append(feature_text)
    
    return banner_content


def standardize_banner_images(soup: BeautifulSoup) -> BeautifulSoup:
    """
    æ ‡å‡†åŒ–bannerä¸­çš„å›¾ç‰‡ï¼Œæ·»åŠ å ä½ç¬¦
    
    Args:
        soup: BeautifulSoupå¯¹è±¡
        
    Returns:
        å¤„ç†åçš„BeautifulSoupå¯¹è±¡
    """
    print("ğŸ–¼ï¸ æ ‡å‡†åŒ–bannerå›¾ç‰‡...")
    
    # æŸ¥æ‰¾banneråŒºåŸŸ
    banner_selectors = [
        '.banner',
        '.hero',
        '.jumbotron', 
        '.page-header',
        'header',
        '.product-banner'
    ]
    
    processed_count = 0
    
    for selector in banner_selectors:
        banners = soup.select(selector)
        
        for banner in banners:
            # å¤„ç†bannerä¸­çš„å›¾ç‰‡
            images = banner.find_all('img')
            
            for img in images:
                src = img.get('src')
                if src:
                    if src.startswith('/'):
                        img['src'] = f"{{img_hostname}}{src}"
                        processed_count += 1
                    elif not src.startswith(('http', 'https', 'data:')):
                        # ç›¸å¯¹è·¯å¾„
                        img['src'] = f"{{img_hostname}}/{src}"
                        processed_count += 1
            
            # å¤„ç†background-imageæ ·å¼
            for element in banner.find_all(style=True):
                style = element.get('style', '')
                if 'background-image' in style:
                    # æ›¿æ¢ç›¸å¯¹è·¯å¾„
                    new_style = re.sub(
                        r'background-image:\s*url\(["\']?(/[^"\']*?)["\']?\)',
                        r'background-image: url("{img_hostname}\1")',
                        style
                    )
                    if new_style != style:
                        element['style'] = new_style
                        processed_count += 1
    
    if processed_count > 0:
        print(f"  âœ“ æ ‡å‡†åŒ–äº† {processed_count} ä¸ªbannerå›¾ç‰‡")
    
    return soup


def extract_structured_content(soup: BeautifulSoup, 
                             important_section_titles: List[str]) -> Dict[str, Any]:
    """
    æå–ç»“æ„åŒ–å†…å®¹ï¼ŒåŒ…æ‹¬å„ä¸ªé‡è¦section
    
    Args:
        soup: BeautifulSoupå¯¹è±¡
        important_section_titles: é‡è¦sectionæ ‡é¢˜åˆ—è¡¨
        
    Returns:
        ç»“æ„åŒ–å†…å®¹å­—å…¸
    """
    structured_content = {
        'sections': [],
        'pricing_tables': [],
        'feature_lists': [],
        'call_to_actions': []
    }
    
    # æå–é‡è¦sections
    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    
    for heading in headings:
        if is_important_section_title(heading, important_section_titles):
            section_content = {
                'title': heading.get_text(strip=True),
                'level': heading.name,
                'content': ''
            }
            
            # æ”¶é›†è¯¥æ ‡é¢˜ä¸‹çš„å†…å®¹
            next_sibling = heading.find_next_sibling()
            content_parts = []
            
            while next_sibling and next_sibling.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                if next_sibling.get_text(strip=True):
                    content_parts.append(next_sibling.get_text(strip=True))
                next_sibling = next_sibling.find_next_sibling()
            
            section_content['content'] = ' '.join(content_parts[:3])  # é™åˆ¶å†…å®¹é•¿åº¦
            structured_content['sections'].append(section_content)
    
    # æå–å®šä»·è¡¨æ ¼
    tables = soup.find_all('table')
    for table in tables:
        # ç®€å•çš„è¡¨æ ¼å†…å®¹æå–
        table_text = table.get_text(strip=True)
        if any(keyword in table_text.lower() for keyword in ['ä»·æ ¼', 'price', 'è´¹ç”¨', 'cost']):
            structured_content['pricing_tables'].append({
                'content': table_text[:500]  # é™åˆ¶é•¿åº¦
            })
    
    # æå–ç‰¹æ€§åˆ—è¡¨
    feature_lists = soup.find_all(['ul', 'ol'])
    for ul in feature_lists:
        items = ul.find_all('li')
        if len(items) >= 2:  # è‡³å°‘2ä¸ªé¡¹ç›®æ‰ç®—ç‰¹æ€§åˆ—è¡¨
            features = [li.get_text(strip=True) for li in items[:10]]  # æœ€å¤š10ä¸ªç‰¹æ€§
            if any(len(f) > 10 for f in features):  # è¿‡æ»¤è¿‡çŸ­çš„ç‰¹æ€§
                structured_content['feature_lists'].append({
                    'items': features
                })
    
    # æå–è¡ŒåŠ¨å·å¬é“¾æ¥
    cta_keywords = ['å¼€å§‹ä½¿ç”¨', 'ç«‹å³è¯•ç”¨', 'äº†è§£æ›´å¤š', 'get started', 'learn more', 'try now']
    links = soup.find_all('a', href=True)
    
    for link in links:
        link_text = link.get_text(strip=True).lower()
        if any(keyword.lower() in link_text for keyword in cta_keywords):
            structured_content['call_to_actions'].append({
                'text': link.get_text(strip=True),
                'href': link.get('href')
            })
    
    return structured_content