#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Azure å®šä»·æ•°æ®æ‰¹é‡å¤„ç†å™¨
å¤„ç† prod-html ç›®å½•ä¸­çš„æ‰€æœ‰ HTML æ–‡ä»¶ï¼Œåº”ç”¨åŒºåŸŸè¿‡æ»¤è§„åˆ™ï¼Œç”Ÿæˆç»“æž„åŒ– JSON è¾“å‡º

åŸºäºŽåŒºåŸŸçš„è¡¨æ ¼è¿‡æ»¤è§„åˆ™ï¼š
- è§„åˆ™1ï¼šåŒºåŸŸä¸åœ¨é…ç½®æ–‡ä»¶ä¸­ â†’ åŒ…å«æ‰€æœ‰è¡¨æ ¼
- è§„åˆ™2ï¼šåŒºåŸŸåœ¨é…ç½®æ–‡ä»¶ä¸­ä½† tableIDs ä¸ºç©º â†’ åŒ…å«æ‰€æœ‰è¡¨æ ¼  
- è§„åˆ™3ï¼šåŒºåŸŸåœ¨é…ç½®æ–‡ä»¶ä¸­ä¸” tableIDs æœ‰å†…å®¹ â†’ æŽ’é™¤æŒ‡å®šè¡¨æ ¼
"""

import os
import json
import glob
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from azure_general_parser import AzurePricingParser


class AzureBatchProcessor:
    """Azure å®šä»·æ•°æ®æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, 
                 input_dir: str = "prod-html",
                 output_dir: str = "output", 
                 config_file: str = "soft-category.json",
                 include_region_info: bool = True,
                 region_info_mode: str = "full"):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            input_dir: HTMLæ–‡ä»¶è¾“å…¥ç›®å½•
            output_dir: JSONè¾“å‡ºç›®å½•
            config_file: åŒºåŸŸè¿‡æ»¤é…ç½®æ–‡ä»¶è·¯å¾„
            include_region_info: æ˜¯å¦åŒ…å«åŒºåŸŸä¿¡æ¯
            region_info_mode: åŒºåŸŸä¿¡æ¯æ¨¡å¼ ('minimal', 'hybrid', 'full')
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.config_file = config_file
        self.include_region_info = include_region_info
        self.region_info_mode = region_info_mode
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(exist_ok=True)
        
        # å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            'total_files': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'start_time': None,
            'end_time': None,
            'processed_files': [],
            'failed_files': [],
            'skipped_files': []
        }
        
    def get_html_files(self) -> List[Path]:
        """èŽ·å–æ‰€æœ‰HTMLæ–‡ä»¶"""
        html_files = []
        
        # æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶
        for pattern in ['*.html', '*.htm']:
            html_files.extend(self.input_dir.glob(pattern))
            
        # æŒ‰æ–‡ä»¶åæŽ’åº
        html_files.sort(key=lambda x: x.name)
        
        print(f"âœ“ å‘çŽ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        for file in html_files:
            print(f"  - {file.name}")
            
        return html_files
    
    def detect_product_from_filename(self, filename: str) -> Optional[str]:
        """ä»Žæ–‡ä»¶åæ£€æµ‹äº§å“ç±»åž‹"""
        filename_lower = filename.lower()
        
        # äº§å“ç±»åž‹æ˜ å°„
        product_mappings = {
            'mysql': 'mysql',
            'ssis': 'ssis',
            'cosmos-db': 'cosmos-db',
            'anomaly-detector': 'anomaly-detector',
            'search': 'search',
            'machine-learning': 'machine-learning',
            'postgresql': 'postgresql',
            'power-bi': 'power-bi',
            'sql-database': 'sql-database',
            'entra': 'entra'
        }
        
        for key, product_type in product_mappings.items():
            if key in filename_lower:
                return product_type
                
        return None
    
    def process_single_file(self, html_file: Path) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªHTMLæ–‡ä»¶"""
        print(f"\n{'='*60}")
        print(f"å¤„ç†æ–‡ä»¶: {html_file.name}")
        print(f"{'='*60}")
        
        try:
            # è¯»å–HTMLå†…å®¹
            with open(html_file, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # ä»Žæ–‡ä»¶åé¢„æ£€æµ‹äº§å“ç±»åž‹
            filename_product = self.detect_product_from_filename(html_file.name)
            if filename_product:
                print(f"âœ“ ä»Žæ–‡ä»¶åæ£€æµ‹åˆ°äº§å“ç±»åž‹: {filename_product}")
            
            # åˆ›å»ºè§£æžå™¨ï¼ˆä½¿ç”¨autoæ£€æµ‹æˆ–æ–‡ä»¶åæ£€æµ‹ç»“æžœï¼‰
            parser = AzurePricingParser(
                html_content=html_content,
                product_type=filename_product or "auto",
                config_file_path=self.config_file,
                include_region_info=self.include_region_info,
                region_info_mode=self.region_info_mode
            )
            
            # æ‰§è¡Œè§£æž
            results = parser.parse_all()
            
            # æ·»åŠ æ–‡ä»¶ä¿¡æ¯åˆ°å…ƒæ•°æ®
            results['extraction_metadata'].update({
                'source_file': html_file.name,
                'file_size_bytes': html_file.stat().st_size,
                'filename_detected_product': filename_product
            })
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            detected_product = results['extraction_metadata']['product_type']
            output_filename = f"{detected_product}_{self.region_info_mode}.json"
            output_path = self.output_dir / output_filename
            
            # ä¿å­˜ç»“æžœ
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ“ è§£æžæˆåŠŸï¼Œç»“æžœä¿å­˜åˆ°: {output_filename}")
            
            # è¿”å›žå¤„ç†ç»“æžœæ‘˜è¦
            summary = {
                'status': 'success',
                'source_file': html_file.name,
                'output_file': output_filename,
                'product_type': detected_product,
                'total_tables': len(results['pricing_tables']),
                'total_regions': len(results['regions']),
                'total_faqs': len(results['faqs']),
                'filtered_tables': results['region_filter_info']['total_filtered'],
                'active_region': results['region_filter_info']['active_region']
            }
            
            return summary
            
        except Exception as e:
            error_msg = f"å¤„ç†æ–‡ä»¶ {html_file.name} æ—¶å‡ºé”™: {str(e)}"
            print(f"âœ— {error_msg}")
            
            return {
                'status': 'failed',
                'source_file': html_file.name,
                'error': str(e),
                'error_type': type(e).__name__
            }
    
    def process_all_files(self) -> Dict[str, Any]:
        """å¤„ç†æ‰€æœ‰HTMLæ–‡ä»¶"""
        print("ðŸš€ å¼€å§‹æ‰¹é‡å¤„ç†Azureå®šä»·HTMLæ–‡ä»¶")
        print(f"è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"é…ç½®æ–‡ä»¶: {self.config_file}")
        print(f"åŒºåŸŸä¿¡æ¯æ¨¡å¼: {self.region_info_mode}")
        
        self.processing_stats['start_time'] = datetime.now()
        
        # èŽ·å–æ‰€æœ‰HTMLæ–‡ä»¶
        html_files = self.get_html_files()
        self.processing_stats['total_files'] = len(html_files)
        
        if not html_files:
            print("âš  æœªæ‰¾åˆ°HTMLæ–‡ä»¶")
            return self.processing_stats
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for html_file in html_files:
            result = self.process_single_file(html_file)
            
            if result['status'] == 'success':
                self.processing_stats['successful'] += 1
                self.processing_stats['processed_files'].append(result)
            elif result['status'] == 'failed':
                self.processing_stats['failed'] += 1
                self.processing_stats['failed_files'].append(result)
            else:
                self.processing_stats['skipped'] += 1
                self.processing_stats['skipped_files'].append(result)
        
        self.processing_stats['end_time'] = datetime.now()
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        self.generate_processing_report()
        
        return self.processing_stats
    
    def generate_processing_report(self):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        stats = self.processing_stats
        duration = stats['end_time'] - stats['start_time']
        
        print(f"\n{'='*80}")
        print("ðŸ“Š æ‰¹é‡å¤„ç†å®ŒæˆæŠ¥å‘Š")
        print(f"{'='*80}")
        print(f"å¤„ç†æ—¶é—´: {duration}")
        print(f"æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"æˆåŠŸå¤„ç†: {stats['successful']}")
        print(f"å¤„ç†å¤±è´¥: {stats['failed']}")
        print(f"è·³è¿‡æ–‡ä»¶: {stats['skipped']}")
        
        if stats['successful'] > 0:
            print(f"\nâœ… æˆåŠŸå¤„ç†çš„æ–‡ä»¶:")
            for result in stats['processed_files']:
                print(f"  ðŸ“„ {result['source_file']} â†’ {result['output_file']}")
                print(f"     äº§å“ç±»åž‹: {result['product_type']}")
                print(f"     è¡¨æ ¼æ•°: {result['total_tables']} (è¿‡æ»¤: {result['filtered_tables']})")
                print(f"     åŒºåŸŸæ•°: {result['total_regions']}")
                print(f"     FAQæ•°: {result['total_faqs']}")
                print(f"     å½“å‰åŒºåŸŸ: {result['active_region']}")
        
        if stats['failed'] > 0:
            print(f"\nâŒ å¤„ç†å¤±è´¥çš„æ–‡ä»¶:")
            for result in stats['failed_files']:
                print(f"  ðŸ“„ {result['source_file']}: {result['error']}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.output_dir / f"processing_report_{self.region_info_mode}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
            stats_copy = stats.copy()
            stats_copy['start_time'] = stats['start_time'].isoformat()
            stats_copy['end_time'] = stats['end_time'].isoformat()
            stats_copy['duration_seconds'] = duration.total_seconds()
            
            json.dump(stats_copy, f, ensure_ascii=False, indent=2)
        
        print(f"\nðŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Azureå®šä»·æ•°æ®æ‰¹é‡å¤„ç†å™¨')
    parser.add_argument('--input-dir', default='prod-html', help='HTMLæ–‡ä»¶è¾“å…¥ç›®å½•')
    parser.add_argument('--output-dir', default='output', help='JSONè¾“å‡ºç›®å½•')
    parser.add_argument('--config-file', default='soft-category.json', help='åŒºåŸŸè¿‡æ»¤é…ç½®æ–‡ä»¶')
    parser.add_argument('--region-mode', choices=['minimal', 'hybrid', 'full'], 
                       default='full', help='åŒºåŸŸä¿¡æ¯æ¨¡å¼')
    parser.add_argument('--no-region-info', action='store_true', help='ä¸åŒ…å«åŒºåŸŸä¿¡æ¯')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
    processor = AzureBatchProcessor(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        config_file=args.config_file,
        include_region_info=not args.no_region_info,
        region_info_mode=args.region_mode
    )
    
    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    stats = processor.process_all_files()
    
    # è¿”å›žé€€å‡ºç 
    if stats['failed'] > 0:
        exit(1)
    else:
        exit(0)


if __name__ == "__main__":
    main()
